# ğŸ§  Makine Ã–ÄŸrenimi'nin OlasÄ±lÄ±k Temelleri (Probability Foundations of Machine Learning)

Makine Ã¶ÄŸrenimi, bÃ¼yÃ¼k Ã¶lÃ§Ã¼de bilinmeyen bir sonucun, elimizdeki verilere dayanarak ortaya Ã§Ä±kma **olasÄ±lÄ±ÄŸÄ±nÄ± hesaplama** sanatÄ±dÄ±r. Temelde ML, bir olasÄ±lÄ±k hesaplama makinesi olarak iÅŸlev gÃ¶rÃ¼r.

--- 

## 1. KoÅŸullu OlasÄ±lÄ±k (Conditional Probability) ğŸ“Š

Makine Ã¶ÄŸrenimindeki en yaygÄ±n kullanÄ±m alanÄ±, bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± **baÅŸka faktÃ¶rler (features)** verildiÄŸinde hesaplamaktÄ±r.

| Uygulama (Application) | Hesaplanan KoÅŸullu OlasÄ±lÄ±k | AÃ§Ä±klama (Explanation) |
| :--- | :--- | :--- |
| **Spam Tespiti** (Spam Detection) ğŸ“§ | P(Spam \| Kelimeler, AlÄ±cÄ±lar, ...) | Bir e-postanÄ±n **istenmeyen posta (spam)** olma olasÄ±lÄ±ÄŸÄ±, e-postadaki **kelimeler (words)** ve diÄŸer **Ã¶zellikler (features)** verildiÄŸinde. |
| **Duygu Analizi** (Sentiment Analysis) ğŸ˜Š | P(Mutlu \| Metindeki Kelimeler) | Bir metnin **mutlu (happy)** olma olasÄ±lÄ±ÄŸÄ±, iÃ§erdiÄŸi **kelimeler (words)** verildiÄŸinde. |
| **GÃ¶rÃ¼ntÃ¼ TanÄ±ma** (Image Recognition) ğŸ“¸ | P(Kedi \| Pikseller) | Bir resimde **kedi (cat)** olma olasÄ±lÄ±ÄŸÄ±, resimdeki **pikseller (pixels)** verildiÄŸinde. |

**Teknik TanÄ±m:** KoÅŸullu olasÄ±lÄ±k, **P(A \| B)** formÃ¼lÃ¼ ile ifade edilir ve "**B olayÄ± gerÃ§ekleÅŸtiÄŸinde A olayÄ±nÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±**" anlamÄ±na gelir. Makine Ã¶ÄŸrenimi **sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±nÄ±n (classifiers)** yaptÄ±ÄŸÄ± temel iÅŸ budur.

---

## 2. SÃ¼pervizyonlu Ã–ÄŸrenme ve KoÅŸullu OlasÄ±lÄ±k (Supervised Learning and Conditional Probability)

Metinde bahsedilen Spam Tespiti, Duygu Analizi ve GÃ¶rÃ¼ntÃ¼ TanÄ±ma Ã¶rnekleri **SÃ¼pervizyonlu Ã–ÄŸrenme (Supervised Learning)** alanÄ±na aittir.

* Bu alanda, model **etiketli veri (labeled data)** kullanÄ±larak eÄŸitilir ve sorulara cevap arar.
* **AmaÃ§:** Verilen girdi iÃ§in doÄŸru **etiketi (label)** tahmin etmektir.
* **Ã–rnekler:** Resim kedi mi? CÃ¼mle olumlu mu? E-posta spam mi?

---

## 3. Bayes Teoremi ve ArtÃ§Ä± OlasÄ±lÄ±k (Bayes' Theorem and Posterior Probability) ğŸ²

Bayes Teoremi, bir **sÄ±nÄ±flandÄ±rÄ±cÄ± (classifier)** oluÅŸturarak bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± baÅŸka bir ÅŸey verildiÄŸinde hesaplamanÄ±zÄ± saÄŸlayan matematiksel bir mekanizma sunar.

$$\mathbf{P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}}$$

OlasÄ±lÄ±ÄŸÄ±n hesaplanmasÄ±nda **Bayes Teoremi'nin** nasÄ±l kullanÄ±ldÄ±ÄŸÄ± aÃ§Ä±klanmÄ±ÅŸtÄ±r:

1.  **Ã–nsel OlasÄ±lÄ±k (Prior Probability):** BaÅŸlangÄ±Ã§ olasÄ±lÄ±ÄŸÄ±dÄ±r (Ã–rn: TÃ¼m e-postalar iÃ§inde spam oranÄ±, P(Spam)).
2.  **KanÄ±t (Evidence) veya Olay (Event):** Yeni bir bilginin ortaya Ã§Ä±kmasÄ± (Ã–rn: E-postada "lottery" kelimesinin geÃ§mesi).
3.  **ArtÃ§Ä± OlasÄ±lÄ±k (Posterior Probability):** Ã–nsel olasÄ±lÄ±ÄŸÄ±n yeni bilgiyle gÃ¼ncellenmiÅŸ halidir (Ã–rn: P(Spam \| Lottery)).

---

## 4. Jeneratif Modeller ve Saf OlasÄ±lÄ±k (Generative Models and Pure Probability) âœ¨

Metin, koÅŸullu olasÄ±lÄ±ÄŸÄ±n yanÄ± sÄ±ra **saf olasÄ±lÄ±klarÄ±n (pure probabilities)** kullanÄ±ldÄ±ÄŸÄ± bÃ¼yÃ¼k bir ML alanÄ±nÄ± daha tanÄ±tÄ±r: **Jeneratif Makine Ã–ÄŸrenimi (Generative Machine Learning)**. Bu, **GÃ¶zetimsiz Ã–ÄŸrenme (Unsupervised Learning)** alanÄ±nÄ±n bir parÃ§asÄ±dÄ±r.

* **AmaÃ§:** Modelin, var olan veriye benzeyen **yeni iÃ§erik** (gÃ¶rÃ¼ntÃ¼, metin, ses) Ã¼retmesini saÄŸlamaktÄ±r.
* **Ä°ÅŸleyiÅŸ:** Burada amaÃ§, koÅŸullu olasÄ±lÄ±k hesaplamak yerine, yeni Ã¼retilen verinin gerÃ§ekÃ§i olma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmektir (maximize probability)**.
    * **GÃ¶rÃ¼ntÃ¼ Ãœretimi (Image Generation - Ã–rn: StyleGAN):** Model, rastgele piksellerin insan yÃ¼zÃ¼ oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
    * **Metin Ãœretimi (Text Generation):** Model, rastgele kelimelerin anlamlÄ± ve baÄŸlamÄ±na uygun bir metin oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
 
    * # ğŸ’¡ Makine Ã–ÄŸrenmesinde Bayes Teoremi ve Ä°lgili Konular

Bayes Teoremi, makine Ã¶ÄŸrenmesinde **olasÄ±lÄ±ksal sÄ±nÄ±flandÄ±rma** algoritmalarÄ±nÄ±n temelini oluÅŸturur. Ã–zellikle **NaÃ¯ve Bayes SÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±** gibi yaygÄ±n algoritmalarÄ± anlamak iÃ§in bu kavramlara derinlemesine hakim olmak kritiktir.

---

## ğŸ“ Bayes Teoremi'nin FormÃ¼lÃ¼ ve BileÅŸenleri

Bayes Teoremi, bir olayÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ±, Ã¶nceden bilinen koÅŸullu ve marjinal olasÄ±lÄ±klarÄ± kullanarak hesaplar.

$$\huge P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

| Terim | AdÄ± | AÃ§Ä±klama (Makine Ã–ÄŸrenmesi) |
| :--- | :--- | :--- |
| **P(A|B)** | **Sonsal OlasÄ±lÄ±k (Posterior)** | Verilen veri (B) ile Ã¶rneÄŸin belli bir sÄ±nÄ±fa (A) ait olma olasÄ±lÄ±ÄŸÄ±. (Hedefimiz budur!) |
| **P(B|A)** | **Olabilirlik (Likelihood)** | Verilen sÄ±nÄ±f (A) iÃ§in verinin (B) gÃ¶zlemlenme olasÄ±lÄ±ÄŸÄ±. |
| **P(A)** | **Ã–nsel OlasÄ±lÄ±k (Prior)** | B olayÄ± hakkÄ±nda hiÃ§bir bilgi olmadan A sÄ±nÄ±fÄ±nÄ±n genel olasÄ±lÄ±ÄŸÄ±. |
| **P(B)** | **KanÄ±t (Evidence)** | Sonsal olasÄ±lÄ±ÄŸÄ± normalize eden sabittir. |

## ğŸ¤– Naive Bayes SÄ±nÄ±flandÄ±rÄ±cÄ± Ã‡eÅŸitleri ve KarÅŸÄ±laÅŸtÄ±rmasÄ±

Naive Bayes, **Ã¶zelliklerin birbirinden baÄŸÄ±msÄ±z olduÄŸu** (saf varsayÄ±m) varsayÄ±mÄ±na dayanÄ±r ve basitliÄŸi ile Ã¶ne Ã§Ä±kar.

| Algoritma AdÄ± | Temel Ã–zellik VarsayÄ±mÄ± | Uygulama AlanÄ± | Ã–rnek Veri Tipi | AvantajlarÄ± |
| :--- | :--- | :--- | :--- | :--- |
| **Gaussian Naive Bayes** | Ã–zellikler **Normal (Gauss) daÄŸÄ±lÄ±mÄ±na** (Normal/Gaussian Distribution) uyar. | SÃ¼rekli sayÄ±sal verilerin olduÄŸu sÄ±nÄ±flandÄ±rma problemleri. | Boy, kilo, sÄ±caklÄ±k gibi sÃ¼rekli deÄŸerler. | HÄ±zlÄ± ve kÃ¼Ã§Ã¼k veri kÃ¼melerinde baÅŸarÄ±lÄ±dÄ±r. |
| **Multinomial Naive Bayes** | Ã–zellikler **Multinomial daÄŸÄ±lÄ±mÄ±na** (Multinomial Distribution) uyar (sayÄ±m verileri). | Metin sÄ±nÄ±flandÄ±rma (spam, duygu analizi). | Bir belgedeki kelimelerin frekansÄ± (sayÄ±mÄ±). | Metin verilerinde en baÅŸarÄ±lÄ± Ã§eÅŸitlerdendir. |
| **Bernoulli Naive Bayes** | Ã–zellikler **Bernoulli daÄŸÄ±lÄ±mÄ±na** (Bernoulli Distribution) uyar (ikili/Boolean). | Belge varlÄ±ÄŸÄ±/yokluÄŸu, ikili Ã¶zelliklerin olduÄŸu sÄ±nÄ±flandÄ±rmalar. | Kelimenin bir belgede bulunup bulunmamasÄ± (1/0). | Ä°kili Ã¶zellik setleri iÃ§in etkilidir. |

---

## ğŸ§  Bilinmesi Gereken Kritik Ek Konular

Bayes algoritmalarÄ±nÄ± etkili kullanmak iÃ§in anlaÅŸÄ±lmasÄ± gereken temel kavramlar:

### 1. KoÅŸullu BaÄŸÄ±msÄ±zlÄ±k VarsayÄ±mÄ± (Conditional Independence)
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in "saf" kÄ±smÄ± buradan gelir. Hedef sÄ±nÄ±f verildiÄŸinde, girdilerin (Ã¶zelliklerin) birbirlerinden baÄŸÄ±msÄ±z olduÄŸu varsayÄ±lÄ±r. Bu, hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r.
* **Matematiksel Ä°fade:** $P(x_1, x_2, \dots, x_n | C) = \prod_{i=1}^{n} P(x_i | C)$

### 2. SÄ±fÄ±r Frekans Problemi ve Ã‡Ã¶zÃ¼mÃ¼
* **Problem:** EÄŸitim verisinde hiÃ§ gÃ¶rÃ¼lmeyen bir Ã¶zellik-sÄ±nÄ±f kombinasyonu test verisinde ortaya Ã§Ä±karsa, ilgili **Olabilirlik** ($P(B|A)$) sÄ±fÄ±r olur. Bu durumda, sonuÃ§ **Sonsal OlasÄ±lÄ±k** da sÄ±fÄ±rlanÄ±r.
* **Ã‡Ã¶zÃ¼m:** **Laplace YumuÅŸatmasÄ± (Laplace Smoothing)** kullanÄ±lÄ±r. TÃ¼m sayÄ±mlara kÃ¼Ã§Ã¼k bir pozitif deÄŸer ($\alpha$, genellikle 1) eklenerek sÄ±fÄ±r olasÄ±lÄ±klar engellenir.

### 3. Maksimum Sonsal OlasÄ±lÄ±k (Maximum A Posteriori - MAP) Karar KuralÄ±

* **AÃ§Ä±klama:** SÄ±nÄ±flandÄ±rma yaparken NaÃ¯ve Bayes, olasÄ± tÃ¼m sÄ±nÄ±flar ($C_k$) arasÄ±ndan **en yÃ¼ksek sonsal olasÄ±lÄ±ÄŸa** sahip olan sÄ±nÄ±fÄ± seÃ§er. Bu, modelin tahmin mekanizmasÄ±dÄ±r.


### 4. Bayes AÄŸlarÄ± (Bayesian Networks) ğŸŒ
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in baÄŸÄ±msÄ±zlÄ±k varsayÄ±mÄ±nÄ±n Ã¶tesine geÃ§en, Ã¶zellikler arasÄ±ndaki **baÄŸÄ±mlÄ±lÄ±klarÄ±** modelleyebilen daha geliÅŸmiÅŸ olasÄ±lÄ±ksal grafik modellerdir.
* **Ã–nemi:** Ã–zellikler arasÄ±ndaki nedensel iliÅŸkileri (YÃ¶nlendirilmiÅŸ Asiklik Grafikler - DAG) kullanarak daha doÄŸru, ancak daha karmaÅŸÄ±k Ã§Ä±karÄ±mlar saÄŸlar.

---

## âœ‰ï¸ Ã–rnek Uygulama: E-posta Spam Tespiti (Multinomial NB)

**AmaÃ§:** "Ãœcretsiz para" e-postasÄ±nÄ± sÄ±nÄ±flandÄ±rmak.

### 1. Ã–nsel OlasÄ±lÄ±klar (Prior)

| SÄ±nÄ±f | SayÄ±m | $P(C)$ |
| :--- | :--- | :--- |
| Spam | 3 | $3/5 = 0.6$ |
| Ham | 2 | $2/5 = 0.4$ |

### 2. Olabilirlikler (Likelihood) - Laplace YumuÅŸatmasÄ± UygulanmÄ±ÅŸ

KullanÄ±lan kelimeler: "Ãœcretsiz" ve "Para".

* $P(\text{Ãœcretsiz}|\text{Spam}) = 3/17$
* $P(\text{Para}|\text{Spam}) = 2/17$

* $P(\text{Ãœcretsiz}|\text{Ham}) = 1/14$
* $P(\text{Para}|\text{Ham}) = 1/14$

### 3. Sonsal OlasÄ±lÄ±k HesabÄ±

MAP kuralÄ±nÄ± uygulayarak:

* **Spam PuanÄ±:** $P(\text{Spam}|D) \propto P(\text{Ãœcretsiz}|\text{Spam}) \cdot P(\text{Para}|\text{Spam}) \cdot P(\text{Spam})$
    * Spam PuanÄ± $\propto (3/17) \cdot (2/17) \cdot 0.6 \approx 0.0124$

* **Ham PuanÄ±:** $P(\text{Ham}|D) \propto P(\text{Ãœcretsiz}|\text{Ham}) \cdot P(\text{Para}|\text{Ham}) \cdot P(\text{Ham})$
    * Ham PuanÄ± $\propto (1/14) \cdot (1/14) \cdot 0.4 \approx 0.0020$

**SonuÃ§:** $0.0124 > 0.0020$ olduÄŸu iÃ§in model, e-postayÄ± **SPAM** olarak sÄ±nÄ±flandÄ±rÄ±r. âœ…

---
---

<img width="930" height="347" alt="image" src="https://github.com/user-attachments/assets/ebf529b1-ea70-4f52-b2cc-874a6a8607f4" />

## â“ OlasÄ±lÄ±k Problemi Ã‡Ã¶zÃ¼mÃ¼: Yetersiz Bilgi

Bu, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ± (Probability of Union)** hesaplama problemidir. Temel kural, iki olayÄ±n **kesiÅŸiminin ($P(A \text{ ve } B)$)** bilinmesini gerektirir.

### ğŸ’Š Deney Verileri

* **Toplam Hasta SayÄ±sÄ±:** 100
* **BaÅŸ AÄŸrÄ±sÄ± YaÅŸayanlar (Headache - H):** 50
* **AteÅŸ YaÅŸayanlar (Fever - F):** 50

**OlasÄ±lÄ±klar (Prior):**
* $P(H) = \frac{50}{100} = 0.5$
* $P(F) = \frac{50}{100} = 0.5$

### ğŸ¯ Ä°stenen OlasÄ±lÄ±k

Doktorlar, bir hastanÄ±n **BaÅŸ AÄŸrÄ±sÄ± VEYA AteÅŸ** yaÅŸama olasÄ±lÄ±ÄŸÄ±nÄ±, yani $\mathbf{P(H \cup F)}$'i bulmak istiyor.

### ğŸ“ OlasÄ±lÄ±klarÄ±n BirleÅŸim KuralÄ± (The Addition Rule)

Genel kural ÅŸudur:
$$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

Bu kuralÄ± uygulamak iÃ§in $\mathbf{P(\text{BaÅŸ AÄŸrÄ±sÄ± ve AteÅŸ})}$ terimine ihtiyacÄ±mÄ±z var. Bu, aynÄ± anda hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ yaÅŸayan hastalarÄ±n oranÄ±nÄ± ifade eder. Soruda bu **kesiÅŸim olasÄ±lÄ±ÄŸÄ±** (yani kaÃ§ hastanÄ±n Ã§ifte semptom yaÅŸadÄ±ÄŸÄ±) **verilmemiÅŸtir**.

### âŒ GeÃ§ersiz VarsayÄ±mlarÄ±n Ä°ncelenmesi

| VarsayÄ±m | Kural | Neden GeÃ§ersiz? |
| :--- | :--- | :--- |
| **AyrÄ±k Olaylar** (Mutually Exclusive) | $P(A \text{ veya } B) = P(A) + P(B) = 1$ | HiÃ§bir hastanÄ±n iki semptomu birden yaÅŸamadÄ±ÄŸÄ± varsayÄ±lÄ±r. Soruda bu bilgi **yoktur**. |
| **BaÄŸÄ±msÄ±z Olaylar** (Independent) | $P(A \text{ ve } B) = P(A) \cdot P(B) = 0.25$ | TÄ±bbi semptomlar genellikle **baÄŸÄ±mlÄ±dÄ±r**. Ä°laÃ§, semptomlardan birini tetikleyebilir veya engelleyebilir. Bu varsayÄ±mÄ±n doÄŸru olduÄŸu **garanti edilemez**. |

### ğŸ›‘ SonuÃ§

$\mathbf{P(\text{H ve F})}$ deÄŸeri (kesiÅŸim) bilinmediÄŸi sÃ¼rece, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ±** doÄŸru bir ÅŸekilde hesaplanamaz.

* **Ek Bilgi Gereksinimi:** KaÃ§ hasta **sadece** baÅŸ aÄŸrÄ±sÄ±, kaÃ§ hasta **sadece** ateÅŸ ve kaÃ§ hasta **hem** baÅŸ aÄŸrÄ±sÄ± **hem de** ateÅŸ yaÅŸadÄ±?

**DoÄŸru Ä°fade:**
> **Not enough information is given to calculate $P(\text{fever or headache})$.**
> (BaÅŸ aÄŸrÄ±sÄ± veya ateÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± hesaplamak iÃ§in yeterli bilgi verilmemiÅŸtir.)
>
> ## ğŸ“Œ AÃ§Ä±klamanÄ±n Ã–zeti: Neden Yetersiz Bilgi?

AÃ§Ä±klama, temel olasÄ±lÄ±k kurallarÄ±ndan yola Ã§Ä±karak ÅŸunlarÄ± sÃ¶ylÃ¼yor:

1.  **Olaylar OrtaktÄ±r (Joint Events) ğŸ¤:** Problem metninde, iki olayÄ±n (**baÅŸ aÄŸrÄ±sÄ±** ve **ateÅŸ**) **ayrÄ±k (disjoint)** olduÄŸu, yani aynÄ± anda gerÃ§ekleÅŸemeyeceÄŸi sÃ¶ylenmiyor. DolayÄ±sÄ±yla, bazÄ± kiÅŸilerin **hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ** yaÅŸama ihtimali vardÄ±r. Bu tÃ¼r Ã§akÄ±ÅŸan olaylara **ortak olaylar (joint events)** denir.

2.  **KesiÅŸim Bilinmeli â“:** Ä°ki olayÄ±n birleÅŸim olasÄ±lÄ±ÄŸÄ±nÄ± ($P(A \text{ veya } B)$) hesaplamak iÃ§in genel **Toplama KuralÄ± (Addition Rule)**'nÄ± kullanmak zorunludur:
    $$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

3.  **SonuÃ§ ğŸ›‘:** Bu nedenle, $P(\text{ateÅŸ veya baÅŸ aÄŸrÄ±sÄ±})$ olasÄ±lÄ±ÄŸÄ±nÄ± bulmak iÃ§in, olaylarÄ±n kesiÅŸim olasÄ±lÄ±ÄŸÄ± olan **$P(\text{ateÅŸ VE baÅŸ aÄŸrÄ±sÄ±})$** deÄŸerini **bilmeniz gerekir**. Bu bilgi problem metninde verilmediÄŸi iÃ§in, olasÄ±lÄ±k hesaplanamaz.

---

<img width="870" height="357" alt="image" src="https://github.com/user-attachments/assets/f76afc36-6297-45df-9f2a-011289ddd967" />


## ğŸ KoÅŸullu OlasÄ±lÄ±k Ã‡Ã¶zÃ¼mÃ¼: YazÄ±lÄ±m Testi

Bu problem, **Bayes Teoremi'nin temelini oluÅŸturan** bir **KoÅŸullu OlasÄ±lÄ±k (Conditional Probability)** problemidir. Bir kullanÄ±cÄ±nÄ±n hata deneyimlediÄŸi bilgisi **verildiÄŸinde**, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±nÄ± bulmayÄ± amaÃ§lar.

### ğŸ“ 1. Olay TanÄ±mlarÄ± ve FormÃ¼l

* **B:** KullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmesi.
* **H:** KullanÄ±cÄ±nÄ±n **Hata (Bug)** deneyimlemesi.

Aranan olasÄ±lÄ±k: $P(B|H)$ (Hata deneyimlediÄŸine gÃ¶re, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±).

$$\mathbf{P(B|H)} = \frac{P(B \cap H)}{P(H)}$$

### ğŸ“Š 2. Verilerin Ã–zetlenmesi

Toplam kullanÄ±cÄ± sayÄ±sÄ±: $4000 + 5000 = \mathbf{9000}$

| Kategori | DeÄŸer | Olay SayÄ±sÄ± (N) | OlasÄ±lÄ±k (P) |
| :--- | :--- | :--- | :--- |
| **Versiyon B KullanÄ±cÄ±larÄ±** | 5000 | $N(B)$ | $P(B) = 5000/9000$ |
| **Toplam Hata Deneyimleyenler** | 3000 | $N(H)$ | $P(H) = 3000/9000$ |
| **Versiyon B ve Hata KesiÅŸimi** | 1500 | $N(B \cap H)$ | $P(B \cap H) = 1500/9000$ |

### ğŸ§  3. Hesaplama (KullanÄ±cÄ± SayÄ±larÄ±yla)

OlasÄ±lÄ±klar yerine, hesaplamanÄ±n daha basit olmasÄ± iÃ§in doÄŸrudan **kullanÄ±cÄ± sayÄ±larÄ±nÄ±** kullanabiliriz, Ã§Ã¼nkÃ¼ payda ($N(\text{Toplam})$) sadeleÅŸecektir:

$$\mathbf{P(B|H)} = \frac{N(\text{B ve H})}{N(H)} = \frac{\text{Versiyon B ile hata deneyimleyenler}}{\text{Toplam hata deneyimleyenler}}$$

$$P(B|H) = \frac{1500}{3000}$$

$$P(B|H) = \frac{1}{2} = \mathbf{0.5}$$

---

### âœ… SonuÃ§

Bir kullanÄ±cÄ±nÄ±n **hata deneyimlediÄŸi** bilgisi verildiÄŸinde, bu kullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmiÅŸ olma olasÄ±lÄ±ÄŸÄ± $\mathbf{1/2}$ (**%50**) olarak bulunur.


---

<img width="966" height="326" alt="image" src="https://github.com/user-attachments/assets/c8262fec-89f0-4c5b-ac2f-3c45b1eb903f" />

# ğŸ”¬ Bayes Teoremi UygulamasÄ±: TÄ±bbi Test Analizi

Bu, bir kiÅŸinin test sonucu pozitif Ã§Ä±ktÄ±ÄŸÄ±nda **gerÃ§ekte hasta olma olasÄ±lÄ±ÄŸÄ±nÄ± (Sonsal OlasÄ±lÄ±k)** hesaplayan klasik bir Bayes Teoremi problemidir.

### ğŸ¯ Aranan OlasÄ±lÄ±k

Ä°stenen olasÄ±lÄ±k, testin pozitif Ã§Ä±ktÄ±ÄŸÄ± bilgisi **verildiÄŸinde** kiÅŸinin hasta olma olasÄ±lÄ±ÄŸÄ±dÄ±r: $\mathbf{P(\text{hasta} | \text{test poz.})}$.

**Bayes Teoremi FormÃ¼lÃ¼:**
$$P(\text{hasta} | \text{test poz.}) = \frac{P(\text{test poz.} | \text{hasta}) \cdot P(\text{hasta})}{P(\text{test poz.})}$$

---

### 1. Parametrelerin TanÄ±mlanmasÄ± ve GiriÅŸ Verileri

| Veri | TanÄ±m | DeÄŸer |
| :--- | :--- | :--- |
| **Ã–nsel OlasÄ±lÄ±k** | $P(\text{hasta})$ (HastalÄ±k yaygÄ±nlÄ±ÄŸÄ±) | $1\% = \mathbf{0.01}$ |
| **Olabilirlik (True Positive)** | $P(\text{test poz.} | \text{hasta})$ (Hasta iken testin pozitif Ã§Ä±kmasÄ±) | $95\% = \mathbf{0.95}$ |
| **True Negative** | $P(\text{test neg.} | \text{saÄŸlÄ±klÄ±})$ (SaÄŸlÄ±klÄ± iken testin negatif Ã§Ä±kmasÄ±) | $90\% = \mathbf{0.90}$ |

---

### 2. Eksik OlasÄ±lÄ±klarÄ±n HesaplanmasÄ± (TÃ¼mleyen KuralÄ±)

Bayes formÃ¼lÃ¼nÃ¼ tamamlamak iÃ§in gerekli deÄŸerler:

#### A) SaÄŸlÄ±klÄ± Olma OlasÄ±lÄ±ÄŸÄ± ($P(\text{saÄŸlÄ±klÄ±})$)
$$P(\text{saÄŸlÄ±klÄ±}) = 1 - P(\text{hasta}) = 1 - 0.01 = \mathbf{0.99}$$

#### B) YanlÄ±ÅŸ Pozitif OlasÄ±lÄ±ÄŸÄ± ($P(\text{test poz.} | \text{saÄŸlÄ±klÄ±})$)
SaÄŸlÄ±klÄ± bir kiÅŸide testin pozitif Ã§Ä±kma olasÄ±lÄ±ÄŸÄ± (False Positive):
$$P(\text{test poz.} | \text{saÄŸlÄ±klÄ±}) = 1 - P(\text{test neg.} | \text{saÄŸlÄ±klÄ±}) = 1 - 0.90 = \mathbf{0.10}$$

---

### 3. KanÄ±tÄ±n HesaplanmasÄ± ($P(\text{test poz.})$)

KanÄ±t (payda), hem doÄŸru pozitifler hem de yanlÄ±ÅŸ pozitifler dahil olmak Ã¼zere **tÃ¼m pozitif test sonuÃ§larÄ±nÄ±n** toplam olasÄ±lÄ±ÄŸÄ±dÄ±r (Toplam OlasÄ±lÄ±k KuralÄ±):

$$P(\text{test poz.}) = \big(P(\text{test poz.} | \text{hasta}) \cdot P(\text{hasta})\big) + \big(P(\text{test poz.} | \text{saÄŸlÄ±klÄ±}) \cdot P(\text{saÄŸlÄ±klÄ±})\big)$$

$$P(\text{test poz.}) = (0.95 \cdot 0.01) + (0.10 \cdot 0.99)$$$$P(\text{test poz.}) = 0.0095 + 0.0990$$$$\mathbf{P(\text{test poz.})} = \mathbf{0.1085}$$

---

### 4. Sonsal OlasÄ±lÄ±ÄŸÄ±n HesaplanmasÄ± (Final)

TÃ¼m deÄŸerleri Bayes Teoremi formÃ¼lÃ¼ne yerleÅŸtirelim:

$$P(\text{hasta} | \text{test poz.}) = \frac{0.0095}{0.1085}$$

$$P(\text{hasta} | \text{test poz.}) \approx \mathbf{0.08755}$$

### âœ… Nihai SonuÃ§ ve Yorum

Test sonucu pozitif Ã§Ä±kan bir kiÅŸinin **gerÃ§ekten hasta olma olasÄ±lÄ±ÄŸÄ±** yaklaÅŸÄ±k olarak **%8.76**'dÄ±r ($\mathbf{0.0876}$).

Bu dÃ¼ÅŸÃ¼k sonuÃ§ ÅŸaÅŸÄ±rtÄ±cÄ±dÄ±r ve temel olarak ÅŸundan kaynaklanÄ±r:
> **HastalÄ±k nadirdir.** PopÃ¼lasyonun sadece %1'i hastadÄ±r. Test oldukÃ§a doÄŸru olsa da, Ã§ok sayÄ±da saÄŸlÄ±klÄ± insanÄ±n yanlÄ±ÅŸ pozitif ($0.10 \cdot 0.99 = 0.0990$) sonuÃ§ vermesi, doÄŸru pozitif sonuÃ§lardan ($0.95 \cdot 0.01 = 0.0095$) sayÄ±ca Ã§ok daha fazladÄ±r.
> 

---
---

<img width="966" height="582" alt="image" src="https://github.com/user-attachments/assets/a5111561-ea89-40d6-80df-f1ed8bdea4b9" />

# ğŸ² Binom DaÄŸÄ±lÄ±mÄ± (Binomial Distribution) Problemi Ã‡Ã¶zÃ¼mÃ¼

Bu problem, sabit sayÄ±da denemeden (20 zar atÄ±ÅŸÄ±), belirli sayÄ±da (7 kez) baÅŸarÄ± elde etme olasÄ±lÄ±ÄŸÄ±nÄ± hesapladÄ±ÄŸÄ±mÄ±z klasik bir **Binom DaÄŸÄ±lÄ±mÄ±** Ã¶rneÄŸidir.

### ğŸ“ Binom DaÄŸÄ±lÄ±mÄ± FormÃ¼lÃ¼

$n$ denemeden $k$ kez baÅŸarÄ± elde etme olasÄ±lÄ±ÄŸÄ±:

$$P(X=k) = C(n, k) \cdot p^k \cdot (1-p)^{n-k}$$

---

### 1. Problemin Parametreleri

| Parametre | TanÄ±m | DeÄŸer |
| :--- | :--- | :--- |
| **$n$** (Deneme SayÄ±sÄ±) | Zar atÄ±ÅŸÄ± sayÄ±sÄ± | $\mathbf{20}$ |
| **$k$** (Ä°stenen BaÅŸarÄ±) | "4" gelme sayÄ±sÄ± | $\mathbf{7}$ |
| **$p$** (BaÅŸarÄ± OlasÄ±lÄ±ÄŸÄ±) | Bir atÄ±ÅŸta '4' gelme olasÄ±lÄ±ÄŸÄ± | $\mathbf{1/6}$ |
| **$1-p$** (BaÅŸarÄ±sÄ±zlÄ±k OlasÄ±lÄ±ÄŸÄ±) | '4' dÄ±ÅŸÄ±nda bir sayÄ± gelme olasÄ±lÄ±ÄŸÄ± | $1 - 1/6 = \mathbf{5/6}$ |
| **$n-k$** (BaÅŸarÄ±sÄ±zlÄ±k SayÄ±sÄ±) | BaÅŸarÄ±sÄ±z atÄ±ÅŸ sayÄ±sÄ± | $20 - 7 = \mathbf{13}$ |

---

### 2. FormÃ¼lÃ¼n UygulanmasÄ±

Bulunan deÄŸerler formÃ¼lde yerine konur. $C(20, 7)$ ifadesi, kombinasyon gÃ¶sterimi olan $\binom{20}{7}$ ÅŸeklinde yazÄ±lÄ±r:

$$P(X=7) = \binom{20}{7} \cdot \left(\frac{1}{6}\right)^7 \cdot \left(\frac{5}{6}\right)^{13}$$

### âœ… DoÄŸru SeÃ§enek

Bu matematiksel ifadeye karÅŸÄ±lÄ±k gelen seÃ§enek, **dÃ¶rdÃ¼ncÃ¼ (alttan birinci) seÃ§enektir**.

$$P(X=7) = \binom{20}{7} \cdot \left(\frac{1}{6}\right)^7 \cdot \left(\frac{5}{6}\right)^{13}$$

---
---

<img width="763" height="616" alt="image" src="https://github.com/user-attachments/assets/f69d8cc5-702d-47bc-9460-cc0addb2f6fd" />

# ğŸš• AyrÄ±k OlasÄ±lÄ±k Ã‡Ã¶zÃ¼mÃ¼: KÃ¼mÃ¼latif DaÄŸÄ±lÄ±m

Bu, bir taksi yolculuÄŸunda yolcu sayÄ±sÄ±nÄ±n ($X$) **3 veya daha az** olma olasÄ±lÄ±ÄŸÄ±nÄ± hesaplayan bir **AyrÄ±k OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ±** problemidir. Ä°stenen olasÄ±lÄ±k $\mathbf{P(X \le 3)}$'tÃ¼r.

### ğŸ“ Ã‡Ã¶zÃ¼m: KÃ¼mÃ¼latif OlasÄ±lÄ±k Hesaplama

$P(X \le 3)$ olasÄ±lÄ±ÄŸÄ±, $X$'in 0, 1, 2 veya 3 olduÄŸu ayrÄ± olasÄ±lÄ±klarÄ±n toplamÄ±na eÅŸittir:

$$P(X \le 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3)$$

### 1. Tablodan OlasÄ±lÄ±klarÄ± Alma

| Yolcu SayÄ±sÄ± ($x_i$) | OlasÄ±lÄ±k ($p_i$) |
| :--- | :--- |
| $X=0$ | $0.10$ |
| $X=1$ | $0.25$ |
| $X=2$ | $0.25$ |
| $X=3$ | $0.15$ |

### 2. OlasÄ±lÄ±klarÄ± Toplama

Ä°stenen kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± bulmak iÃ§in deÄŸerleri toplarÄ±z:

$$P(X \le 3) = 0.10 + 0.25 + 0.25 + 0.15$$

$$\mathbf{P(X \le 3)} = \mathbf{0.75}$$

### âœ… SonuÃ§

Rastgele seÃ§ilen bir taksi yolculuÄŸunda yolcu sayÄ±sÄ±nÄ±n 3 veya daha az olma olasÄ±lÄ±ÄŸÄ± $\mathbf{0.75}$'tir (veya **%75**).

---
---

<img width="827" height="529" alt="image" src="https://github.com/user-attachments/assets/c48b6a1e-6605-4e0e-a500-d76c386a4d64" />

<img width="674" height="524" alt="image" src="https://github.com/user-attachments/assets/4c938733-bd84-49ce-b1d1-f1ebfef7b961" />

# ğŸ“‰ DÃ¶rt Normal DaÄŸÄ±lÄ±mÄ±n (Gaussian) KarÅŸÄ±laÅŸtÄ±rmalÄ± Analizi

Bu analiz, dÃ¶rt farklÄ± Normal DaÄŸÄ±lÄ±mÄ±n (Ã‡an EÄŸrisi) iki temel parametresi olan **Ortalama ($\mu$)** ve **Standart Sapma ($\sigma$)** deÄŸerlerinin grafikte nasÄ±l yorumlandÄ±ÄŸÄ±nÄ± gÃ¶sterir.

### ğŸ§  Normal DaÄŸÄ±lÄ±m Parametrelerinin YorumlanmasÄ±

* **Ortalama ($\mu$) Konumu:** EÄŸrinin **en yÃ¼ksek noktasÄ± (tepesi)** $\mu$ deÄŸerine denk gelir.
* **Standart Sapma ($\sigma$) YayÄ±lÄ±mÄ±:** EÄŸri ne kadar **dar ve uzunsa**, $\sigma$ o kadar **kÃ¼Ã§Ã¼ktÃ¼r** (dÃ¼ÅŸÃ¼k varyans). EÄŸri ne kadar **geniÅŸ ve alÃ§aksa**, $\sigma$ o kadar **bÃ¼yÃ¼ktÃ¼r** (yÃ¼ksek varyans).

---

### 1. DaÄŸÄ±lÄ±mlarÄ±n DetaylÄ± Analizi Tablosu

| DaÄŸÄ±lÄ±m | Renk/Ã‡izgi Tipi | Ortalama ($\mu$) Yorumu (Tepe NoktasÄ±) | Standart Sapma ($\sigma$) Yorumu (GeniÅŸlik/YÃ¼kseklik) |
| :--- | :--- | :--- | :--- |
| **normal\_A** | Mavi, DÃ¼z | $\mathbf{x \approx -2.5}$ civarÄ±ndadÄ±r. | En **dar** ve en **yÃ¼ksek** eÄŸridir. **En kÃ¼Ã§Ã¼k $\sigma$** deÄŸerine sahiptir. |
| **normal\_B** | Turuncu, Kesik | $\mathbf{x \approx 0}$ civarÄ±ndadÄ±r. | normal\_A'dan daha geniÅŸ, ancak diÄŸerlerinden kÃ¼Ã§Ã¼ktÃ¼r. |
| **normal\_C** | YeÅŸil, Nokta-Kesik | $\mathbf{x \approx 1.5}$ civarÄ±ndadÄ±r. | normal\_B'den daha geniÅŸtir. |
| **normal\_D** | KÄ±rmÄ±zÄ±, NoktalÄ± | $\mathbf{x \approx 4}$ civarÄ±ndadÄ±r. | En **geniÅŸ** ve en **alÃ§ak** eÄŸridir. **En bÃ¼yÃ¼k $\sigma$** deÄŸerine sahiptir. |

---

### 2. SÄ±ralamalarÄ±n Ã–zeti

| Parametre | SÄ±ralama (KÃ¼Ã§Ã¼kten BÃ¼yÃ¼ÄŸe) |
| :--- | :--- |
| **Ortalama ($\mu$)** | $\mu_{\text{normal\_A}} < \mu_{\text{normal\_B}} < \mu_{\text{normal\_C}} < \mu_{\text{normal\_D}}$ |
| **Standart Sapma ($\sigma$)** | $\sigma_{\text{normal\_A}} < \sigma_{\text{normal\_B}} < \sigma_{\text{normal\_C}} < \sigma_{\text{normal\_D}}$ |

---

### 3. âœ… DoÄŸru Ä°fadelerin SeÃ§imi

Bu sÄ±ralamalara gÃ¶re, grafiÄŸi doÄŸru yorumlayan ifadeler ÅŸunlardÄ±r:

* $$\mathbf{\sigma_{\text{normal\_D}} > \sigma_{\text{normal\_A}}}$$
    (En geniÅŸ olan D, en dar olan A'dan bÃ¼yÃ¼ktÃ¼r.)

* $$\mathbf{\mu_{\text{normal\_D}} > \mu_{\text{normal\_C}}}$$
    (En saÄŸdaki D, C'den daha bÃ¼yÃ¼k ortalamaya sahiptir.)

* $$\mathbf{\sigma_{\text{normal\_C}} > \sigma_{\text{normal\_B}}}$$
    (C, B'den daha yayvan/geniÅŸ bir daÄŸÄ±lÄ±ma sahiptir.)


---
<img width="1199" height="587" alt="image" src="https://github.com/user-attachments/assets/f66e93f1-f141-456d-8302-054b7308bc2f" />

### Fayda: DaÄŸÄ±lÄ±mÄ±n StandartlaÅŸtÄ±rÄ±lmasÄ±nÄ±n FaydalarÄ±

| Benefit (Concept) | Explanation / Impact |
| :--- | :--- |
| **Standard Scale Transformation** ğŸ“ | It transforms datasets into a standard scale, making it easier to compare between different datasets. |
| **Statistical Simplification** ğŸ“Š | It simplifies statistical analysis, particularly when using techniques that assume a standard normal distribution. |
| **Machine Learning Performance** ğŸš€ | Standardizing features in machine learning can improve the convergence rate of optimization algorithms and prevent some features from dominating others, leading to improved model performance. |
---

<img width="717" height="243" alt="image" src="https://github.com/user-attachments/assets/df2a2ea2-7d39-49a5-a0e9-afaebd94f442" />

### Kurtosis Analizi: Game A vs. Game B DaÄŸÄ±lÄ±m KarÅŸÄ±laÅŸtÄ±rmasÄ± âš–ï¸

Kurtosis, bir daÄŸÄ±lÄ±mÄ±n **kuyruklarÄ±nÄ±n aÄŸÄ±rlÄ±ÄŸÄ±nÄ±** ve aÅŸÄ±rÄ± uÃ§ deÄŸerlere sahip olma eÄŸilimini Ã¶lÃ§er. YÃ¼ksek Kurtosis, daha aÄŸÄ±r kuyruklar demektir.

| Kriter (Criterion) | Game A Analizi (Game A Analysis) | Game B Analizi (Game B Analysis) |
| :--- | :--- | :--- |
| **Kazanma/Kaybetme DeÄŸerleri** | $\{-1, +2\}$ | $\{-2, -0.50, +0.50, +5\}$ |
| **AralÄ±k (Range)** | KÃ¼Ã§Ã¼k aralÄ±k ($\text{Range} = 3$ birim). | BÃ¼yÃ¼k aralÄ±k ($\text{Range} = 7$ birim). |
| **UÃ§ DeÄŸerler (Extremes)** | TÃ¼m olasÄ± sonuÃ§lar birbirine nispeten yakÄ±ndÄ±r. | Ã‡ok daha **uÃ§ sonuÃ§lar** ($\text{+5}$ ve $\text{-2}$) mevcuttur. |
| **Kuyruk AÄŸÄ±rlÄ±ÄŸÄ± (Tail Weight)** | Kuyruklar **hafif** olma eÄŸilimindedir. | Kuyruklar **aÄŸÄ±r** olma eÄŸilimindedir (daha fazla aÅŸÄ±rÄ± deÄŸer olasÄ±lÄ±ÄŸÄ±). |
| **Kurtosis Sonucu** | Daha kÃ¼Ã§Ã¼ktÃ¼r ($\text{Smaller}$). | Daha bÃ¼yÃ¼ktÃ¼r ($\text{Larger}$). |

#### Nihai SonuÃ§ (Final Conclusion)

<img width="742" height="406" alt="image" src="https://github.com/user-attachments/assets/f7b0f71d-02a3-45e1-8d98-b306052c9c88" />


Game B, Game A'ya kÄ±yasla daÄŸÄ±lÄ±mÄ±nÄ±n merkezinden Ã§ok daha uzakta yer alan bÃ¼yÃ¼k uÃ§ deÄŸerlere sahip olduÄŸu iÃ§in, **Game B'nin kurtosis deÄŸeri, Game A'nÄ±n kurtosis deÄŸerinden daha bÃ¼yÃ¼ktÃ¼r.**

$$\text{Game A's kurtosis is smaller than Game B's kurtosis.}$$
$$\text{Kurtosis}(\text{A}) < \text{Kurtosis}(\text{B})$$

---

<img width="521" height="455" alt="image" src="https://github.com/user-attachments/assets/979a8b6f-a4af-4fcb-b5ef-b1f61ba89819" />

### BaÄŸÄ±msÄ±z Normal DaÄŸÄ±lÄ±mlÄ± DeÄŸiÅŸkenlerin ToplamÄ± Analizi â•

**Verilenler (Given):**
* $X \sim \text{Normal}(3, 1^2)$
* $Y \sim \text{Normal}(2, 2^2)$

**Kural (Rule):** $X$ ve $Y$ baÄŸÄ±msÄ±z ise, $Z = X + Y$ de Normal daÄŸÄ±lÄ±m izler: $Z \sim \text{Normal}(\mu_Z, \sigma_Z^2)$.

---

#### 1. Ortalama ($\mu_Z$) HesaplanmasÄ± (Mean Calculation)

BaÄŸÄ±msÄ±z deÄŸiÅŸkenlerin ortalamasÄ±, bireysel ortalamalarÄ±n toplamÄ±dÄ±r:
$$\mu_Z = \mu_X + \mu_Y$$

| DeÄŸiÅŸken | Ortalama ($\mu$) |
| :---: | :---: |
| $X$ | $\mu_X = 3$ |
| $Y$ | $\mu_Y = 2$ |

$$\mu_Z = 3 + 2 = 5$$

---

#### 2. Varyans ($\sigma_Z^2$) HesaplanmasÄ± (Variance Calculation)

BaÄŸÄ±msÄ±z deÄŸiÅŸkenlerin varyansÄ±, bireysel varyanslarÄ±n toplamÄ±dÄ±r:
$$\sigma_Z^2 = \sigma_X^2 + \sigma_Y^2$$

| DeÄŸiÅŸken | Varyans ($\sigma^2$) |
| :---: | :---: |
| $X$ | $\sigma_X^2 = 1^2 = 1$ |
| $Y$ | $\sigma_Y^2 = 2^2 = 4$ |

$$\sigma_Z^2 = 1 + 4 = 5$$

---

#### 3. Standart Sapma ($\sigma_Z$) HesaplanmasÄ± (Standard Deviation)

Standart sapma, varyansÄ±n karekÃ¶kÃ¼dÃ¼r:
$$\sigma_Z = \sqrt{\sigma_Z^2}$$

$$\sigma_Z = \sqrt{5}$$

---

#### SonuÃ§ (Final Result)

$Z = X + Y$ deÄŸiÅŸkeni, $\text{Normal}(\mu, \sigma^2)$ daÄŸÄ±lÄ±mÄ±na sahiptir:
$$\mu = 5, \quad \sigma = \sqrt{5}$$

---

<img width="680" height="646" alt="image" src="https://github.com/user-attachments/assets/8995caa0-e725-45a3-a22e-c8194d872070" />

### Kutu GrafiÄŸi Analizi: Class A vs. Class B Test SkorlarÄ± ğŸ“Š

Bu analizde, medyan (merkez) ve Ã§eyrekler arasÄ± aralÄ±k (yayÄ±lÄ±m) karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

| Ä°statistiksel Ã–lÃ§Ã¼ | TanÄ±m (Definition) | Class A DeÄŸerleri | Class B DeÄŸerleri |
| :--- | :--- | :--- | :--- |
| **Medyan (Median)** ğŸ”´ | Kutunun iÃ§indeki yatay kÄ±rmÄ±zÄ± Ã§izgidir. | YaklaÅŸÄ±k $\mathbf{74}$ | YaklaÅŸÄ±k $\mathbf{85}$ |
| **IQR (Ã‡eyrekler ArasÄ± AralÄ±k)** ğŸ“ | Kutunun yÃ¼ksekliÄŸidir ($\text{Q3} - \text{Q1}$). | $\text{Q3} \approx 79, \text{Q1} \approx 60 \implies \text{IQR} \approx \mathbf{19}$ | $\text{Q3} \approx 90, \text{Q1} \approx 80 \implies \text{IQR} \approx \mathbf{10}$ |

---

#### Temel SonuÃ§lar (Key Findings)

* **Medyan KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Class B'nin medyan skoru ($\mathbf{85}$), Class A'nÄ±n medyan skorundan ($\mathbf{74}$) **daha yÃ¼ksektir**. ($\text{Class B}$ daha iyi bir merkezi eÄŸilime sahiptir.)
* **IQR KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Class A'nÄ±n IQR'Ä± ($\mathbf{19}$), Class B'nin IQR'Ä±ndan ($\mathbf{10}$) **daha bÃ¼yÃ¼ktÃ¼r**. ($\text{Class A}$'nÄ±n skorlarÄ± daha fazla yayÄ±lmÄ±ÅŸtÄ±r/daÄŸÄ±nÄ±ktÄ±r.)

### Kutu GrafiÄŸi Analizi: Ä°fadelerin KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± (Comparison of Statements) ğŸ¯

AÅŸaÄŸÄ±daki tablo, Class A ve Class B iÃ§in hesaplanan Medyan ve IQR deÄŸerlerine gÃ¶re verilen ifadelerin doÄŸruluÄŸunu kontrol etmektedir.

| Ä°fade (Statement) | DeÄŸerler (Values) | SonuÃ§ (Result) |
| :--- | :--- | :--- |
| **Class A's median score is higher than Class B's median score.** | $74 > 85$ | YanlÄ±ÅŸ (False) âŒ |
| **Class B's interquartile range (IQR) is larger than Class A's interquartile range.** | $10 > 19$ | YanlÄ±ÅŸ (False) âŒ |
| **Class A's interquartile range (IQR) is larger than Class B's interquartile range.** | $19 > 10$ | **DoÄŸru (True)** âœ… |
| **Class B's median score is higher than Class A's median score.** | $85 > 74$ | **DoÄŸru (True)** âœ… |

#### Analiz Ã–zeti

GrafiÄŸe gÃ¶re, **Class B** daha yÃ¼ksek bir merkezi eÄŸilime (Medyan = 85) sahipken, **Class A** daha bÃ¼yÃ¼k bir yayÄ±lÄ±ma ($\text{IQR} = 19$) sahiptir.

### Kutu GrafiÄŸi SonuÃ§larÄ±: DoÄŸru Ä°fadeler (Final Box Plot Conclusions) âœ…

Analiz sonucunda, verilen dÃ¶rt seÃ§enek arasÄ±ndan **iki ifadenin** doÄŸru olduÄŸu belirlenmiÅŸtir. Bu durum, veri setinin hem merkezindeki hem de yayÄ±lÄ±mÄ±ndaki farklÄ±lÄ±klarÄ± yansÄ±tmaktadÄ±r.

| Ä°fade (Statement) | Dayanak (Evidence) | SonuÃ§ (Result) |
| :--- | :--- | :--- |
| **Class A's interquartile range (IQR) is larger than Class B's interquartile range.** | $\text{IQR(A): 19} \quad > \quad \text{IQR(B): 10}$ | **DoÄŸru (True)** |
| **Class B's median score is higher than Class A's median score.** | $\text{Median(B): 85} \quad > \quad \text{Median(A): 74}$ | **DoÄŸru (True)** |

---

#### Ã–zet (Summary)

Class B'nin **daha yÃ¼ksek bir performansa** (daha yÃ¼ksek medyan) sahip olduÄŸu, ancak Class A'nÄ±n skorlarÄ±nÄ±n **daha daÄŸÄ±nÄ±k** (daha bÃ¼yÃ¼k IQR) olduÄŸu sonucuna varÄ±lmÄ±ÅŸtÄ±r.

---

<img width="560" height="599" alt="image" src="https://github.com/user-attachments/assets/d7d014ed-90f3-4423-b5c5-f7d020806d74" />

### QQ Plot Analizi: Normal DaÄŸÄ±lÄ±mÄ±n DeÄŸerlendirilmesi ğŸ“ğŸ“Š

Bu analiz, bir veri setinin **QQ Plot** (Kuantil-Kuantil GrafiÄŸi) kullanÄ±larak Normal ($\text{Gaussian}$) daÄŸÄ±lÄ±ma ne kadar uyduÄŸunu deÄŸerlendirmektedir.

| Kriter (Criterion) | GÃ¶zlem (Observation) | SonuÃ§ (Implication) |
| :--- | :--- | :--- |
| **Genel Kural** | Veri noktalarÄ± dÃ¼z Ã§izgiyi takip ediyorsa, veri Normal daÄŸÄ±lÄ±ma uyar. Veri noktalarÄ± Ã§izgiden sapÄ±yorsa, daÄŸÄ±lÄ±m gÃ¶stermez. | QQ Plot, verinin daÄŸÄ±lÄ±m uyumunu belirlemenin en iyi yoludur. |
| **Merkez BÃ¶lge** ğŸŸ¢ | Veri noktalarÄ±nÄ±n bÃ¼yÃ¼k bir kÄ±smÄ± ($-1$'den $1$'e kadar olan kÄ±sÄ±m), turuncu Ã§izginin Ã¼zerinde Ã§ok yakÄ±ndÄ±r. | Verinin merkezinin Normal daÄŸÄ±lÄ±ma **gÃ¼Ã§lÃ¼ bir ÅŸekilde** uyduÄŸunu gÃ¶sterir. |
| **Kuyruklar (UÃ§lar)** âš ï¸ | Noktalar, hem alt kuyrukta (yaklaÅŸÄ±k $-1$'den sonra) hem de Ã¼st kuyrukta (yaklaÅŸÄ±k $1$'den sonra) Ã§izgiden hafifÃ§e sapmaktadÄ±r. | DaÄŸÄ±lÄ±mÄ±n kusursuz Normal olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir, ancak sapma kÃ¼Ã§Ã¼ktÃ¼r. |
| **Nihai Karar** âœ… | SapmalarÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼, tÃ¼m veri setini "Normal DaÄŸÄ±lmÄ±ÅŸ DeÄŸil" olarak sÄ±nÄ±flandÄ±rmak iÃ§in genellikle yeterli deÄŸildir. | **The data looks normally distributed.** (Veri, Normal daÄŸÄ±lmÄ±ÅŸ gÃ¶rÃ¼nÃ¼yor.) |

---

<img width="1077" height="490" alt="image" src="https://github.com/user-attachments/assets/2d51ea35-a613-4ae1-9d9c-8db533ba4c7b" />

---

<img width="1076" height="492" alt="image" src="https://github.com/user-attachments/assets/4b7028bb-a6b7-4750-b775-4937b2213b2e" />

---

<img width="361" height="149" alt="image" src="https://github.com/user-attachments/assets/dc036e40-7f62-4faf-88c7-6b21cf5e2af5" />

---
<img width="706" height="335" alt="image" src="https://github.com/user-attachments/assets/0b875d7d-3e2a-46c0-a74f-107855a6e3a5" />

### AyrÄ±k OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ±nÄ±n Beklenen DeÄŸeri ($E[X]$) HesaplamasÄ± ğŸ§®

Bu problem, rastgele deÄŸiÅŸken $X$'in alacaÄŸÄ± deÄŸerlerin kendi olasÄ±lÄ±klarÄ± ile Ã§arpÄ±lÄ±p toplanmasÄ±yla bulunan **Beklenen Ortalama** ($\mu$) deÄŸerini hesaplamayÄ± gerektirir.

#### Temel FormÃ¼l (Expected Value Formula)
$$\mu = E[X] = \sum_{i} x_i \cdot P(x_i)$$

---

#### 1. Veri DeÄŸerleri ve OlasÄ±lÄ±klar Tablosu ğŸ”¢

| $X$ DeÄŸeri ($x_i$) | OlasÄ±lÄ±k ($P(x_i)$) |
| :---: | :---: |
| $1$ | $0.3$ |
| $3$ | $0.4$ |
| $5$ | $0.3$ |

---

#### 2. Beklenen DeÄŸer ($E[X]$) HesaplamasÄ±

Her $x_i$ deÄŸerini karÅŸÄ±lÄ±k gelen $P(x_i)$ olasÄ±lÄ±ÄŸÄ± ile Ã§arpÄ±p toplayalÄ±m:

$$E[X] = (1 \cdot 0.3) + (3 \cdot 0.4) + (5 \cdot 0.3)$$

$$E[X] = 0.3 + 1.2 + 1.5$$

$$E[X] = 3.0$$

---

#### SonuÃ§ (Final Result)

Beklenen ortalama ($\mu$) ÅŸuna eÅŸittir:
$$\mu = 3.0$$

---

<img width="709" height="414" alt="image" src="https://github.com/user-attachments/assets/3de533ee-51e0-43d5-89a5-2750bbc2fc8a" />

### Ortak OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ± Analizi: Ã‡ift DeÄŸer OlasÄ±lÄ±ÄŸÄ± ($P(X=\text{even}, Y=\text{even})$) ğŸ²

Bu problem, iki rastgele deÄŸiÅŸkenin ($X$ ve $Y$) **bir arada Ã§ift deÄŸer** almasÄ± olasÄ±lÄ±ÄŸÄ±nÄ± bulmayÄ± gerektirir.

---

#### 1. KoÅŸulun Belirlenmesi (Defining the Condition)

| DeÄŸiÅŸken (Variable) | OlasÄ± DeÄŸerler (Possible Values) | Ã‡ift DeÄŸer (Even Value) |
| :---: | :---: | :---: |
| $X$ | $\{1, 2, 3\}$ | $X=2$ |
| $Y$ | $\{1, 2\}$ | $Y=2$ |

KoÅŸul: $X$ ve $Y$ her ikisi de Ã§ift deÄŸer alsÄ±n.
$$\text{Aranan OlasÄ±lÄ±k: } P(X=2, Y=2)$$

---

#### 2. OlasÄ±lÄ±ÄŸÄ±n Tablodan OkunmasÄ± (Reading the Joint Probability)

Verilen ortak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± tablosu:

| $X \setminus Y$ | $1$ | $2$ | $3$ |
| :---: | :---: | :---: | :---: |
| $1$ | $0.1$ | $0.2$ | $0.3$ |
| $2$ | $0.2$ | **0.1** | $0.1$ |

Tabloda $X=2$ satÄ±rÄ± ile $Y=2$ sÃ¼tununun kesiÅŸimi aranan olasÄ±lÄ±ktÄ±r:

$$P(X=2, Y=2) = 0.1$$

---

#### Nihai SonuÃ§ (Final Result) âœ…

DoÄŸru hesaplama ve tablodan okunan deÄŸer ÅŸudur:

$$\mathbf{P(X=2, Y=2) = 0.1}$$

---

<img width="699" height="354" alt="image" src="https://github.com/user-attachments/assets/43b4b95f-fd08-4fdd-aa5f-fccf8190a6ea" />


### KoÅŸullu OlasÄ±lÄ±k HesaplamasÄ±: $P(X=3 \mid Y=1)$ ğŸ¯

Bu problem, ortak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± tablosu kullanÄ±larak koÅŸullu olasÄ±lÄ±ÄŸÄ±n hesaplanmasÄ±nÄ± gerektirir.

#### KoÅŸullu OlasÄ±lÄ±k FormÃ¼lÃ¼
$$P(X=x \mid Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}$$

---

#### 1. Payda HesaplamasÄ±: Marjinal OlasÄ±lÄ±k $P(Y=1)$ â•

$P(Y=1)$, $Y=1$ sÃ¼tunundaki tÃ¼m deÄŸerlerin toplamÄ±dÄ±r:

| $X$ DeÄŸeri | $P(X, Y=1)$ |
| :---: | :---: |
| 1 | $0.05$ |
| 2 | $0.10$ |
| 3 | $0.15$ |
| **Toplam** | $\mathbf{0.30}$ |

$$P(Y=1) = 0.05 + 0.10 + 0.15 = 0.30$$

---

#### 2. Pay DeÄŸeri: Ortak OlasÄ±lÄ±k $P(X=3, Y=1)$

Tablodan okunan deÄŸer:
$$P(X=3, Y=1) = 0.15$$

---

#### 3. Nihai Hesaplama (Final Calculation)

$$P(X=3 \mid Y=1) = \frac{P(X=3, Y=1)}{P(Y=1)} = \frac{0.15}{0.30}$$

$$\mathbf{P(X=3 \mid Y=1) = 0.5}$$

---

<img width="731" height="167" alt="image" src="https://github.com/user-attachments/assets/52ab6daf-7543-4fc5-937b-cb656964b808" />


### Kovaryans HesaplamasÄ±: $\text{Cov}(X, Y)$ ğŸ”—

Bu analiz, $\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$ formÃ¼lÃ¼ne dayanmaktadÄ±r.

#### Ortak OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ± Tablosu
| $X \setminus Y$ | 0 | 1 | **$P(X=x)$** |
| :---: | :---: | :---: | :---: |
| 0 | $0.2$ | $0.1$ | $0.3$ |
| 1 | $0.1$ | $0.6$ | $0.7$ |
| **$P(Y=y)$** | $0.3$ | $0.7$ | **1.0** |

---

#### 1. Beklenen DeÄŸerlerin HesaplanmasÄ± (Expected Values)

* **$E[X]$:** $(0 \cdot 0.3) + (1 \cdot 0.7) = \mathbf{0.7}$
* **$E[Y]$:** $(0 \cdot 0.3) + (1 \cdot 0.7) = \mathbf{0.7}$

---

#### 2. Ã‡arpÄ±mÄ±n Beklenen DeÄŸeri ($E[XY]$)

| $x$ | $y$ | $x \cdot y$ | $P(x, y)$ | $(x \cdot y) \cdot P(x, y)$ |
| :---: | :---: | :---: | :---: | :---: |
| 0 | 0 | 0 | 0.2 | 0.0 |
| 0 | 1 | 0 | 0.1 | 0.0 |
| 1 | 0 | 0 | 0.1 | 0.0 |
| 1 | 1 | 1 | 0.6 | 0.6 |

$$E[XY] = 0.0 + 0.0 + 0.0 + 0.6 = \mathbf{0.6}$$

---

#### 3. KovaryansÄ±n HesaplanmasÄ± (Final Covariance)

$$\text{Cov}(X, Y) = E[XY] - E[X] E[Y]$$
$$\text{Cov}(X, Y) = 0.6 - (0.7) \cdot (0.7)$$
$$\text{Cov}(X, Y) = 0.6 - 0.49$$

$$\mathbf{\text{Cov}(X, Y) = 0.11}$$

---

# ğŸ“ˆ Merkezi Limit Teoremi (CLT) vs. BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | Merkezi Limit Teoremi (CLT) ğŸ’¡ | BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) âš–ï¸ |
| :--- | :--- | :--- |
| **Temel TanÄ±m** | Bir popÃ¼lasyondan alÄ±nan baÄŸÄ±msÄ±z ve aynÄ± daÄŸÄ±lÄ±ma sahip **Ã¶rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±**, Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) yeterince arttÄ±kÃ§a, popÃ¼lasyonun orijinal daÄŸÄ±lÄ±mÄ± ne olursa olsun, **Normal DaÄŸÄ±lÄ±ma** yaklaÅŸÄ±r. | Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) arttÄ±kÃ§a, **Ã¶rneklem ortalamasÄ± ($\bar{x}$)**, popÃ¼lasyonun gerÃ§ek **ortalamasÄ±na ($\mu$)** yaklaÅŸÄ±r ve ona yakÄ±n kalÄ±r. |
| **Odak NoktasÄ±** | **DaÄŸÄ±lÄ±mÄ±n Åekli** (Ã–rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ± nasÄ±l gÃ¶rÃ¼nÃ¼r?) | **OrtalamanÄ±n DeÄŸeri** (Ã–rneklem ortalamasÄ± nereye gider?) |
| **Ä°stenen $n$** | DaÄŸÄ±lÄ±mÄ±n Normal'e yakÄ±nsamasÄ± iÃ§in genellikle $n \ge 30$ olmasÄ± beklenir. | Ne kadar bÃ¼yÃ¼k olursa o kadar iyi; yakÄ±nsama iÃ§in tek ÅŸart $n \to \infty$ (sonsuz) olmasÄ±dÄ±r. |
| **Matematiksel Ä°fade** | $$\bar{X} \xrightarrow{d} N(\mu, \sigma^2/n)$$ (DaÄŸÄ±lÄ±m, Normal DaÄŸÄ±lÄ±ma yakÄ±nsar) | $$\bar{X} \xrightarrow{p} \mu$$ (Ortalama, deÄŸere olasÄ±lÄ±kla yakÄ±nsar) |
| **Varyans Ä°liÅŸkisi** | Ã–rnekleme daÄŸÄ±lÄ±mÄ±nÄ±n varyansÄ± $\sigma^2/n$'dir, bu da $n$ arttÄ±kÃ§a **daÄŸÄ±lÄ±mÄ±n daraldÄ±ÄŸÄ±** anlamÄ±na gelir (Daha kesin tahmin). | LLN, esas olarak merkezi eÄŸilimle ilgilenir, varyansÄ±n azalmasÄ± CLT'nin bir sonucudur. |
| **Pratik AnlamÄ±** | Ä°statistiksel testlerin (t-testi, Z-testi) ve gÃ¼ven aralÄ±klarÄ±nÄ±n oluÅŸturulmasÄ±nÄ±n temelini saÄŸlar. PopÃ¼lasyon bilinmese bile Ã§Ä±karÄ±m yapmayÄ± mÃ¼mkÃ¼n kÄ±lar. | Uzun vadede gÃ¶zlemlenen sonucun teorik beklentiye eÅŸit olacaÄŸÄ±nÄ± garanti eder (Ã–r. 1000 kez yazÄ± tura atÄ±ldÄ±ÄŸÄ±nda yazÄ± gelme oranÄ±nÄ±n %50'ye yaklaÅŸmasÄ±). |
| **Ã–rnek** | Bir madeni parayÄ± 30 kez Ã§evirip ortalama tura gelme sayÄ±sÄ±nÄ± defalarca kaydettiÄŸinizde, bu ortalamalarÄ±n bir histogramÄ± Ã§an eÄŸrisine benzer. | Bir madeni parayÄ± defalarca Ã§evirdiÄŸinizde, tura gelme oranÄ±nÄ±n $\frac{1}{2}$ deÄŸerine giderek daha Ã§ok yaklaÅŸmasÄ±. |

### Metaforik KarÅŸÄ±laÅŸtÄ±rma (Ã–rnekler) ğŸ²ğŸ¯

| Teorem | Metafor | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **Merkezi Limit Teoremi (CLT) ğŸ“** | **Oyunun KurallarÄ±** | Bir zar oyunu oynarken, tek bir zarÄ±n sonucu rastgele (dÃ¼zgÃ¼n daÄŸÄ±lÄ±m). Ancak **Ã§ok sayÄ±da** tur oynarsanÄ±z ve her turdaki ortalama puanÄ± toplarsanÄ±z, bu ortalama puanlarÄ±n grafiÄŸi bir **Ã§an eÄŸrisi** (normal daÄŸÄ±lÄ±m) ÅŸeklini alÄ±r. CLT bize, oyunun kurallarÄ± (daÄŸÄ±lÄ±mÄ±n ÅŸekli) ne olursa olsun, *bÃ¼yÃ¼k serilerin* hep aynÄ± ÅŸekilde davrandÄ±ÄŸÄ±nÄ± sÃ¶yler. |
| **BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) ğŸ¯** | **Hedefe UlaÅŸma** | Bir hedef tahtasÄ±na atÄ±ÅŸ yapÄ±yorsunuz. Ä°lk birkaÃ§ atÄ±ÅŸÄ±nÄ±z rastgele yerlere dÃ¼ÅŸebilir. Ancak atÄ±ÅŸ sayÄ±nÄ±zÄ± **binlerceye** Ã§Ä±kardÄ±ÄŸÄ±nÄ±zda, atÄ±ÅŸlarÄ±nÄ±zÄ±n **ortalamasÄ±** (merkezi) hedefin tam ortasÄ±na (gerÃ§ek popÃ¼lasyon ortalamasÄ±na) giderek daha Ã§ok yaklaÅŸacaktÄ±r. LLN, bize yeterince deneme yaparsak **hedefi vuracaÄŸÄ±mÄ±zÄ±** garanti eder. |

---

# Maksimum Olabilirlik Tahmincisi (Maximum Likelihood Estimation - MLE)
## Gauss PopÃ¼lasyonu Ä°Ã§in MLE (MLE for Gaussian Population)


### Matematiksel FormÃ¼lasyon (Mathematical Formulation)
OrtalamasÄ± $\mu$ ve varyansÄ± $\sigma^2$ olan bir Gauss daÄŸÄ±lÄ±mÄ±ndan alÄ±nan $n$ Ã¶rnekleme $X=(X_1, X_2, \dots, X_n)$ sahip olduÄŸunuzu varsayalÄ±m. Bu, $X_i \sim_{i.i.d.} N(\mu, \sigma^2)$ anlamÄ±na gelir.

EÄŸer $\mu$ ve $\sigma$ iÃ§in MLE istiyorsanÄ±z, ilk adÄ±m **Olabilirlik Fonksiyonu (Likelihood)**'nu tanÄ±mlamaktÄ±r. EÄŸer hem $\mu$ hem de $\sigma$ bilinmiyorsa, olabilirlik bu iki parametrenin bir fonksiyonu olacaktÄ±r. $x=(x_1, x_2, \dots, x_n)$ ile verilen $X$'in bir gerÃ§ekleÅŸimi (realization) iÃ§in:

$$ L(\mu,\sigma; \boldsymbol{x}) = \prod_{i=1}^n f_{X_i}(x_i) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma } e^{-\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2}} $$
$$ L(\mu,\sigma; \boldsymbol{x}) = \frac{1}{(\sqrt{2\pi})^n\sigma^n }e^{-\frac{1}{2}\frac{\sum_{i=1}^n (x_i-\mu)^2}{\sigma^2}} $$

Åimdi yapmanÄ±z gereken tek ÅŸey, olabilirlik $L(\mu, \sigma; \boldsymbol{x})$'i maksimize eden $\mu$ ve $\sigma$ deÄŸerlerini bulmaktÄ±r.

### Log-Olabilirlik (Log-Likelihood) Fonksiyonu

Olabilirlik fonksiyonunun tÃ¼revini almak karmaÅŸÄ±k olduÄŸu iÃ§in, logaritma fonksiyonunun her zaman artan olmasÄ±ndan faydalanarak **Log-Olabilirlik Fonksiyonu** kullanÄ±lÄ±r:

$$ \ell(\mu,\sigma) = \log(L(\mu,\sigma; \boldsymbol{x})) $$

LogaritmanÄ±n Ã§arpÄ±mÄ± toplama dÃ¶nÃ¼ÅŸtÃ¼rme Ã¶zelliÄŸini ($\log(a \cdot b) = \log(a) + \log(b)$) ve diÄŸer logaritma Ã¶zelliklerini kullanarak Log-Olabilirlik ÅŸu ÅŸekilde basitleÅŸtirilir:

$$ \ell(\mu,\sigma) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2}\frac{\sum_{i=1}^n (x_i-\mu)^2}{\sigma^2} $$

### MLE'nin TÃ¼retilmesi (Derivation of MLE)

MLE iÃ§in $\mu$ ve $\sigma$ deÄŸerlerini bulmak iÃ§in, Log-Olabilirlik'in kÄ±smi tÃ¼revleri (partial derivatives) alÄ±nÄ±r ve sÄ±fÄ±ra eÅŸitlenir.

#### a) $\mu$ Ä°Ã§in KÄ±smi TÃ¼rev ($\partial / \partial \mu$):

$$\frac{\partial }{\partial \mu}\ell(\mu, \sigma) = \frac{1}{\sigma^2}\left(\sum_{i=1}^n x_i - n\mu\right) = 0$$

$\sigma > 0$ olduÄŸu iÃ§in $\sum_{i=1}^n x_i - n\mu = 0$ olmalÄ±dÄ±r. Buradan $\mu$ iÃ§in MLE tahmini:

$$\hat{\mu} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}$$
**SonuÃ§:** Ortalama iÃ§in MLE, **Ã–rneklem OrtalamasÄ± (Sample Mean)**'dÄ±r.

#### b) $\sigma$ Ä°Ã§in KÄ±smi TÃ¼rev ($\partial / \partial \sigma$):

$$\frac{\partial }{\partial \sigma}\ell(\mu, \sigma) = -\frac{n}{\sigma} + \left(\sum_{i=1}^n (x_i-\mu)^2\right)\frac{1}{\sigma^3} = 0$$

$\mu$'yu $\hat{\mu}=\bar{x}$ ile deÄŸiÅŸtirip $\sigma > 0$ olduÄŸu iÃ§in ifadeyi basitleÅŸtirirsek:

$$\frac{\partial }{\partial \sigma}\ell(\mu, \sigma) = -n + \left(\sum_{i=1}^n (x_i-\bar{x})^2\right)\frac{1}{\sigma^2} = 0$$

Buradan varyans iÃ§in MLE tahmini:

$$\hat{\sigma}^2 = \frac{\sum(x_i-\bar{x})^2}{n}$$

**SonuÃ§:** Standart sapma iÃ§in MLE ($\hat{\sigma}$), bu ifadenin karekÃ¶kÃ¼dÃ¼r. Bu ifade, Ã¶rneklem standart sapmasÄ± (sample standard deviation) iÃ§in Ã¶ÄŸrendiÄŸiniz formÃ¼le Ã§ok benzerdir, tek fark **$1/n$** ile normalleÅŸtirme yapÄ±lmasÄ±dÄ±r. Ã–rneklem standart sapmasÄ± ise **$1/(n-1)$** kullanÄ±r.
---

# ğŸ“Š Veri Bilimi ve Makine Ã–ÄŸrenimi Ã–ÄŸrenim PlanÄ±

Bu belge, Veri Bilimi ve Makine Ã–ÄŸrenimi alanÄ±ndaki Coursera eÄŸitimlerini mantÄ±ksal aÅŸamalara gÃ¶re gruplandÄ±rÄ±lmÄ±ÅŸ bir Ã§alÄ±ÅŸma planÄ±nÄ± sunar.

---

## ğŸš€ Maksimum Olabilirlik Tahmincisi (MLE) AÃ§Ä±klamasÄ±

MLE, eldeki veriyi en olasÄ± (highest likelihood) kÄ±lan model parametrelerini bulma yÃ¶ntemidir. ML'de Ã‡apraz Entropi gibi maliyet fonksiyonlarÄ±nÄ±n temelini oluÅŸturur.

### MLE vs. En KÃ¼Ã§Ã¼k Kareler (Least Squares) KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Kriter | Maksimum Olabilirlik Tahmincisi (MLE) ğŸ¯ | En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) ğŸ“ |
| :--- | :--- | :--- |
| **Temel Felsefe** | OlasÄ±lÄ±ÄŸa (Likelihood) dayanÄ±r. | HatalarÄ±n karesini minimize etmeye dayanÄ±r. |
| **Hesaplama AmacÄ±** | GÃ¶zlemleri **en olasÄ±** yapan $\theta$ parametrelerini bulmak. | Tahminler ($ \hat{y} $) ile gerÃ§ek deÄŸerler ($ y $) arasÄ±ndaki **mesafeyi** minimize etmek. |
| **Gereken VarsayÄ±m** | Verinin **olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±** bilmek zorunludur (Normal, Bernoulli vb.). | HatanÄ±n daÄŸÄ±lÄ±mÄ± hakkÄ±nda aÃ§Ä±k bir varsayÄ±m yapmaz. |
| **ML'deki KarÅŸÄ±lÄ±ÄŸÄ±** | Ã‡apraz Entropi KaybÄ± (Cross-Entropy Loss), Lojistik Regresyon. | Ortalama Karesel Hata (Mean Squared Error - MSE). |
| **EÅŸitlik Durumu** | Hata terimleri **Normal DaÄŸÄ±lÄ±m'a** sahipse, MLE'yi maksimize etmek, LS'yi minimize etmeye eÅŸdeÄŸerdir. |

---

# ğŸ¤– Makine Ã–ÄŸreniminde Maksimum Olabilirlik (Maximum Likelihood Estimation - MLE)

### AmaÃ§: ğŸ¯

MLE'nin temel amacÄ±, elimizdeki gÃ¶zlemlenen veriyi ($\mathbf{X}$) en olasÄ± (en yÃ¼ksek olasÄ±lÄ±klÄ±) hale getiren model parametrelerini ($\theta$) bulmaktÄ±r.

BaÅŸka bir deyiÅŸle, "EÄŸer modelimin parametreleri $\theta$ olsaydÄ±, bu veriyi gÃ¶rme olasÄ±lÄ±ÄŸÄ±m ne olurdu?" sorusuna cevap vererek bu olasÄ±lÄ±ÄŸÄ± maksimize etmektir.


---

### ML'de KullanÄ±m AlanÄ± ve RolÃ¼: ğŸ› ï¸

MLE, genellikle modelin Ã§Ä±ktÄ±sÄ±nÄ±n olasÄ±lÄ±ksal (probabilistic) olarak modellendiÄŸi durumlarda bir **Maliyet Fonksiyonu (Loss Function)** olarak kullanÄ±lÄ±r.

#### 1. SÄ±nÄ±flandÄ±rma (Classification):

* Ã–zellikle **Lojistik Regresyon (Logistic Regression)** ve **Yapay Sinir AÄŸlarÄ± (Neural Networks)** gibi modellerde, tahmin edilen Ã§Ä±ktÄ±nÄ±n bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (Ã¶rneÄŸin, Bernoulli veya Kategorik daÄŸÄ±lÄ±m) olduÄŸu varsayÄ±lÄ±r.
* Bu modellerde kullanÄ±lan **Ã‡apraz Entropi KaybÄ± (Cross-Entropy Loss)**, aslÄ±nda MLE'nin bir uygulamasÄ±dÄ±r. Ã‡apraz Entropi'yi minimize etmek, doÄŸru sÄ±nÄ±fÄ± gÃ¶zlemleme olasÄ±lÄ±ÄŸÄ±nÄ± maksimize eden parametreleri bulmaya eÅŸdeÄŸerdir.

#### 2. Regresyon (Regression):

* EÄŸer modelin hatalarÄ±nÄ±n (rezidÃ¼ellerinin) **Normal DaÄŸÄ±lÄ±m'a (Gaussian Distribution)** sahip olduÄŸu varsayÄ±lÄ±rsa, **En KÃ¼Ã§Ã¼k Kareler (Least Squares)** yÃ¶nteminin uygulanmasÄ±, matematiksel olarak MLE'nin uygulanmasÄ±yla aynÄ± parametre tahminlerini verir (yukarÄ±daki tÃ¼retme Ã¶rneÄŸinde olduÄŸu gibi).

#### 3. Ãœretici Modeller (Generative Models):

* Veri kÃ¼mesinin tamamÄ±nÄ±n nasÄ±l oluÅŸturulduÄŸunu Ã¶ÄŸrenen (Ã¶rneÄŸin, **Naif Bayes (NaÃ¯ve Bayes)**) modeller, parametrelerini tahmin etmek iÃ§in sÄ±klÄ±kla MLE'yi kullanÄ±r.

### âš–ï¸ MLE ve En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) KarÅŸÄ±laÅŸtÄ±rmasÄ±

En KÃ¼Ã§Ã¼k Kareler (LS), genellikle regresyonda kullanÄ±lan basit bir maliyet fonksiyonudur.

| Kriter | Maksimum Olabilirlik Tahmincisi (MLE) ğŸ¯ | En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) ğŸ“ |
| :--- | :--- | :--- |
| **Temel Felsefe** | OlasÄ±lÄ±ÄŸa (Likelihood) dayanÄ±r. | HatalarÄ±n karesini minimize etmeye dayanÄ±r. |
| **Hesaplama AmacÄ±** | GÃ¶zlemleri **en olasÄ±** yapan $\theta$ parametrelerini bulmak. | Tahminler ($\hat{y}$) ile gerÃ§ek deÄŸerler ($y$) arasÄ±ndaki **mesafeyi** minimize etmek. |
| **Gereken VarsayÄ±m** | Verinin (veya hatanÄ±n) **olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±** bilmek zorunludur (Ã¶rneÄŸin: Normal, Bernoulli, Poisson). | HatanÄ±n daÄŸÄ±lÄ±mÄ± hakkÄ±nda **aÃ§Ä±k bir varsayÄ±m yapmaz**, sadece varyansÄ±n sabit olduÄŸunu varsayar (Homoscedasticity). |
| **ML'deki KarÅŸÄ±lÄ±ÄŸÄ±** | Ã‡apraz Entropi (Cross-Entropy Loss), Lojistik Regresyon, Yapay Sinir AÄŸlarÄ± (NN). | Ortalama Karesel Hata (Mean Squared Error - MSE) ve Basit Lineer Regresyon. |
| **EÅŸitlik Durumu** | EÄŸer hata terimleri **Normal DaÄŸÄ±lÄ±m'a** sahipse, MLE'yi maksimize etmek, LS'yi minimize etmeye eÅŸdeÄŸerdir. |


# ğŸ”¬ SÄ±klÄ±kÃ§Ä± YaklaÅŸÄ±m (Frequentist Statistics)

Bu yaklaÅŸÄ±m, olasÄ±lÄ±k ve bilgi kavramlarÄ±nÄ± tamamen farklÄ± yorumlayan iki ana ekolden (SÄ±klÄ±kÃ§Ä± ve BayesÃ§i) biridir.

SÄ±klÄ±kÃ§Ä±lÄ±k, olasÄ±lÄ±ÄŸÄ±, uzun vadede bir olayÄ±n gerÃ§ekleÅŸme sÄ±klÄ±ÄŸÄ± olarak gÃ¶rÃ¼r.

## 1. SÄ±klÄ±kÃ§Ä± YaklaÅŸÄ±m (Frequentist Statistics) ğŸ“

| Kavram | AÃ§Ä±klama |
| :--- | :--- |
| **OlasÄ±lÄ±k AnlayÄ±ÅŸÄ±** | **Uzun Vadeli Olay SÄ±klÄ±ÄŸÄ± (Long-term Frequency):** OlasÄ±lÄ±k, bir deneyi sonsuz kez tekrarladÄ±ÄŸÄ±mÄ±zda bir sonucun ne sÄ±klÄ±kla ortaya Ã§Ä±kacaÄŸÄ±nÄ±n limitidir. Ã–rnek: Bir madeni paranÄ±n tura gelme olasÄ±lÄ±ÄŸÄ± %50'dir, Ã§Ã¼nkÃ¼ parayÄ± binlerce kez attÄ±ÄŸÄ±mÄ±zda tura gelme sÄ±klÄ±ÄŸÄ± bu deÄŸere yakÄ±nsar. |
| **Temel Kavram** | **Olabilirlik (Likelihood):** Elimizdeki veriler (gÃ¶zlemler) verildiÄŸinde, belirli bir model parametresinin ($\theta$) ne kadar olasÄ± olduÄŸunu Ã¶lÃ§er. Bu, genellikle $P(\text{Veri} \mid \theta)$ olarak ifade edilir. SÄ±klÄ±kÃ§Ä±lar, sadece veriye bakarak Ã§alÄ±ÅŸÄ±r. |
| **AmaÃ§** | **Veriyi En OlasÄ± Ãœreten Modeli Bulmak:** AmaÃ§, gÃ¶zlemlenen veriyi en iyi aÃ§Ä±klayan ve en yÃ¼ksek olabilirlik deÄŸerini veren **sabit model parametrelerini** bulmaktÄ±r (Ã–rn: p-deÄŸerleri, GÃ¼ven AralÄ±klarÄ± hesaplama). |
| **Parametreler** | PopÃ¼lasyon parametreleri ($\mu, \sigma$ vb.) **sabit ancak bilinmeyen** deÄŸerler olarak kabul edilir. |

# ğŸ§  BayesÃ§i YaklaÅŸÄ±m (Bayesian Statistics)

BayesÃ§ilik, olasÄ±lÄ±ÄŸÄ±, bilinmeyene olan kiÅŸisel inancÄ±n veya kesinliÄŸin derecesi olarak gÃ¶rÃ¼r.

## 2. BayesÃ§i YaklaÅŸÄ±m (Bayesian Statistics) ğŸ’¡

| Kavram | AÃ§Ä±klama |
| :--- | :--- |
| **OlasÄ±lÄ±k AnlayÄ±ÅŸÄ±** | **Ä°nanÃ§ Derecesi (Degree of Belief or Certainty):** OlasÄ±lÄ±k, bir kiÅŸinin veya sistemin, eldeki bilgi Ä±ÅŸÄ±ÄŸÄ±nda bir Ã¶nermenin doÄŸru olduÄŸuna ne kadar inandÄ±ÄŸÄ±nÄ±n sÃ¼bjektif Ã¶lÃ§Ã¼sÃ¼dÃ¼r. Yeni bilgi geldikÃ§e bu inanÃ§ gÃ¼ncellenir. |
| **Temel Kavram** | **Ã–nsel (Prior) ğŸ¤”:** Veri gÃ¶zlenmeden Ã¶nce parametrelerin ($\theta$) olasÄ± deÄŸerleri hakkÄ±ndaki inancÄ±mÄ±zdÄ±r. Ã–nsel daÄŸÄ±lÄ±m $P(\theta)$ olarak ifade edilir. BayesÃ§iler, veriyi **Ã¶n bilgi** ile birleÅŸtirir. |
| **AmaÃ§** | **Ã–nsel Ä°nancÄ± GÃ¶zlemlere DayalÄ± GÃ¼ncellemek ğŸ”„:** AmaÃ§, Bayes Teoremi'ni kullanarak Ã¶nsel inancÄ±, gÃ¶zlemlenen verilerle birleÅŸtirmek ve daha doÄŸru bir **sonsal (Posterior)** inanÃ§ elde etmektir. |
| **Parametreler** | PopÃ¼lasyon parametreleri **rastgele deÄŸiÅŸkenler** olarak kabul edilir ve bunlar hakkÄ±nda bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (inanÃ§) vardÄ±r. |
