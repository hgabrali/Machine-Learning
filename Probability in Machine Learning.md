# ğŸ§  Makine Ã–ÄŸrenimi'nin OlasÄ±lÄ±k Temelleri (Probability Foundations of Machine Learning)

Makine Ã¶ÄŸrenimi, bÃ¼yÃ¼k Ã¶lÃ§Ã¼de bilinmeyen bir sonucun, elimizdeki verilere dayanarak ortaya Ã§Ä±kma **olasÄ±lÄ±ÄŸÄ±nÄ± hesaplama** sanatÄ±dÄ±r. Temelde ML, bir olasÄ±lÄ±k hesaplama makinesi olarak iÅŸlev gÃ¶rÃ¼r.

[JSXGraph](https://jsxgraph.uni-bayreuth.de/home/)

[Bootstrap](https://getbootstrap.com/docs/5.0/getting-started/introduction/)

##### The following books were used as references in this course.

* Probability and Statistics (4th Edition) , Morris H. DeGroot, Mark J. Schervish, Pearson, 2011

* All of Statistics: A Concise Course in Statistical Inference by Larry Wasserman, Springer, 2010 

* Probabilistic Machine Learning: An Introduction by Kevin Patrick Murphy. MIT Press, March 2022.  


--- 

## 1. KoÅŸullu OlasÄ±lÄ±k (Conditional Probability) ğŸ“Š

Makine Ã¶ÄŸrenimindeki en yaygÄ±n kullanÄ±m alanÄ±, bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± **baÅŸka faktÃ¶rler (features)** verildiÄŸinde hesaplamaktÄ±r.

| Uygulama (Application) | Hesaplanan KoÅŸullu OlasÄ±lÄ±k | AÃ§Ä±klama (Explanation) |
| :--- | :--- | :--- |
| **Spam Tespiti** (Spam Detection) ğŸ“§ | P(Spam \| Kelimeler, AlÄ±cÄ±lar, ...) | Bir e-postanÄ±n **istenmeyen posta (spam)** olma olasÄ±lÄ±ÄŸÄ±, e-postadaki **kelimeler (words)** ve diÄŸer **Ã¶zellikler (features)** verildiÄŸinde. |
| **Duygu Analizi** (Sentiment Analysis) ğŸ˜Š | P(Mutlu \| Metindeki Kelimeler) | Bir metnin **mutlu (happy)** olma olasÄ±lÄ±ÄŸÄ±, iÃ§erdiÄŸi **kelimeler (words)** verildiÄŸinde. |
| **GÃ¶rÃ¼ntÃ¼ TanÄ±ma** (Image Recognition) ğŸ“¸ | P(Kedi \| Pikseller) | Bir resimde **kedi (cat)** olma olasÄ±lÄ±ÄŸÄ±, resimdeki **pikseller (pixels)** verildiÄŸinde. |

**Teknik TanÄ±m:** KoÅŸullu olasÄ±lÄ±k, **P(A \| B)** formÃ¼lÃ¼ ile ifade edilir ve "**B olayÄ± gerÃ§ekleÅŸtiÄŸinde A olayÄ±nÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±**" anlamÄ±na gelir. Makine Ã¶ÄŸrenimi **sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±nÄ±n (classifiers)** yaptÄ±ÄŸÄ± temel iÅŸ budur.

---

## 2. SÃ¼pervizyonlu Ã–ÄŸrenme ve KoÅŸullu OlasÄ±lÄ±k (Supervised Learning and Conditional Probability)

Metinde bahsedilen Spam Tespiti, Duygu Analizi ve GÃ¶rÃ¼ntÃ¼ TanÄ±ma Ã¶rnekleri **SÃ¼pervizyonlu Ã–ÄŸrenme (Supervised Learning)** alanÄ±na aittir.

* Bu alanda, model **etiketli veri (labeled data)** kullanÄ±larak eÄŸitilir ve sorulara cevap arar.
* **AmaÃ§:** Verilen girdi iÃ§in doÄŸru **etiketi (label)** tahmin etmektir.
* **Ã–rnekler:** Resim kedi mi? CÃ¼mle olumlu mu? E-posta spam mi?

---

## 3. Bayes Teoremi ve ArtÃ§Ä± OlasÄ±lÄ±k (Bayes' Theorem and Posterior Probability) ğŸ²

Bayes Teoremi, bir **sÄ±nÄ±flandÄ±rÄ±cÄ± (classifier)** oluÅŸturarak bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± baÅŸka bir ÅŸey verildiÄŸinde hesaplamanÄ±zÄ± saÄŸlayan matematiksel bir mekanizma sunar.

$$\mathbf{P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}}$$

OlasÄ±lÄ±ÄŸÄ±n hesaplanmasÄ±nda **Bayes Teoremi'nin** nasÄ±l kullanÄ±ldÄ±ÄŸÄ± aÃ§Ä±klanmÄ±ÅŸtÄ±r:

1.  **Ã–nsel OlasÄ±lÄ±k (Prior Probability):** BaÅŸlangÄ±Ã§ olasÄ±lÄ±ÄŸÄ±dÄ±r (Ã–rn: TÃ¼m e-postalar iÃ§inde spam oranÄ±, P(Spam)).
2.  **KanÄ±t (Evidence) veya Olay (Event):** Yeni bir bilginin ortaya Ã§Ä±kmasÄ± (Ã–rn: E-postada "lottery" kelimesinin geÃ§mesi).
3.  **ArtÃ§Ä± OlasÄ±lÄ±k (Posterior Probability):** Ã–nsel olasÄ±lÄ±ÄŸÄ±n yeni bilgiyle gÃ¼ncellenmiÅŸ halidir (Ã–rn: P(Spam \| Lottery)).

---

## 4. Jeneratif Modeller ve Saf OlasÄ±lÄ±k (Generative Models and Pure Probability) âœ¨

Metin, koÅŸullu olasÄ±lÄ±ÄŸÄ±n yanÄ± sÄ±ra **saf olasÄ±lÄ±klarÄ±n (pure probabilities)** kullanÄ±ldÄ±ÄŸÄ± bÃ¼yÃ¼k bir ML alanÄ±nÄ± daha tanÄ±tÄ±r: **Jeneratif Makine Ã–ÄŸrenimi (Generative Machine Learning)**. Bu, **GÃ¶zetimsiz Ã–ÄŸrenme (Unsupervised Learning)** alanÄ±nÄ±n bir parÃ§asÄ±dÄ±r.

* **AmaÃ§:** Modelin, var olan veriye benzeyen **yeni iÃ§erik** (gÃ¶rÃ¼ntÃ¼, metin, ses) Ã¼retmesini saÄŸlamaktÄ±r.
* **Ä°ÅŸleyiÅŸ:** Burada amaÃ§, koÅŸullu olasÄ±lÄ±k hesaplamak yerine, yeni Ã¼retilen verinin gerÃ§ekÃ§i olma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmektir (maximize probability)**.
    * **GÃ¶rÃ¼ntÃ¼ Ãœretimi (Image Generation - Ã–rn: StyleGAN):** Model, rastgele piksellerin insan yÃ¼zÃ¼ oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
    * **Metin Ãœretimi (Text Generation):** Model, rastgele kelimelerin anlamlÄ± ve baÄŸlamÄ±na uygun bir metin oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
 
    * # ğŸ’¡ Makine Ã–ÄŸrenmesinde Bayes Teoremi ve Ä°lgili Konular

Bayes Teoremi, makine Ã¶ÄŸrenmesinde **olasÄ±lÄ±ksal sÄ±nÄ±flandÄ±rma** algoritmalarÄ±nÄ±n temelini oluÅŸturur. Ã–zellikle **NaÃ¯ve Bayes SÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±** gibi yaygÄ±n algoritmalarÄ± anlamak iÃ§in bu kavramlara derinlemesine hakim olmak kritiktir.

---

## ğŸ“ Bayes Teoremi'nin FormÃ¼lÃ¼ ve BileÅŸenleri

Bayes Teoremi, bir olayÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ±, Ã¶nceden bilinen koÅŸullu ve marjinal olasÄ±lÄ±klarÄ± kullanarak hesaplar.

$$\huge P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

| Terim | AdÄ± | AÃ§Ä±klama (Makine Ã–ÄŸrenmesi) |
| :--- | :--- | :--- |
| **P(A|B)** | **Sonsal OlasÄ±lÄ±k (Posterior)** | Verilen veri (B) ile Ã¶rneÄŸin belli bir sÄ±nÄ±fa (A) ait olma olasÄ±lÄ±ÄŸÄ±. (Hedefimiz budur!) |
| **P(B|A)** | **Olabilirlik (Likelihood)** | Verilen sÄ±nÄ±f (A) iÃ§in verinin (B) gÃ¶zlemlenme olasÄ±lÄ±ÄŸÄ±. |
| **P(A)** | **Ã–nsel OlasÄ±lÄ±k (Prior)** | B olayÄ± hakkÄ±nda hiÃ§bir bilgi olmadan A sÄ±nÄ±fÄ±nÄ±n genel olasÄ±lÄ±ÄŸÄ±. |
| **P(B)** | **KanÄ±t (Evidence)** | Sonsal olasÄ±lÄ±ÄŸÄ± normalize eden sabittir. |

## ğŸ¤– Naive Bayes SÄ±nÄ±flandÄ±rÄ±cÄ± Ã‡eÅŸitleri ve KarÅŸÄ±laÅŸtÄ±rmasÄ±

Naive Bayes, **Ã¶zelliklerin birbirinden baÄŸÄ±msÄ±z olduÄŸu** (saf varsayÄ±m) varsayÄ±mÄ±na dayanÄ±r ve basitliÄŸi ile Ã¶ne Ã§Ä±kar.

| Algoritma AdÄ± | Temel Ã–zellik VarsayÄ±mÄ± | Uygulama AlanÄ± | Ã–rnek Veri Tipi | AvantajlarÄ± |
| :--- | :--- | :--- | :--- | :--- |
| **Gaussian Naive Bayes** | Ã–zellikler **Normal (Gauss) daÄŸÄ±lÄ±mÄ±na** (Normal/Gaussian Distribution) uyar. | SÃ¼rekli sayÄ±sal verilerin olduÄŸu sÄ±nÄ±flandÄ±rma problemleri. | Boy, kilo, sÄ±caklÄ±k gibi sÃ¼rekli deÄŸerler. | HÄ±zlÄ± ve kÃ¼Ã§Ã¼k veri kÃ¼melerinde baÅŸarÄ±lÄ±dÄ±r. |
| **Multinomial Naive Bayes** | Ã–zellikler **Multinomial daÄŸÄ±lÄ±mÄ±na** (Multinomial Distribution) uyar (sayÄ±m verileri). | Metin sÄ±nÄ±flandÄ±rma (spam, duygu analizi). | Bir belgedeki kelimelerin frekansÄ± (sayÄ±mÄ±). | Metin verilerinde en baÅŸarÄ±lÄ± Ã§eÅŸitlerdendir. |
| **Bernoulli Naive Bayes** | Ã–zellikler **Bernoulli daÄŸÄ±lÄ±mÄ±na** (Bernoulli Distribution) uyar (ikili/Boolean). | Belge varlÄ±ÄŸÄ±/yokluÄŸu, ikili Ã¶zelliklerin olduÄŸu sÄ±nÄ±flandÄ±rmalar. | Kelimenin bir belgede bulunup bulunmamasÄ± (1/0). | Ä°kili Ã¶zellik setleri iÃ§in etkilidir. |

---

## ğŸ§  Bilinmesi Gereken Kritik Ek Konular

Bayes algoritmalarÄ±nÄ± etkili kullanmak iÃ§in anlaÅŸÄ±lmasÄ± gereken temel kavramlar:

### 1. KoÅŸullu BaÄŸÄ±msÄ±zlÄ±k VarsayÄ±mÄ± (Conditional Independence)
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in "saf" kÄ±smÄ± buradan gelir. Hedef sÄ±nÄ±f verildiÄŸinde, girdilerin (Ã¶zelliklerin) birbirlerinden baÄŸÄ±msÄ±z olduÄŸu varsayÄ±lÄ±r. Bu, hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r.
* **Matematiksel Ä°fade:** $P(x_1, x_2, \dots, x_n | C) = \prod_{i=1}^{n} P(x_i | C)$

### 2. SÄ±fÄ±r Frekans Problemi ve Ã‡Ã¶zÃ¼mÃ¼
* **Problem:** EÄŸitim verisinde hiÃ§ gÃ¶rÃ¼lmeyen bir Ã¶zellik-sÄ±nÄ±f kombinasyonu test verisinde ortaya Ã§Ä±karsa, ilgili **Olabilirlik** ($P(B|A)$) sÄ±fÄ±r olur. Bu durumda, sonuÃ§ **Sonsal OlasÄ±lÄ±k** da sÄ±fÄ±rlanÄ±r.
* **Ã‡Ã¶zÃ¼m:** **Laplace YumuÅŸatmasÄ± (Laplace Smoothing)** kullanÄ±lÄ±r. TÃ¼m sayÄ±mlara kÃ¼Ã§Ã¼k bir pozitif deÄŸer ($\alpha$, genellikle 1) eklenerek sÄ±fÄ±r olasÄ±lÄ±klar engellenir.

### 3. Maksimum Sonsal OlasÄ±lÄ±k (Maximum A Posteriori - MAP) Karar KuralÄ±

* **AÃ§Ä±klama:** SÄ±nÄ±flandÄ±rma yaparken NaÃ¯ve Bayes, olasÄ± tÃ¼m sÄ±nÄ±flar ($C_k$) arasÄ±ndan **en yÃ¼ksek sonsal olasÄ±lÄ±ÄŸa** sahip olan sÄ±nÄ±fÄ± seÃ§er. Bu, modelin tahmin mekanizmasÄ±dÄ±r.


### 4. Bayes AÄŸlarÄ± (Bayesian Networks) ğŸŒ
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in baÄŸÄ±msÄ±zlÄ±k varsayÄ±mÄ±nÄ±n Ã¶tesine geÃ§en, Ã¶zellikler arasÄ±ndaki **baÄŸÄ±mlÄ±lÄ±klarÄ±** modelleyebilen daha geliÅŸmiÅŸ olasÄ±lÄ±ksal grafik modellerdir.
* **Ã–nemi:** Ã–zellikler arasÄ±ndaki nedensel iliÅŸkileri (YÃ¶nlendirilmiÅŸ Asiklik Grafikler - DAG) kullanarak daha doÄŸru, ancak daha karmaÅŸÄ±k Ã§Ä±karÄ±mlar saÄŸlar.

---

## âœ‰ï¸ Ã–rnek Uygulama: E-posta Spam Tespiti (Multinomial NB)

**AmaÃ§:** "Ãœcretsiz para" e-postasÄ±nÄ± sÄ±nÄ±flandÄ±rmak.

### 1. Ã–nsel OlasÄ±lÄ±klar (Prior)

| SÄ±nÄ±f | SayÄ±m | $P(C)$ |
| :--- | :--- | :--- |
| Spam | 3 | $3/5 = 0.6$ |
| Ham | 2 | $2/5 = 0.4$ |

### 2. Olabilirlikler (Likelihood) - Laplace YumuÅŸatmasÄ± UygulanmÄ±ÅŸ

KullanÄ±lan kelimeler: "Ãœcretsiz" ve "Para".

* $P(\text{Ãœcretsiz}|\text{Spam}) = 3/17$
* $P(\text{Para}|\text{Spam}) = 2/17$

* $P(\text{Ãœcretsiz}|\text{Ham}) = 1/14$
* $P(\text{Para}|\text{Ham}) = 1/14$

### 3. Sonsal OlasÄ±lÄ±k HesabÄ±

MAP kuralÄ±nÄ± uygulayarak:

* **Spam PuanÄ±:** $P(\text{Spam}|D) \propto P(\text{Ãœcretsiz}|\text{Spam}) \cdot P(\text{Para}|\text{Spam}) \cdot P(\text{Spam})$
    * Spam PuanÄ± $\propto (3/17) \cdot (2/17) \cdot 0.6 \approx 0.0124$

* **Ham PuanÄ±:** $P(\text{Ham}|D) \propto P(\text{Ãœcretsiz}|\text{Ham}) \cdot P(\text{Para}|\text{Ham}) \cdot P(\text{Ham})$
    * Ham PuanÄ± $\propto (1/14) \cdot (1/14) \cdot 0.4 \approx 0.0020$

**SonuÃ§:** $0.0124 > 0.0020$ olduÄŸu iÃ§in model, e-postayÄ± **SPAM** olarak sÄ±nÄ±flandÄ±rÄ±r. âœ…

---
---

<img width="930" height="347" alt="image" src="https://github.com/user-attachments/assets/ebf529b1-ea70-4f52-b2cc-874a6a8607f4" />

## â“ OlasÄ±lÄ±k Problemi Ã‡Ã¶zÃ¼mÃ¼: Yetersiz Bilgi

Bu, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ± (Probability of Union)** hesaplama problemidir. Temel kural, iki olayÄ±n **kesiÅŸiminin ($P(A \text{ ve } B)$)** bilinmesini gerektirir.

### ğŸ’Š Deney Verileri

* **Toplam Hasta SayÄ±sÄ±:** 100
* **BaÅŸ AÄŸrÄ±sÄ± YaÅŸayanlar (Headache - H):** 50
* **AteÅŸ YaÅŸayanlar (Fever - F):** 50

**OlasÄ±lÄ±klar (Prior):**
* $P(H) = \frac{50}{100} = 0.5$
* $P(F) = \frac{50}{100} = 0.5$

### ğŸ¯ Ä°stenen OlasÄ±lÄ±k

Doktorlar, bir hastanÄ±n **BaÅŸ AÄŸrÄ±sÄ± VEYA AteÅŸ** yaÅŸama olasÄ±lÄ±ÄŸÄ±nÄ±, yani $\mathbf{P(H \cup F)}$'i bulmak istiyor.

### ğŸ“ OlasÄ±lÄ±klarÄ±n BirleÅŸim KuralÄ± (The Addition Rule)

Genel kural ÅŸudur:
$$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

Bu kuralÄ± uygulamak iÃ§in $\mathbf{P(\text{BaÅŸ AÄŸrÄ±sÄ± ve AteÅŸ})}$ terimine ihtiyacÄ±mÄ±z var. Bu, aynÄ± anda hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ yaÅŸayan hastalarÄ±n oranÄ±nÄ± ifade eder. Soruda bu **kesiÅŸim olasÄ±lÄ±ÄŸÄ±** (yani kaÃ§ hastanÄ±n Ã§ifte semptom yaÅŸadÄ±ÄŸÄ±) **verilmemiÅŸtir**.

### âŒ GeÃ§ersiz VarsayÄ±mlarÄ±n Ä°ncelenmesi

| VarsayÄ±m | Kural | Neden GeÃ§ersiz? |
| :--- | :--- | :--- |
| **AyrÄ±k Olaylar** (Mutually Exclusive) | $P(A \text{ veya } B) = P(A) + P(B) = 1$ | HiÃ§bir hastanÄ±n iki semptomu birden yaÅŸamadÄ±ÄŸÄ± varsayÄ±lÄ±r. Soruda bu bilgi **yoktur**. |
| **BaÄŸÄ±msÄ±z Olaylar** (Independent) | $P(A \text{ ve } B) = P(A) \cdot P(B) = 0.25$ | TÄ±bbi semptomlar genellikle **baÄŸÄ±mlÄ±dÄ±r**. Ä°laÃ§, semptomlardan birini tetikleyebilir veya engelleyebilir. Bu varsayÄ±mÄ±n doÄŸru olduÄŸu **garanti edilemez**. |

### ğŸ›‘ SonuÃ§

$\mathbf{P(\text{H ve F})}$ deÄŸeri (kesiÅŸim) bilinmediÄŸi sÃ¼rece, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ±** doÄŸru bir ÅŸekilde hesaplanamaz.

* **Ek Bilgi Gereksinimi:** KaÃ§ hasta **sadece** baÅŸ aÄŸrÄ±sÄ±, kaÃ§ hasta **sadece** ateÅŸ ve kaÃ§ hasta **hem** baÅŸ aÄŸrÄ±sÄ± **hem de** ateÅŸ yaÅŸadÄ±?

**DoÄŸru Ä°fade:**
> **Not enough information is given to calculate $P(\text{fever or headache})$.**
> (BaÅŸ aÄŸrÄ±sÄ± veya ateÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± hesaplamak iÃ§in yeterli bilgi verilmemiÅŸtir.)
>
> ## ğŸ“Œ AÃ§Ä±klamanÄ±n Ã–zeti: Neden Yetersiz Bilgi?

AÃ§Ä±klama, temel olasÄ±lÄ±k kurallarÄ±ndan yola Ã§Ä±karak ÅŸunlarÄ± sÃ¶ylÃ¼yor:

1.  **Olaylar OrtaktÄ±r (Joint Events) ğŸ¤:** Problem metninde, iki olayÄ±n (**baÅŸ aÄŸrÄ±sÄ±** ve **ateÅŸ**) **ayrÄ±k (disjoint)** olduÄŸu, yani aynÄ± anda gerÃ§ekleÅŸemeyeceÄŸi sÃ¶ylenmiyor. DolayÄ±sÄ±yla, bazÄ± kiÅŸilerin **hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ** yaÅŸama ihtimali vardÄ±r. Bu tÃ¼r Ã§akÄ±ÅŸan olaylara **ortak olaylar (joint events)** denir.

2.  **KesiÅŸim Bilinmeli â“:** Ä°ki olayÄ±n birleÅŸim olasÄ±lÄ±ÄŸÄ±nÄ± ($P(A \text{ veya } B)$) hesaplamak iÃ§in genel **Toplama KuralÄ± (Addition Rule)**'nÄ± kullanmak zorunludur:
    $$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

3.  **SonuÃ§ ğŸ›‘:** Bu nedenle, $P(\text{ateÅŸ veya baÅŸ aÄŸrÄ±sÄ±})$ olasÄ±lÄ±ÄŸÄ±nÄ± bulmak iÃ§in, olaylarÄ±n kesiÅŸim olasÄ±lÄ±ÄŸÄ± olan **$P(\text{ateÅŸ VE baÅŸ aÄŸrÄ±sÄ±})$** deÄŸerini **bilmeniz gerekir**. Bu bilgi problem metninde verilmediÄŸi iÃ§in, olasÄ±lÄ±k hesaplanamaz.

---

<img width="870" height="357" alt="image" src="https://github.com/user-attachments/assets/f76afc36-6297-45df-9f2a-011289ddd967" />


## ğŸ KoÅŸullu OlasÄ±lÄ±k Ã‡Ã¶zÃ¼mÃ¼: YazÄ±lÄ±m Testi

Bu problem, **Bayes Teoremi'nin temelini oluÅŸturan** bir **KoÅŸullu OlasÄ±lÄ±k (Conditional Probability)** problemidir. Bir kullanÄ±cÄ±nÄ±n hata deneyimlediÄŸi bilgisi **verildiÄŸinde**, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±nÄ± bulmayÄ± amaÃ§lar.

### ğŸ“ 1. Olay TanÄ±mlarÄ± ve FormÃ¼l

* **B:** KullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmesi.
* **H:** KullanÄ±cÄ±nÄ±n **Hata (Bug)** deneyimlemesi.

Aranan olasÄ±lÄ±k: $P(B|H)$ (Hata deneyimlediÄŸine gÃ¶re, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±).

$$\mathbf{P(B|H)} = \frac{P(B \cap H)}{P(H)}$$

### ğŸ“Š 2. Verilerin Ã–zetlenmesi

Toplam kullanÄ±cÄ± sayÄ±sÄ±: $4000 + 5000 = \mathbf{9000}$

| Kategori | DeÄŸer | Olay SayÄ±sÄ± (N) | OlasÄ±lÄ±k (P) |
| :--- | :--- | :--- | :--- |
| **Versiyon B KullanÄ±cÄ±larÄ±** | 5000 | $N(B)$ | $P(B) = 5000/9000$ |
| **Toplam Hata Deneyimleyenler** | 3000 | $N(H)$ | $P(H) = 3000/9000$ |
| **Versiyon B ve Hata KesiÅŸimi** | 1500 | $N(B \cap H)$ | $P(B \cap H) = 1500/9000$ |

### ğŸ§  3. Hesaplama (KullanÄ±cÄ± SayÄ±larÄ±yla)

OlasÄ±lÄ±klar yerine, hesaplamanÄ±n daha basit olmasÄ± iÃ§in doÄŸrudan **kullanÄ±cÄ± sayÄ±larÄ±nÄ±** kullanabiliriz, Ã§Ã¼nkÃ¼ payda ($N(\text{Toplam})$) sadeleÅŸecektir:

$$\mathbf{P(B|H)} = \frac{N(\text{B ve H})}{N(H)} = \frac{\text{Versiyon B ile hata deneyimleyenler}}{\text{Toplam hata deneyimleyenler}}$$

$$P(B|H) = \frac{1500}{3000}$$

$$P(B|H) = \frac{1}{2} = \mathbf{0.5}$$

---

### âœ… SonuÃ§

Bir kullanÄ±cÄ±nÄ±n **hata deneyimlediÄŸi** bilgisi verildiÄŸinde, bu kullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmiÅŸ olma olasÄ±lÄ±ÄŸÄ± $\mathbf{1/2}$ (**%50**) olarak bulunur.


---

<img width="966" height="326" alt="image" src="https://github.com/user-attachments/assets/c8262fec-89f0-4c5b-ac2f-3c45b1eb903f" />

# ğŸ”¬ Bayes Teoremi UygulamasÄ±: TÄ±bbi Test Analizi

Bu, bir kiÅŸinin test sonucu pozitif Ã§Ä±ktÄ±ÄŸÄ±nda **gerÃ§ekte hasta olma olasÄ±lÄ±ÄŸÄ±nÄ± (Sonsal OlasÄ±lÄ±k)** hesaplayan klasik bir Bayes Teoremi problemidir.

### ğŸ¯ Aranan OlasÄ±lÄ±k

Ä°stenen olasÄ±lÄ±k, testin pozitif Ã§Ä±ktÄ±ÄŸÄ± bilgisi **verildiÄŸinde** kiÅŸinin hasta olma olasÄ±lÄ±ÄŸÄ±dÄ±r: $\mathbf{P(\text{hasta} | \text{test poz.})}$.

**Bayes Teoremi FormÃ¼lÃ¼:**
$$P(\text{hasta} | \text{test poz.}) = \frac{P(\text{test poz.} | \text{hasta}) \cdot P(\text{hasta})}{P(\text{test poz.})}$$

---

### 1. Parametrelerin TanÄ±mlanmasÄ± ve GiriÅŸ Verileri

| Veri | TanÄ±m | DeÄŸer |
| :--- | :--- | :--- |
| **Ã–nsel OlasÄ±lÄ±k** | $P(\text{hasta})$ (HastalÄ±k yaygÄ±nlÄ±ÄŸÄ±) | $1\% = \mathbf{0.01}$ |
| **Olabilirlik (True Positive)** | $P(\text{test poz.} | \text{hasta})$ (Hasta iken testin pozitif Ã§Ä±kmasÄ±) | $95\% = \mathbf{0.95}$ |
| **True Negative** | $P(\text{test neg.} | \text{saÄŸlÄ±klÄ±})$ (SaÄŸlÄ±klÄ± iken testin negatif Ã§Ä±kmasÄ±) | $90\% = \mathbf{0.90}$ |

---

### 2. Eksik OlasÄ±lÄ±klarÄ±n HesaplanmasÄ± (TÃ¼mleyen KuralÄ±)

Bayes formÃ¼lÃ¼nÃ¼ tamamlamak iÃ§in gerekli deÄŸerler:

#### A) SaÄŸlÄ±klÄ± Olma OlasÄ±lÄ±ÄŸÄ± ($P(\text{saÄŸlÄ±klÄ±})$)
$$P(\text{saÄŸlÄ±klÄ±}) = 1 - P(\text{hasta}) = 1 - 0.01 = \mathbf{0.99}$$

#### B) YanlÄ±ÅŸ Pozitif OlasÄ±lÄ±ÄŸÄ± ($P(\text{test poz.} | \text{saÄŸlÄ±klÄ±})$)
SaÄŸlÄ±klÄ± bir kiÅŸide testin pozitif Ã§Ä±kma olasÄ±lÄ±ÄŸÄ± (False Positive):
$$P(\text{test poz.} | \text{saÄŸlÄ±klÄ±}) = 1 - P(\text{test neg.} | \text{saÄŸlÄ±klÄ±}) = 1 - 0.90 = \mathbf{0.10}$$

---

### 3. KanÄ±tÄ±n HesaplanmasÄ± ($P(\text{test poz.})$)

KanÄ±t (payda), hem doÄŸru pozitifler hem de yanlÄ±ÅŸ pozitifler dahil olmak Ã¼zere **tÃ¼m pozitif test sonuÃ§larÄ±nÄ±n** toplam olasÄ±lÄ±ÄŸÄ±dÄ±r (Toplam OlasÄ±lÄ±k KuralÄ±):

$$P(\text{test poz.}) = \big(P(\text{test poz.} | \text{hasta}) \cdot P(\text{hasta})\big) + \big(P(\text{test poz.} | \text{saÄŸlÄ±klÄ±}) \cdot P(\text{saÄŸlÄ±klÄ±})\big)$$

$$P(\text{test poz.}) = (0.95 \cdot 0.01) + (0.10 \cdot 0.99)$$$$P(\text{test poz.}) = 0.0095 + 0.0990$$$$\mathbf{P(\text{test poz.})} = \mathbf{0.1085}$$

---

### 4. Sonsal OlasÄ±lÄ±ÄŸÄ±n HesaplanmasÄ± (Final)

TÃ¼m deÄŸerleri Bayes Teoremi formÃ¼lÃ¼ne yerleÅŸtirelim:

$$P(\text{hasta} | \text{test poz.}) = \frac{0.0095}{0.1085}$$

$$P(\text{hasta} | \text{test poz.}) \approx \mathbf{0.08755}$$

### âœ… Nihai SonuÃ§ ve Yorum

Test sonucu pozitif Ã§Ä±kan bir kiÅŸinin **gerÃ§ekten hasta olma olasÄ±lÄ±ÄŸÄ±** yaklaÅŸÄ±k olarak **%8.76**'dÄ±r ($\mathbf{0.0876}$).

Bu dÃ¼ÅŸÃ¼k sonuÃ§ ÅŸaÅŸÄ±rtÄ±cÄ±dÄ±r ve temel olarak ÅŸundan kaynaklanÄ±r:
> **HastalÄ±k nadirdir.** PopÃ¼lasyonun sadece %1'i hastadÄ±r. Test oldukÃ§a doÄŸru olsa da, Ã§ok sayÄ±da saÄŸlÄ±klÄ± insanÄ±n yanlÄ±ÅŸ pozitif ($0.10 \cdot 0.99 = 0.0990$) sonuÃ§ vermesi, doÄŸru pozitif sonuÃ§lardan ($0.95 \cdot 0.01 = 0.0095$) sayÄ±ca Ã§ok daha fazladÄ±r.
> 

---
---

<img width="966" height="582" alt="image" src="https://github.com/user-attachments/assets/a5111561-ea89-40d6-80df-f1ed8bdea4b9" />

# ğŸ² Binom DaÄŸÄ±lÄ±mÄ± (Binomial Distribution) Problemi Ã‡Ã¶zÃ¼mÃ¼

Bu problem, sabit sayÄ±da denemeden (20 zar atÄ±ÅŸÄ±), belirli sayÄ±da (7 kez) baÅŸarÄ± elde etme olasÄ±lÄ±ÄŸÄ±nÄ± hesapladÄ±ÄŸÄ±mÄ±z klasik bir **Binom DaÄŸÄ±lÄ±mÄ±** Ã¶rneÄŸidir.

### ğŸ“ Binom DaÄŸÄ±lÄ±mÄ± FormÃ¼lÃ¼

$n$ denemeden $k$ kez baÅŸarÄ± elde etme olasÄ±lÄ±ÄŸÄ±:

$$P(X=k) = C(n, k) \cdot p^k \cdot (1-p)^{n-k}$$

---

### 1. Problemin Parametreleri

| Parametre | TanÄ±m | DeÄŸer |
| :--- | :--- | :--- |
| **$n$** (Deneme SayÄ±sÄ±) | Zar atÄ±ÅŸÄ± sayÄ±sÄ± | $\mathbf{20}$ |
| **$k$** (Ä°stenen BaÅŸarÄ±) | "4" gelme sayÄ±sÄ± | $\mathbf{7}$ |
| **$p$** (BaÅŸarÄ± OlasÄ±lÄ±ÄŸÄ±) | Bir atÄ±ÅŸta '4' gelme olasÄ±lÄ±ÄŸÄ± | $\mathbf{1/6}$ |
| **$1-p$** (BaÅŸarÄ±sÄ±zlÄ±k OlasÄ±lÄ±ÄŸÄ±) | '4' dÄ±ÅŸÄ±nda bir sayÄ± gelme olasÄ±lÄ±ÄŸÄ± | $1 - 1/6 = \mathbf{5/6}$ |
| **$n-k$** (BaÅŸarÄ±sÄ±zlÄ±k SayÄ±sÄ±) | BaÅŸarÄ±sÄ±z atÄ±ÅŸ sayÄ±sÄ± | $20 - 7 = \mathbf{13}$ |

---

### 2. FormÃ¼lÃ¼n UygulanmasÄ±

Bulunan deÄŸerler formÃ¼lde yerine konur. $C(20, 7)$ ifadesi, kombinasyon gÃ¶sterimi olan $\binom{20}{7}$ ÅŸeklinde yazÄ±lÄ±r:

$$P(X=7) = \binom{20}{7} \cdot \left(\frac{1}{6}\right)^7 \cdot \left(\frac{5}{6}\right)^{13}$$

### âœ… DoÄŸru SeÃ§enek

Bu matematiksel ifadeye karÅŸÄ±lÄ±k gelen seÃ§enek, **dÃ¶rdÃ¼ncÃ¼ (alttan birinci) seÃ§enektir**.

$$P(X=7) = \binom{20}{7} \cdot \left(\frac{1}{6}\right)^7 \cdot \left(\frac{5}{6}\right)^{13}$$

---
---

<img width="763" height="616" alt="image" src="https://github.com/user-attachments/assets/f69d8cc5-702d-47bc-9460-cc0addb2f6fd" />

# ğŸš• AyrÄ±k OlasÄ±lÄ±k Ã‡Ã¶zÃ¼mÃ¼: KÃ¼mÃ¼latif DaÄŸÄ±lÄ±m

Bu, bir taksi yolculuÄŸunda yolcu sayÄ±sÄ±nÄ±n ($X$) **3 veya daha az** olma olasÄ±lÄ±ÄŸÄ±nÄ± hesaplayan bir **AyrÄ±k OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ±** problemidir. Ä°stenen olasÄ±lÄ±k $\mathbf{P(X \le 3)}$'tÃ¼r.

### ğŸ“ Ã‡Ã¶zÃ¼m: KÃ¼mÃ¼latif OlasÄ±lÄ±k Hesaplama

$P(X \le 3)$ olasÄ±lÄ±ÄŸÄ±, $X$'in 0, 1, 2 veya 3 olduÄŸu ayrÄ± olasÄ±lÄ±klarÄ±n toplamÄ±na eÅŸittir:

$$P(X \le 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3)$$

### 1. Tablodan OlasÄ±lÄ±klarÄ± Alma

| Yolcu SayÄ±sÄ± ($x_i$) | OlasÄ±lÄ±k ($p_i$) |
| :--- | :--- |
| $X=0$ | $0.10$ |
| $X=1$ | $0.25$ |
| $X=2$ | $0.25$ |
| $X=3$ | $0.15$ |

### 2. OlasÄ±lÄ±klarÄ± Toplama

Ä°stenen kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± bulmak iÃ§in deÄŸerleri toplarÄ±z:

$$P(X \le 3) = 0.10 + 0.25 + 0.25 + 0.15$$

$$\mathbf{P(X \le 3)} = \mathbf{0.75}$$

### âœ… SonuÃ§

Rastgele seÃ§ilen bir taksi yolculuÄŸunda yolcu sayÄ±sÄ±nÄ±n 3 veya daha az olma olasÄ±lÄ±ÄŸÄ± $\mathbf{0.75}$'tir (veya **%75**).

---
---

<img width="827" height="529" alt="image" src="https://github.com/user-attachments/assets/c48b6a1e-6605-4e0e-a500-d76c386a4d64" />

<img width="674" height="524" alt="image" src="https://github.com/user-attachments/assets/4c938733-bd84-49ce-b1d1-f1ebfef7b961" />

# ğŸ“‰ DÃ¶rt Normal DaÄŸÄ±lÄ±mÄ±n (Gaussian) KarÅŸÄ±laÅŸtÄ±rmalÄ± Analizi

Bu analiz, dÃ¶rt farklÄ± Normal DaÄŸÄ±lÄ±mÄ±n (Ã‡an EÄŸrisi) iki temel parametresi olan **Ortalama ($\mu$)** ve **Standart Sapma ($\sigma$)** deÄŸerlerinin grafikte nasÄ±l yorumlandÄ±ÄŸÄ±nÄ± gÃ¶sterir.

### ğŸ§  Normal DaÄŸÄ±lÄ±m Parametrelerinin YorumlanmasÄ±

* **Ortalama ($\mu$) Konumu:** EÄŸrinin **en yÃ¼ksek noktasÄ± (tepesi)** $\mu$ deÄŸerine denk gelir.
* **Standart Sapma ($\sigma$) YayÄ±lÄ±mÄ±:** EÄŸri ne kadar **dar ve uzunsa**, $\sigma$ o kadar **kÃ¼Ã§Ã¼ktÃ¼r** (dÃ¼ÅŸÃ¼k varyans). EÄŸri ne kadar **geniÅŸ ve alÃ§aksa**, $\sigma$ o kadar **bÃ¼yÃ¼ktÃ¼r** (yÃ¼ksek varyans).

---

### 1. DaÄŸÄ±lÄ±mlarÄ±n DetaylÄ± Analizi Tablosu

| DaÄŸÄ±lÄ±m | Renk/Ã‡izgi Tipi | Ortalama ($\mu$) Yorumu (Tepe NoktasÄ±) | Standart Sapma ($\sigma$) Yorumu (GeniÅŸlik/YÃ¼kseklik) |
| :--- | :--- | :--- | :--- |
| **normal\_A** | Mavi, DÃ¼z | $\mathbf{x \approx -2.5}$ civarÄ±ndadÄ±r. | En **dar** ve en **yÃ¼ksek** eÄŸridir. **En kÃ¼Ã§Ã¼k $\sigma$** deÄŸerine sahiptir. |
| **normal\_B** | Turuncu, Kesik | $\mathbf{x \approx 0}$ civarÄ±ndadÄ±r. | normal\_A'dan daha geniÅŸ, ancak diÄŸerlerinden kÃ¼Ã§Ã¼ktÃ¼r. |
| **normal\_C** | YeÅŸil, Nokta-Kesik | $\mathbf{x \approx 1.5}$ civarÄ±ndadÄ±r. | normal\_B'den daha geniÅŸtir. |
| **normal\_D** | KÄ±rmÄ±zÄ±, NoktalÄ± | $\mathbf{x \approx 4}$ civarÄ±ndadÄ±r. | En **geniÅŸ** ve en **alÃ§ak** eÄŸridir. **En bÃ¼yÃ¼k $\sigma$** deÄŸerine sahiptir. |

---

### 2. SÄ±ralamalarÄ±n Ã–zeti

| Parametre | SÄ±ralama (KÃ¼Ã§Ã¼kten BÃ¼yÃ¼ÄŸe) |
| :--- | :--- |
| **Ortalama ($\mu$)** | $\mu_{\text{normal\_A}} < \mu_{\text{normal\_B}} < \mu_{\text{normal\_C}} < \mu_{\text{normal\_D}}$ |
| **Standart Sapma ($\sigma$)** | $\sigma_{\text{normal\_A}} < \sigma_{\text{normal\_B}} < \sigma_{\text{normal\_C}} < \sigma_{\text{normal\_D}}$ |

---

### 3. âœ… DoÄŸru Ä°fadelerin SeÃ§imi

Bu sÄ±ralamalara gÃ¶re, grafiÄŸi doÄŸru yorumlayan ifadeler ÅŸunlardÄ±r:

* $$\mathbf{\sigma_{\text{normal\_D}} > \sigma_{\text{normal\_A}}}$$
    (En geniÅŸ olan D, en dar olan A'dan bÃ¼yÃ¼ktÃ¼r.)

* $$\mathbf{\mu_{\text{normal\_D}} > \mu_{\text{normal\_C}}}$$
    (En saÄŸdaki D, C'den daha bÃ¼yÃ¼k ortalamaya sahiptir.)

* $$\mathbf{\sigma_{\text{normal\_C}} > \sigma_{\text{normal\_B}}}$$
    (C, B'den daha yayvan/geniÅŸ bir daÄŸÄ±lÄ±ma sahiptir.)


---
<img width="1199" height="587" alt="image" src="https://github.com/user-attachments/assets/f66e93f1-f141-456d-8302-054b7308bc2f" />

### Fayda: DaÄŸÄ±lÄ±mÄ±n StandartlaÅŸtÄ±rÄ±lmasÄ±nÄ±n FaydalarÄ±

| Benefit (Concept) | Explanation / Impact |
| :--- | :--- |
| **Standard Scale Transformation** ğŸ“ | It transforms datasets into a standard scale, making it easier to compare between different datasets. |
| **Statistical Simplification** ğŸ“Š | It simplifies statistical analysis, particularly when using techniques that assume a standard normal distribution. |
| **Machine Learning Performance** ğŸš€ | Standardizing features in machine learning can improve the convergence rate of optimization algorithms and prevent some features from dominating others, leading to improved model performance. |
---

<img width="717" height="243" alt="image" src="https://github.com/user-attachments/assets/df2a2ea2-7d39-49a5-a0e9-afaebd94f442" />

### Kurtosis Analizi: Game A vs. Game B DaÄŸÄ±lÄ±m KarÅŸÄ±laÅŸtÄ±rmasÄ± âš–ï¸

Kurtosis, bir daÄŸÄ±lÄ±mÄ±n **kuyruklarÄ±nÄ±n aÄŸÄ±rlÄ±ÄŸÄ±nÄ±** ve aÅŸÄ±rÄ± uÃ§ deÄŸerlere sahip olma eÄŸilimini Ã¶lÃ§er. YÃ¼ksek Kurtosis, daha aÄŸÄ±r kuyruklar demektir.

| Kriter (Criterion) | Game A Analizi (Game A Analysis) | Game B Analizi (Game B Analysis) |
| :--- | :--- | :--- |
| **Kazanma/Kaybetme DeÄŸerleri** | $\{-1, +2\}$ | $\{-2, -0.50, +0.50, +5\}$ |
| **AralÄ±k (Range)** | KÃ¼Ã§Ã¼k aralÄ±k ($\text{Range} = 3$ birim). | BÃ¼yÃ¼k aralÄ±k ($\text{Range} = 7$ birim). |
| **UÃ§ DeÄŸerler (Extremes)** | TÃ¼m olasÄ± sonuÃ§lar birbirine nispeten yakÄ±ndÄ±r. | Ã‡ok daha **uÃ§ sonuÃ§lar** ($\text{+5}$ ve $\text{-2}$) mevcuttur. |
| **Kuyruk AÄŸÄ±rlÄ±ÄŸÄ± (Tail Weight)** | Kuyruklar **hafif** olma eÄŸilimindedir. | Kuyruklar **aÄŸÄ±r** olma eÄŸilimindedir (daha fazla aÅŸÄ±rÄ± deÄŸer olasÄ±lÄ±ÄŸÄ±). |
| **Kurtosis Sonucu** | Daha kÃ¼Ã§Ã¼ktÃ¼r ($\text{Smaller}$). | Daha bÃ¼yÃ¼ktÃ¼r ($\text{Larger}$). |

#### Nihai SonuÃ§ (Final Conclusion)

<img width="742" height="406" alt="image" src="https://github.com/user-attachments/assets/f7b0f71d-02a3-45e1-8d98-b306052c9c88" />


Game B, Game A'ya kÄ±yasla daÄŸÄ±lÄ±mÄ±nÄ±n merkezinden Ã§ok daha uzakta yer alan bÃ¼yÃ¼k uÃ§ deÄŸerlere sahip olduÄŸu iÃ§in, **Game B'nin kurtosis deÄŸeri, Game A'nÄ±n kurtosis deÄŸerinden daha bÃ¼yÃ¼ktÃ¼r.**

$$\text{Game A's kurtosis is smaller than Game B's kurtosis.}$$
$$\text{Kurtosis}(\text{A}) < \text{Kurtosis}(\text{B})$$

---

<img width="521" height="455" alt="image" src="https://github.com/user-attachments/assets/979a8b6f-a4af-4fcb-b5ef-b1f61ba89819" />

### BaÄŸÄ±msÄ±z Normal DaÄŸÄ±lÄ±mlÄ± DeÄŸiÅŸkenlerin ToplamÄ± Analizi â•

**Verilenler (Given):**
* $X \sim \text{Normal}(3, 1^2)$
* $Y \sim \text{Normal}(2, 2^2)$

**Kural (Rule):** $X$ ve $Y$ baÄŸÄ±msÄ±z ise, $Z = X + Y$ de Normal daÄŸÄ±lÄ±m izler: $Z \sim \text{Normal}(\mu_Z, \sigma_Z^2)$.

---

#### 1. Ortalama ($\mu_Z$) HesaplanmasÄ± (Mean Calculation)

BaÄŸÄ±msÄ±z deÄŸiÅŸkenlerin ortalamasÄ±, bireysel ortalamalarÄ±n toplamÄ±dÄ±r:
$$\mu_Z = \mu_X + \mu_Y$$

| DeÄŸiÅŸken | Ortalama ($\mu$) |
| :---: | :---: |
| $X$ | $\mu_X = 3$ |
| $Y$ | $\mu_Y = 2$ |

$$\mu_Z = 3 + 2 = 5$$

---

#### 2. Varyans ($\sigma_Z^2$) HesaplanmasÄ± (Variance Calculation)

BaÄŸÄ±msÄ±z deÄŸiÅŸkenlerin varyansÄ±, bireysel varyanslarÄ±n toplamÄ±dÄ±r:
$$\sigma_Z^2 = \sigma_X^2 + \sigma_Y^2$$

| DeÄŸiÅŸken | Varyans ($\sigma^2$) |
| :---: | :---: |
| $X$ | $\sigma_X^2 = 1^2 = 1$ |
| $Y$ | $\sigma_Y^2 = 2^2 = 4$ |

$$\sigma_Z^2 = 1 + 4 = 5$$

---

#### 3. Standart Sapma ($\sigma_Z$) HesaplanmasÄ± (Standard Deviation)

Standart sapma, varyansÄ±n karekÃ¶kÃ¼dÃ¼r:
$$\sigma_Z = \sqrt{\sigma_Z^2}$$

$$\sigma_Z = \sqrt{5}$$

---

#### SonuÃ§ (Final Result)

$Z = X + Y$ deÄŸiÅŸkeni, $\text{Normal}(\mu, \sigma^2)$ daÄŸÄ±lÄ±mÄ±na sahiptir:
$$\mu = 5, \quad \sigma = \sqrt{5}$$

---

<img width="680" height="646" alt="image" src="https://github.com/user-attachments/assets/8995caa0-e725-45a3-a22e-c8194d872070" />

### Kutu GrafiÄŸi Analizi: Class A vs. Class B Test SkorlarÄ± ğŸ“Š

Bu analizde, medyan (merkez) ve Ã§eyrekler arasÄ± aralÄ±k (yayÄ±lÄ±m) karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

| Ä°statistiksel Ã–lÃ§Ã¼ | TanÄ±m (Definition) | Class A DeÄŸerleri | Class B DeÄŸerleri |
| :--- | :--- | :--- | :--- |
| **Medyan (Median)** ğŸ”´ | Kutunun iÃ§indeki yatay kÄ±rmÄ±zÄ± Ã§izgidir. | YaklaÅŸÄ±k $\mathbf{74}$ | YaklaÅŸÄ±k $\mathbf{85}$ |
| **IQR (Ã‡eyrekler ArasÄ± AralÄ±k)** ğŸ“ | Kutunun yÃ¼ksekliÄŸidir ($\text{Q3} - \text{Q1}$). | $\text{Q3} \approx 79, \text{Q1} \approx 60 \implies \text{IQR} \approx \mathbf{19}$ | $\text{Q3} \approx 90, \text{Q1} \approx 80 \implies \text{IQR} \approx \mathbf{10}$ |

---

#### Temel SonuÃ§lar (Key Findings)

* **Medyan KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Class B'nin medyan skoru ($\mathbf{85}$), Class A'nÄ±n medyan skorundan ($\mathbf{74}$) **daha yÃ¼ksektir**. ($\text{Class B}$ daha iyi bir merkezi eÄŸilime sahiptir.)
* **IQR KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Class A'nÄ±n IQR'Ä± ($\mathbf{19}$), Class B'nin IQR'Ä±ndan ($\mathbf{10}$) **daha bÃ¼yÃ¼ktÃ¼r**. ($\text{Class A}$'nÄ±n skorlarÄ± daha fazla yayÄ±lmÄ±ÅŸtÄ±r/daÄŸÄ±nÄ±ktÄ±r.)

### Kutu GrafiÄŸi Analizi: Ä°fadelerin KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± (Comparison of Statements) ğŸ¯

AÅŸaÄŸÄ±daki tablo, Class A ve Class B iÃ§in hesaplanan Medyan ve IQR deÄŸerlerine gÃ¶re verilen ifadelerin doÄŸruluÄŸunu kontrol etmektedir.

| Ä°fade (Statement) | DeÄŸerler (Values) | SonuÃ§ (Result) |
| :--- | :--- | :--- |
| **Class A's median score is higher than Class B's median score.** | $74 > 85$ | YanlÄ±ÅŸ (False) âŒ |
| **Class B's interquartile range (IQR) is larger than Class A's interquartile range.** | $10 > 19$ | YanlÄ±ÅŸ (False) âŒ |
| **Class A's interquartile range (IQR) is larger than Class B's interquartile range.** | $19 > 10$ | **DoÄŸru (True)** âœ… |
| **Class B's median score is higher than Class A's median score.** | $85 > 74$ | **DoÄŸru (True)** âœ… |

#### Analiz Ã–zeti

GrafiÄŸe gÃ¶re, **Class B** daha yÃ¼ksek bir merkezi eÄŸilime (Medyan = 85) sahipken, **Class A** daha bÃ¼yÃ¼k bir yayÄ±lÄ±ma ($\text{IQR} = 19$) sahiptir.

### Kutu GrafiÄŸi SonuÃ§larÄ±: DoÄŸru Ä°fadeler (Final Box Plot Conclusions) âœ…

Analiz sonucunda, verilen dÃ¶rt seÃ§enek arasÄ±ndan **iki ifadenin** doÄŸru olduÄŸu belirlenmiÅŸtir. Bu durum, veri setinin hem merkezindeki hem de yayÄ±lÄ±mÄ±ndaki farklÄ±lÄ±klarÄ± yansÄ±tmaktadÄ±r.

| Ä°fade (Statement) | Dayanak (Evidence) | SonuÃ§ (Result) |
| :--- | :--- | :--- |
| **Class A's interquartile range (IQR) is larger than Class B's interquartile range.** | $\text{IQR(A): 19} \quad > \quad \text{IQR(B): 10}$ | **DoÄŸru (True)** |
| **Class B's median score is higher than Class A's median score.** | $\text{Median(B): 85} \quad > \quad \text{Median(A): 74}$ | **DoÄŸru (True)** |

---

#### Ã–zet (Summary)

Class B'nin **daha yÃ¼ksek bir performansa** (daha yÃ¼ksek medyan) sahip olduÄŸu, ancak Class A'nÄ±n skorlarÄ±nÄ±n **daha daÄŸÄ±nÄ±k** (daha bÃ¼yÃ¼k IQR) olduÄŸu sonucuna varÄ±lmÄ±ÅŸtÄ±r.

---

<img width="560" height="599" alt="image" src="https://github.com/user-attachments/assets/d7d014ed-90f3-4423-b5c5-f7d020806d74" />

### QQ Plot Analizi: Normal DaÄŸÄ±lÄ±mÄ±n DeÄŸerlendirilmesi ğŸ“ğŸ“Š

Bu analiz, bir veri setinin **QQ Plot** (Kuantil-Kuantil GrafiÄŸi) kullanÄ±larak Normal ($\text{Gaussian}$) daÄŸÄ±lÄ±ma ne kadar uyduÄŸunu deÄŸerlendirmektedir.

| Kriter (Criterion) | GÃ¶zlem (Observation) | SonuÃ§ (Implication) |
| :--- | :--- | :--- |
| **Genel Kural** | Veri noktalarÄ± dÃ¼z Ã§izgiyi takip ediyorsa, veri Normal daÄŸÄ±lÄ±ma uyar. Veri noktalarÄ± Ã§izgiden sapÄ±yorsa, daÄŸÄ±lÄ±m gÃ¶stermez. | QQ Plot, verinin daÄŸÄ±lÄ±m uyumunu belirlemenin en iyi yoludur. |
| **Merkez BÃ¶lge** ğŸŸ¢ | Veri noktalarÄ±nÄ±n bÃ¼yÃ¼k bir kÄ±smÄ± ($-1$'den $1$'e kadar olan kÄ±sÄ±m), turuncu Ã§izginin Ã¼zerinde Ã§ok yakÄ±ndÄ±r. | Verinin merkezinin Normal daÄŸÄ±lÄ±ma **gÃ¼Ã§lÃ¼ bir ÅŸekilde** uyduÄŸunu gÃ¶sterir. |
| **Kuyruklar (UÃ§lar)** âš ï¸ | Noktalar, hem alt kuyrukta (yaklaÅŸÄ±k $-1$'den sonra) hem de Ã¼st kuyrukta (yaklaÅŸÄ±k $1$'den sonra) Ã§izgiden hafifÃ§e sapmaktadÄ±r. | DaÄŸÄ±lÄ±mÄ±n kusursuz Normal olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir, ancak sapma kÃ¼Ã§Ã¼ktÃ¼r. |
| **Nihai Karar** âœ… | SapmalarÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼, tÃ¼m veri setini "Normal DaÄŸÄ±lmÄ±ÅŸ DeÄŸil" olarak sÄ±nÄ±flandÄ±rmak iÃ§in genellikle yeterli deÄŸildir. | **The data looks normally distributed.** (Veri, Normal daÄŸÄ±lmÄ±ÅŸ gÃ¶rÃ¼nÃ¼yor.) |

---

<img width="1077" height="490" alt="image" src="https://github.com/user-attachments/assets/2d51ea35-a613-4ae1-9d9c-8db533ba4c7b" />

---

<img width="1076" height="492" alt="image" src="https://github.com/user-attachments/assets/4b7028bb-a6b7-4750-b775-4937b2213b2e" />

---

<img width="361" height="149" alt="image" src="https://github.com/user-attachments/assets/dc036e40-7f62-4faf-88c7-6b21cf5e2af5" />

---
<img width="706" height="335" alt="image" src="https://github.com/user-attachments/assets/0b875d7d-3e2a-46c0-a74f-107855a6e3a5" />

### AyrÄ±k OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ±nÄ±n Beklenen DeÄŸeri ($E[X]$) HesaplamasÄ± ğŸ§®

Bu problem, rastgele deÄŸiÅŸken $X$'in alacaÄŸÄ± deÄŸerlerin kendi olasÄ±lÄ±klarÄ± ile Ã§arpÄ±lÄ±p toplanmasÄ±yla bulunan **Beklenen Ortalama** ($\mu$) deÄŸerini hesaplamayÄ± gerektirir.

#### Temel FormÃ¼l (Expected Value Formula)
$$\mu = E[X] = \sum_{i} x_i \cdot P(x_i)$$

---

#### 1. Veri DeÄŸerleri ve OlasÄ±lÄ±klar Tablosu ğŸ”¢

| $X$ DeÄŸeri ($x_i$) | OlasÄ±lÄ±k ($P(x_i)$) |
| :---: | :---: |
| $1$ | $0.3$ |
| $3$ | $0.4$ |
| $5$ | $0.3$ |

---

#### 2. Beklenen DeÄŸer ($E[X]$) HesaplamasÄ±

Her $x_i$ deÄŸerini karÅŸÄ±lÄ±k gelen $P(x_i)$ olasÄ±lÄ±ÄŸÄ± ile Ã§arpÄ±p toplayalÄ±m:

$$E[X] = (1 \cdot 0.3) + (3 \cdot 0.4) + (5 \cdot 0.3)$$

$$E[X] = 0.3 + 1.2 + 1.5$$

$$E[X] = 3.0$$

---

#### SonuÃ§ (Final Result)

Beklenen ortalama ($\mu$) ÅŸuna eÅŸittir:
$$\mu = 3.0$$

---

<img width="709" height="414" alt="image" src="https://github.com/user-attachments/assets/3de533ee-51e0-43d5-89a5-2750bbc2fc8a" />

### Ortak OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ± Analizi: Ã‡ift DeÄŸer OlasÄ±lÄ±ÄŸÄ± ($P(X=\text{even}, Y=\text{even})$) ğŸ²

Bu problem, iki rastgele deÄŸiÅŸkenin ($X$ ve $Y$) **bir arada Ã§ift deÄŸer** almasÄ± olasÄ±lÄ±ÄŸÄ±nÄ± bulmayÄ± gerektirir.

---

#### 1. KoÅŸulun Belirlenmesi (Defining the Condition)

| DeÄŸiÅŸken (Variable) | OlasÄ± DeÄŸerler (Possible Values) | Ã‡ift DeÄŸer (Even Value) |
| :---: | :---: | :---: |
| $X$ | $\{1, 2, 3\}$ | $X=2$ |
| $Y$ | $\{1, 2\}$ | $Y=2$ |

KoÅŸul: $X$ ve $Y$ her ikisi de Ã§ift deÄŸer alsÄ±n.
$$\text{Aranan OlasÄ±lÄ±k: } P(X=2, Y=2)$$

---

#### 2. OlasÄ±lÄ±ÄŸÄ±n Tablodan OkunmasÄ± (Reading the Joint Probability)

Verilen ortak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± tablosu:

| $X \setminus Y$ | $1$ | $2$ | $3$ |
| :---: | :---: | :---: | :---: |
| $1$ | $0.1$ | $0.2$ | $0.3$ |
| $2$ | $0.2$ | **0.1** | $0.1$ |

Tabloda $X=2$ satÄ±rÄ± ile $Y=2$ sÃ¼tununun kesiÅŸimi aranan olasÄ±lÄ±ktÄ±r:

$$P(X=2, Y=2) = 0.1$$

---

#### Nihai SonuÃ§ (Final Result) âœ…

DoÄŸru hesaplama ve tablodan okunan deÄŸer ÅŸudur:

$$\mathbf{P(X=2, Y=2) = 0.1}$$

---

<img width="699" height="354" alt="image" src="https://github.com/user-attachments/assets/43b4b95f-fd08-4fdd-aa5f-fccf8190a6ea" />


### KoÅŸullu OlasÄ±lÄ±k HesaplamasÄ±: $P(X=3 \mid Y=1)$ ğŸ¯

Bu problem, ortak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± tablosu kullanÄ±larak koÅŸullu olasÄ±lÄ±ÄŸÄ±n hesaplanmasÄ±nÄ± gerektirir.

#### KoÅŸullu OlasÄ±lÄ±k FormÃ¼lÃ¼
$$P(X=x \mid Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}$$

---

#### 1. Payda HesaplamasÄ±: Marjinal OlasÄ±lÄ±k $P(Y=1)$ â•

$P(Y=1)$, $Y=1$ sÃ¼tunundaki tÃ¼m deÄŸerlerin toplamÄ±dÄ±r:

| $X$ DeÄŸeri | $P(X, Y=1)$ |
| :---: | :---: |
| 1 | $0.05$ |
| 2 | $0.10$ |
| 3 | $0.15$ |
| **Toplam** | $\mathbf{0.30}$ |

$$P(Y=1) = 0.05 + 0.10 + 0.15 = 0.30$$

---

#### 2. Pay DeÄŸeri: Ortak OlasÄ±lÄ±k $P(X=3, Y=1)$

Tablodan okunan deÄŸer:
$$P(X=3, Y=1) = 0.15$$

---

#### 3. Nihai Hesaplama (Final Calculation)

$$P(X=3 \mid Y=1) = \frac{P(X=3, Y=1)}{P(Y=1)} = \frac{0.15}{0.30}$$

$$\mathbf{P(X=3 \mid Y=1) = 0.5}$$

---

<img width="731" height="167" alt="image" src="https://github.com/user-attachments/assets/52ab6daf-7543-4fc5-937b-cb656964b808" />


### Kovaryans HesaplamasÄ±: $\text{Cov}(X, Y)$ ğŸ”—

Bu analiz, $\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$ formÃ¼lÃ¼ne dayanmaktadÄ±r.

#### Ortak OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ± Tablosu
| $X \setminus Y$ | 0 | 1 | **$P(X=x)$** |
| :---: | :---: | :---: | :---: |
| 0 | $0.2$ | $0.1$ | $0.3$ |
| 1 | $0.1$ | $0.6$ | $0.7$ |
| **$P(Y=y)$** | $0.3$ | $0.7$ | **1.0** |

---

#### 1. Beklenen DeÄŸerlerin HesaplanmasÄ± (Expected Values)

* **$E[X]$:** $(0 \cdot 0.3) + (1 \cdot 0.7) = \mathbf{0.7}$
* **$E[Y]$:** $(0 \cdot 0.3) + (1 \cdot 0.7) = \mathbf{0.7}$

---

#### 2. Ã‡arpÄ±mÄ±n Beklenen DeÄŸeri ($E[XY]$)

| $x$ | $y$ | $x \cdot y$ | $P(x, y)$ | $(x \cdot y) \cdot P(x, y)$ |
| :---: | :---: | :---: | :---: | :---: |
| 0 | 0 | 0 | 0.2 | 0.0 |
| 0 | 1 | 0 | 0.1 | 0.0 |
| 1 | 0 | 0 | 0.1 | 0.0 |
| 1 | 1 | 1 | 0.6 | 0.6 |

$$E[XY] = 0.0 + 0.0 + 0.0 + 0.6 = \mathbf{0.6}$$

---

#### 3. KovaryansÄ±n HesaplanmasÄ± (Final Covariance)

$$\text{Cov}(X, Y) = E[XY] - E[X] E[Y]$$
$$\text{Cov}(X, Y) = 0.6 - (0.7) \cdot (0.7)$$
$$\text{Cov}(X, Y) = 0.6 - 0.49$$

$$\mathbf{\text{Cov}(X, Y) = 0.11}$$

---

# ğŸ“ˆ Merkezi Limit Teoremi (CLT) vs. BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | Merkezi Limit Teoremi (CLT) ğŸ’¡ | BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) âš–ï¸ |
| :--- | :--- | :--- |
| **Temel TanÄ±m** | Bir popÃ¼lasyondan alÄ±nan baÄŸÄ±msÄ±z ve aynÄ± daÄŸÄ±lÄ±ma sahip **Ã¶rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±**, Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) yeterince arttÄ±kÃ§a, popÃ¼lasyonun orijinal daÄŸÄ±lÄ±mÄ± ne olursa olsun, **Normal DaÄŸÄ±lÄ±ma** yaklaÅŸÄ±r. | Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) arttÄ±kÃ§a, **Ã¶rneklem ortalamasÄ± ($\bar{x}$)**, popÃ¼lasyonun gerÃ§ek **ortalamasÄ±na ($\mu$)** yaklaÅŸÄ±r ve ona yakÄ±n kalÄ±r. |
| **Odak NoktasÄ±** | **DaÄŸÄ±lÄ±mÄ±n Åekli** (Ã–rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ± nasÄ±l gÃ¶rÃ¼nÃ¼r?) | **OrtalamanÄ±n DeÄŸeri** (Ã–rneklem ortalamasÄ± nereye gider?) |
| **Ä°stenen $n$** | DaÄŸÄ±lÄ±mÄ±n Normal'e yakÄ±nsamasÄ± iÃ§in genellikle $n \ge 30$ olmasÄ± beklenir. | Ne kadar bÃ¼yÃ¼k olursa o kadar iyi; yakÄ±nsama iÃ§in tek ÅŸart $n \to \infty$ (sonsuz) olmasÄ±dÄ±r. |
| **Matematiksel Ä°fade** | $$\bar{X} \xrightarrow{d} N(\mu, \sigma^2/n)$$ (DaÄŸÄ±lÄ±m, Normal DaÄŸÄ±lÄ±ma yakÄ±nsar) | $$\bar{X} \xrightarrow{p} \mu$$ (Ortalama, deÄŸere olasÄ±lÄ±kla yakÄ±nsar) |
| **Varyans Ä°liÅŸkisi** | Ã–rnekleme daÄŸÄ±lÄ±mÄ±nÄ±n varyansÄ± $\sigma^2/n$'dir, bu da $n$ arttÄ±kÃ§a **daÄŸÄ±lÄ±mÄ±n daraldÄ±ÄŸÄ±** anlamÄ±na gelir (Daha kesin tahmin). | LLN, esas olarak merkezi eÄŸilimle ilgilenir, varyansÄ±n azalmasÄ± CLT'nin bir sonucudur. |
| **Pratik AnlamÄ±** | Ä°statistiksel testlerin (t-testi, Z-testi) ve gÃ¼ven aralÄ±klarÄ±nÄ±n oluÅŸturulmasÄ±nÄ±n temelini saÄŸlar. PopÃ¼lasyon bilinmese bile Ã§Ä±karÄ±m yapmayÄ± mÃ¼mkÃ¼n kÄ±lar. | Uzun vadede gÃ¶zlemlenen sonucun teorik beklentiye eÅŸit olacaÄŸÄ±nÄ± garanti eder (Ã–r. 1000 kez yazÄ± tura atÄ±ldÄ±ÄŸÄ±nda yazÄ± gelme oranÄ±nÄ±n %50'ye yaklaÅŸmasÄ±). |
| **Ã–rnek** | Bir madeni parayÄ± 30 kez Ã§evirip ortalama tura gelme sayÄ±sÄ±nÄ± defalarca kaydettiÄŸinizde, bu ortalamalarÄ±n bir histogramÄ± Ã§an eÄŸrisine benzer. | Bir madeni parayÄ± defalarca Ã§evirdiÄŸinizde, tura gelme oranÄ±nÄ±n $\frac{1}{2}$ deÄŸerine giderek daha Ã§ok yaklaÅŸmasÄ±. |

### Metaforik KarÅŸÄ±laÅŸtÄ±rma (Ã–rnekler) ğŸ²ğŸ¯

| Teorem | Metafor | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **Merkezi Limit Teoremi (CLT) ğŸ“** | **Oyunun KurallarÄ±** | Bir zar oyunu oynarken, tek bir zarÄ±n sonucu rastgele (dÃ¼zgÃ¼n daÄŸÄ±lÄ±m). Ancak **Ã§ok sayÄ±da** tur oynarsanÄ±z ve her turdaki ortalama puanÄ± toplarsanÄ±z, bu ortalama puanlarÄ±n grafiÄŸi bir **Ã§an eÄŸrisi** (normal daÄŸÄ±lÄ±m) ÅŸeklini alÄ±r. CLT bize, oyunun kurallarÄ± (daÄŸÄ±lÄ±mÄ±n ÅŸekli) ne olursa olsun, *bÃ¼yÃ¼k serilerin* hep aynÄ± ÅŸekilde davrandÄ±ÄŸÄ±nÄ± sÃ¶yler. |
| **BÃ¼yÃ¼k SayÄ±lar YasasÄ± (LLN) ğŸ¯** | **Hedefe UlaÅŸma** | Bir hedef tahtasÄ±na atÄ±ÅŸ yapÄ±yorsunuz. Ä°lk birkaÃ§ atÄ±ÅŸÄ±nÄ±z rastgele yerlere dÃ¼ÅŸebilir. Ancak atÄ±ÅŸ sayÄ±nÄ±zÄ± **binlerceye** Ã§Ä±kardÄ±ÄŸÄ±nÄ±zda, atÄ±ÅŸlarÄ±nÄ±zÄ±n **ortalamasÄ±** (merkezi) hedefin tam ortasÄ±na (gerÃ§ek popÃ¼lasyon ortalamasÄ±na) giderek daha Ã§ok yaklaÅŸacaktÄ±r. LLN, bize yeterince deneme yaparsak **hedefi vuracaÄŸÄ±mÄ±zÄ±** garanti eder. |

---

# Maksimum Olabilirlik Tahmincisi (Maximum Likelihood Estimation - MLE)
## Gauss PopÃ¼lasyonu Ä°Ã§in MLE (MLE for Gaussian Population)


### Matematiksel FormÃ¼lasyon (Mathematical Formulation)
OrtalamasÄ± $\mu$ ve varyansÄ± $\sigma^2$ olan bir Gauss daÄŸÄ±lÄ±mÄ±ndan alÄ±nan $n$ Ã¶rnekleme $X=(X_1, X_2, \dots, X_n)$ sahip olduÄŸunuzu varsayalÄ±m. Bu, $X_i \sim_{i.i.d.} N(\mu, \sigma^2)$ anlamÄ±na gelir.

EÄŸer $\mu$ ve $\sigma$ iÃ§in MLE istiyorsanÄ±z, ilk adÄ±m **Olabilirlik Fonksiyonu (Likelihood)**'nu tanÄ±mlamaktÄ±r. EÄŸer hem $\mu$ hem de $\sigma$ bilinmiyorsa, olabilirlik bu iki parametrenin bir fonksiyonu olacaktÄ±r. $x=(x_1, x_2, \dots, x_n)$ ile verilen $X$'in bir gerÃ§ekleÅŸimi (realization) iÃ§in:

$$ L(\mu,\sigma; \boldsymbol{x}) = \prod_{i=1}^n f_{X_i}(x_i) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma } e^{-\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2}} $$
$$ L(\mu,\sigma; \boldsymbol{x}) = \frac{1}{(\sqrt{2\pi})^n\sigma^n }e^{-\frac{1}{2}\frac{\sum_{i=1}^n (x_i-\mu)^2}{\sigma^2}} $$

Åimdi yapmanÄ±z gereken tek ÅŸey, olabilirlik $L(\mu, \sigma; \boldsymbol{x})$'i maksimize eden $\mu$ ve $\sigma$ deÄŸerlerini bulmaktÄ±r.

### Log-Olabilirlik (Log-Likelihood) Fonksiyonu

Olabilirlik fonksiyonunun tÃ¼revini almak karmaÅŸÄ±k olduÄŸu iÃ§in, logaritma fonksiyonunun her zaman artan olmasÄ±ndan faydalanarak **Log-Olabilirlik Fonksiyonu** kullanÄ±lÄ±r:

$$ \ell(\mu,\sigma) = \log(L(\mu,\sigma; \boldsymbol{x})) $$

LogaritmanÄ±n Ã§arpÄ±mÄ± toplama dÃ¶nÃ¼ÅŸtÃ¼rme Ã¶zelliÄŸini ($\log(a \cdot b) = \log(a) + \log(b)$) ve diÄŸer logaritma Ã¶zelliklerini kullanarak Log-Olabilirlik ÅŸu ÅŸekilde basitleÅŸtirilir:

$$ \ell(\mu,\sigma) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2}\frac{\sum_{i=1}^n (x_i-\mu)^2}{\sigma^2} $$

### MLE'nin TÃ¼retilmesi (Derivation of MLE)

MLE iÃ§in $\mu$ ve $\sigma$ deÄŸerlerini bulmak iÃ§in, Log-Olabilirlik'in kÄ±smi tÃ¼revleri (partial derivatives) alÄ±nÄ±r ve sÄ±fÄ±ra eÅŸitlenir.

#### a) $\mu$ Ä°Ã§in KÄ±smi TÃ¼rev ($\partial / \partial \mu$):

$$\frac{\partial }{\partial \mu}\ell(\mu, \sigma) = \frac{1}{\sigma^2}\left(\sum_{i=1}^n x_i - n\mu\right) = 0$$

$\sigma > 0$ olduÄŸu iÃ§in $\sum_{i=1}^n x_i - n\mu = 0$ olmalÄ±dÄ±r. Buradan $\mu$ iÃ§in MLE tahmini:

$$\hat{\mu} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}$$
**SonuÃ§:** Ortalama iÃ§in MLE, **Ã–rneklem OrtalamasÄ± (Sample Mean)**'dÄ±r.

#### b) $\sigma$ Ä°Ã§in KÄ±smi TÃ¼rev ($\partial / \partial \sigma$):

$$\frac{\partial }{\partial \sigma}\ell(\mu, \sigma) = -\frac{n}{\sigma} + \left(\sum_{i=1}^n (x_i-\mu)^2\right)\frac{1}{\sigma^3} = 0$$

$\mu$'yu $\hat{\mu}=\bar{x}$ ile deÄŸiÅŸtirip $\sigma > 0$ olduÄŸu iÃ§in ifadeyi basitleÅŸtirirsek:

$$\frac{\partial }{\partial \sigma}\ell(\mu, \sigma) = -n + \left(\sum_{i=1}^n (x_i-\bar{x})^2\right)\frac{1}{\sigma^2} = 0$$

Buradan varyans iÃ§in MLE tahmini:

$$\hat{\sigma}^2 = \frac{\sum(x_i-\bar{x})^2}{n}$$

**SonuÃ§:** Standart sapma iÃ§in MLE ($\hat{\sigma}$), bu ifadenin karekÃ¶kÃ¼dÃ¼r. Bu ifade, Ã¶rneklem standart sapmasÄ± (sample standard deviation) iÃ§in Ã¶ÄŸrendiÄŸiniz formÃ¼le Ã§ok benzerdir, tek fark **$1/n$** ile normalleÅŸtirme yapÄ±lmasÄ±dÄ±r. Ã–rneklem standart sapmasÄ± ise **$1/(n-1)$** kullanÄ±r.
---

# ğŸ“Š Veri Bilimi ve Makine Ã–ÄŸrenimi Ã–ÄŸrenim PlanÄ±

Bu belge, Veri Bilimi ve Makine Ã–ÄŸrenimi alanÄ±ndaki Coursera eÄŸitimlerini mantÄ±ksal aÅŸamalara gÃ¶re gruplandÄ±rÄ±lmÄ±ÅŸ bir Ã§alÄ±ÅŸma planÄ±nÄ± sunar.

---

## ğŸš€ Maksimum Olabilirlik Tahmincisi (MLE) AÃ§Ä±klamasÄ±

MLE, eldeki veriyi en olasÄ± (highest likelihood) kÄ±lan model parametrelerini bulma yÃ¶ntemidir. ML'de Ã‡apraz Entropi gibi maliyet fonksiyonlarÄ±nÄ±n temelini oluÅŸturur.

### MLE vs. En KÃ¼Ã§Ã¼k Kareler (Least Squares) KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Kriter | Maksimum Olabilirlik Tahmincisi (MLE) ğŸ¯ | En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) ğŸ“ |
| :--- | :--- | :--- |
| **Temel Felsefe** | OlasÄ±lÄ±ÄŸa (Likelihood) dayanÄ±r. | HatalarÄ±n karesini minimize etmeye dayanÄ±r. |
| **Hesaplama AmacÄ±** | GÃ¶zlemleri **en olasÄ±** yapan $\theta$ parametrelerini bulmak. | Tahminler ($ \hat{y} $) ile gerÃ§ek deÄŸerler ($ y $) arasÄ±ndaki **mesafeyi** minimize etmek. |
| **Gereken VarsayÄ±m** | Verinin **olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±** bilmek zorunludur (Normal, Bernoulli vb.). | HatanÄ±n daÄŸÄ±lÄ±mÄ± hakkÄ±nda aÃ§Ä±k bir varsayÄ±m yapmaz. |
| **ML'deki KarÅŸÄ±lÄ±ÄŸÄ±** | Ã‡apraz Entropi KaybÄ± (Cross-Entropy Loss), Lojistik Regresyon. | Ortalama Karesel Hata (Mean Squared Error - MSE). |
| **EÅŸitlik Durumu** | Hata terimleri **Normal DaÄŸÄ±lÄ±m'a** sahipse, MLE'yi maksimize etmek, LS'yi minimize etmeye eÅŸdeÄŸerdir. |

---

# ğŸ¤– Makine Ã–ÄŸreniminde Maksimum Olabilirlik (Maximum Likelihood Estimation - MLE)

### AmaÃ§: ğŸ¯

MLE'nin temel amacÄ±, elimizdeki gÃ¶zlemlenen veriyi ($\mathbf{X}$) en olasÄ± (en yÃ¼ksek olasÄ±lÄ±klÄ±) hale getiren model parametrelerini ($\theta$) bulmaktÄ±r.

BaÅŸka bir deyiÅŸle, "EÄŸer modelimin parametreleri $\theta$ olsaydÄ±, bu veriyi gÃ¶rme olasÄ±lÄ±ÄŸÄ±m ne olurdu?" sorusuna cevap vererek bu olasÄ±lÄ±ÄŸÄ± maksimize etmektir.


---

### ML'de KullanÄ±m AlanÄ± ve RolÃ¼: ğŸ› ï¸

MLE, genellikle modelin Ã§Ä±ktÄ±sÄ±nÄ±n olasÄ±lÄ±ksal (probabilistic) olarak modellendiÄŸi durumlarda bir **Maliyet Fonksiyonu (Loss Function)** olarak kullanÄ±lÄ±r.

#### 1. SÄ±nÄ±flandÄ±rma (Classification):

* Ã–zellikle **Lojistik Regresyon (Logistic Regression)** ve **Yapay Sinir AÄŸlarÄ± (Neural Networks)** gibi modellerde, tahmin edilen Ã§Ä±ktÄ±nÄ±n bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (Ã¶rneÄŸin, Bernoulli veya Kategorik daÄŸÄ±lÄ±m) olduÄŸu varsayÄ±lÄ±r.
* Bu modellerde kullanÄ±lan **Ã‡apraz Entropi KaybÄ± (Cross-Entropy Loss)**, aslÄ±nda MLE'nin bir uygulamasÄ±dÄ±r. Ã‡apraz Entropi'yi minimize etmek, doÄŸru sÄ±nÄ±fÄ± gÃ¶zlemleme olasÄ±lÄ±ÄŸÄ±nÄ± maksimize eden parametreleri bulmaya eÅŸdeÄŸerdir.

#### 2. Regresyon (Regression):

* EÄŸer modelin hatalarÄ±nÄ±n (rezidÃ¼ellerinin) **Normal DaÄŸÄ±lÄ±m'a (Gaussian Distribution)** sahip olduÄŸu varsayÄ±lÄ±rsa, **En KÃ¼Ã§Ã¼k Kareler (Least Squares)** yÃ¶nteminin uygulanmasÄ±, matematiksel olarak MLE'nin uygulanmasÄ±yla aynÄ± parametre tahminlerini verir (yukarÄ±daki tÃ¼retme Ã¶rneÄŸinde olduÄŸu gibi).

#### 3. Ãœretici Modeller (Generative Models):

* Veri kÃ¼mesinin tamamÄ±nÄ±n nasÄ±l oluÅŸturulduÄŸunu Ã¶ÄŸrenen (Ã¶rneÄŸin, **Naif Bayes (NaÃ¯ve Bayes)**) modeller, parametrelerini tahmin etmek iÃ§in sÄ±klÄ±kla MLE'yi kullanÄ±r.

### âš–ï¸ MLE ve En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) KarÅŸÄ±laÅŸtÄ±rmasÄ±

En KÃ¼Ã§Ã¼k Kareler (LS), genellikle regresyonda kullanÄ±lan basit bir maliyet fonksiyonudur.

| Kriter | Maksimum Olabilirlik Tahmincisi (MLE) ğŸ¯ | En KÃ¼Ã§Ã¼k Kareler (Least Squares - LS) ğŸ“ |
| :--- | :--- | :--- |
| **Temel Felsefe** | OlasÄ±lÄ±ÄŸa (Likelihood) dayanÄ±r. | HatalarÄ±n karesini minimize etmeye dayanÄ±r. |
| **Hesaplama AmacÄ±** | GÃ¶zlemleri **en olasÄ±** yapan $\theta$ parametrelerini bulmak. | Tahminler ($\hat{y}$) ile gerÃ§ek deÄŸerler ($y$) arasÄ±ndaki **mesafeyi** minimize etmek. |
| **Gereken VarsayÄ±m** | Verinin (veya hatanÄ±n) **olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±** bilmek zorunludur (Ã¶rneÄŸin: Normal, Bernoulli, Poisson). | HatanÄ±n daÄŸÄ±lÄ±mÄ± hakkÄ±nda **aÃ§Ä±k bir varsayÄ±m yapmaz**, sadece varyansÄ±n sabit olduÄŸunu varsayar (Homoscedasticity). |
| **ML'deki KarÅŸÄ±lÄ±ÄŸÄ±** | Ã‡apraz Entropi (Cross-Entropy Loss), Lojistik Regresyon, Yapay Sinir AÄŸlarÄ± (NN). | Ortalama Karesel Hata (Mean Squared Error - MSE) ve Basit Lineer Regresyon. |
| **EÅŸitlik Durumu** | EÄŸer hata terimleri **Normal DaÄŸÄ±lÄ±m'a** sahipse, MLE'yi maksimize etmek, LS'yi minimize etmeye eÅŸdeÄŸerdir. |


# ğŸ”¬ SÄ±klÄ±kÃ§Ä± YaklaÅŸÄ±m (Frequentist Statistics)

Bu yaklaÅŸÄ±m, olasÄ±lÄ±k ve bilgi kavramlarÄ±nÄ± tamamen farklÄ± yorumlayan iki ana ekolden (SÄ±klÄ±kÃ§Ä± ve BayesÃ§i) biridir.

SÄ±klÄ±kÃ§Ä±lÄ±k, olasÄ±lÄ±ÄŸÄ±, uzun vadede bir olayÄ±n gerÃ§ekleÅŸme sÄ±klÄ±ÄŸÄ± olarak gÃ¶rÃ¼r.

## 1. SÄ±klÄ±kÃ§Ä± YaklaÅŸÄ±m (Frequentist Statistics) ğŸ“

| Kavram | AÃ§Ä±klama |
| :--- | :--- |
| **OlasÄ±lÄ±k AnlayÄ±ÅŸÄ±** | **Uzun Vadeli Olay SÄ±klÄ±ÄŸÄ± (Long-term Frequency):** OlasÄ±lÄ±k, bir deneyi sonsuz kez tekrarladÄ±ÄŸÄ±mÄ±zda bir sonucun ne sÄ±klÄ±kla ortaya Ã§Ä±kacaÄŸÄ±nÄ±n limitidir. Ã–rnek: Bir madeni paranÄ±n tura gelme olasÄ±lÄ±ÄŸÄ± %50'dir, Ã§Ã¼nkÃ¼ parayÄ± binlerce kez attÄ±ÄŸÄ±mÄ±zda tura gelme sÄ±klÄ±ÄŸÄ± bu deÄŸere yakÄ±nsar. |
| **Temel Kavram** | **Olabilirlik (Likelihood):** Elimizdeki veriler (gÃ¶zlemler) verildiÄŸinde, belirli bir model parametresinin ($\theta$) ne kadar olasÄ± olduÄŸunu Ã¶lÃ§er. Bu, genellikle $P(\text{Veri} \mid \theta)$ olarak ifade edilir. SÄ±klÄ±kÃ§Ä±lar, sadece veriye bakarak Ã§alÄ±ÅŸÄ±r. |
| **AmaÃ§** | **Veriyi En OlasÄ± Ãœreten Modeli Bulmak:** AmaÃ§, gÃ¶zlemlenen veriyi en iyi aÃ§Ä±klayan ve en yÃ¼ksek olabilirlik deÄŸerini veren **sabit model parametrelerini** bulmaktÄ±r (Ã–rn: p-deÄŸerleri, GÃ¼ven AralÄ±klarÄ± hesaplama). |
| **Parametreler** | PopÃ¼lasyon parametreleri ($\mu, \sigma$ vb.) **sabit ancak bilinmeyen** deÄŸerler olarak kabul edilir. |

# ğŸ§  BayesÃ§i YaklaÅŸÄ±m (Bayesian Statistics)

BayesÃ§ilik, olasÄ±lÄ±ÄŸÄ±, bilinmeyene olan kiÅŸisel inancÄ±n veya kesinliÄŸin derecesi olarak gÃ¶rÃ¼r.

## 2. BayesÃ§i YaklaÅŸÄ±m (Bayesian Statistics) ğŸ’¡

| Kavram | AÃ§Ä±klama |
| :--- | :--- |
| **OlasÄ±lÄ±k AnlayÄ±ÅŸÄ±** | **Ä°nanÃ§ Derecesi (Degree of Belief or Certainty):** OlasÄ±lÄ±k, bir kiÅŸinin veya sistemin, eldeki bilgi Ä±ÅŸÄ±ÄŸÄ±nda bir Ã¶nermenin doÄŸru olduÄŸuna ne kadar inandÄ±ÄŸÄ±nÄ±n sÃ¼bjektif Ã¶lÃ§Ã¼sÃ¼dÃ¼r. Yeni bilgi geldikÃ§e bu inanÃ§ gÃ¼ncellenir. |
| **Temel Kavram** | **Ã–nsel (Prior) ğŸ¤”:** Veri gÃ¶zlenmeden Ã¶nce parametrelerin ($\theta$) olasÄ± deÄŸerleri hakkÄ±ndaki inancÄ±mÄ±zdÄ±r. Ã–nsel daÄŸÄ±lÄ±m $P(\theta)$ olarak ifade edilir. BayesÃ§iler, veriyi **Ã¶n bilgi** ile birleÅŸtirir. |
| **AmaÃ§** | **Ã–nsel Ä°nancÄ± GÃ¶zlemlere DayalÄ± GÃ¼ncellemek ğŸ”„:** AmaÃ§, Bayes Teoremi'ni kullanarak Ã¶nsel inancÄ±, gÃ¶zlemlenen verilerle birleÅŸtirmek ve daha doÄŸru bir **sonsal (Posterior)** inanÃ§ elde etmektir. |
| **Parametreler** | PopÃ¼lasyon parametreleri **rastgele deÄŸiÅŸkenler** olarak kabul edilir ve bunlar hakkÄ±nda bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (inanÃ§) vardÄ±r. |


<img width="1178" height="460" alt="image" src="https://github.com/user-attachments/assets/24a11c98-87af-4091-bf0f-58b1e4fe0ed2" />

# ğŸ§  BayesÃ§i Ä°statistik ve Bernoulli DaÄŸÄ±lÄ±mÄ± Ã–rneÄŸi

BayesÃ§i Ä°statistik, olasÄ±lÄ±ÄŸÄ± inanÃ§ derecesi olarak ele alan bir Ã§Ä±karÄ±m yÃ¶ntemidir. Bernoulli daÄŸÄ±lÄ±mÄ± ise bu yÃ¶ntemin temellerini anlamak iÃ§in kullanÄ±lan en yaygÄ±n ve en basit Ã¶rneÄŸi saÄŸlar.

## 1. BayesÃ§i Ä°statistik Temel KavramlarÄ± ğŸ¯

BayesÃ§i Ã§Ä±karÄ±m, parametreleri ($\theta$) **sabit** deÄŸerler olarak deÄŸil, **rastgele deÄŸiÅŸkenler** olarak ele alÄ±r.

| BayesÃ§i Terim | FormÃ¼l | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **Ã–nsel DaÄŸÄ±lÄ±m (Prior) ğŸ’¡** | $P(\theta)$ | Veri gÃ¶zlenmeden Ã¶nce model parametresi ($\theta$) hakkÄ±ndaki **baÅŸlangÄ±Ã§ inancÄ±mÄ±zdÄ±r**. Parametrelerin muhtemel deÄŸerlerine ait olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± ifade eder. |
| **Olabilirlik Fonksiyonu (Likelihood) ğŸ”** | $P(\text{Veri} \mid \theta)$ | Belirli bir $\theta$ parametre deÄŸeri verildiÄŸinde, **gÃ¶zlemlenen veriyi** elde etme olasÄ±lÄ±ÄŸÄ±nÄ±n ne kadar yÃ¼ksek olduÄŸunu Ã¶lÃ§er. Bu, SÄ±klÄ±kÃ§Ä± istatistiÄŸin de temelini oluÅŸturan veriye dayalÄ± kÄ±sÄ±mdÄ±r. |
| **Sonsal DaÄŸÄ±lÄ±m (Posterior) ğŸ†** | $P(\theta \mid \text{Veri})$ | Ã–nsel inancÄ±n, yeni gÃ¶zlemlenen veriler Ä±ÅŸÄ±ÄŸÄ±nda **gÃ¼ncellenmiÅŸ halidir**. Bayes Teoremi kullanÄ±larak elde edilen bu daÄŸÄ±lÄ±m, parametre hakkÄ±ndaki nihai inancÄ±mÄ±zÄ± temsil eder. |
| **Bayes Teoremi** | $P(\theta \mid \text{Veri}) \propto P(\text{Veri} \mid \theta) \cdot P(\theta)$ | Sonsal $\propto$ Olabilirlik $\times$ Ã–nsel demektir. Sonsal daÄŸÄ±lÄ±mÄ±n, Olabilirlik ve Ã–nsel daÄŸÄ±lÄ±mlarÄ±n Ã§arpÄ±mÄ± ile orantÄ±lÄ± olduÄŸunu gÃ¶sterir. |

## 2. Bernoulli DaÄŸÄ±lÄ±mÄ± (Bernoulli Distribution) ğŸª™

Bernoulli daÄŸÄ±lÄ±mÄ±, yalnÄ±zca **iki olasÄ± sonuÃ§lu** (dichotomous outcome) tek bir rastgele deneyi tanÄ±mlar (Ã–rn: BaÅŸarÄ±/BaÅŸarÄ±sÄ±zlÄ±k, Evet/HayÄ±r, Tura/YazÄ±).

| Terim | FormÃ¼l | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **AmaÃ§** | $X \sim \text{Bernoulli}(\theta)$ | Tek bir denemenin sonucunu modellemek. |
| **Parametre** | $\theta$ (veya $p$) | BaÅŸarÄ± olasÄ±lÄ±ÄŸÄ±dÄ±r. $\theta \in [0, 1]$ aralÄ±ÄŸÄ±ndadÄ±r. BaÅŸarÄ±sÄ±zlÄ±k olasÄ±lÄ±ÄŸÄ± ise $1 - \theta$'dÄ±r. |
| **OlasÄ±lÄ±k KÃ¼tle Fonksiyonu (PMF)** | $P(X=x) = \theta^x (1-\theta)^{1-x}$ | $x=1$ (BaÅŸarÄ±) iÃ§in $\theta$, $x=0$ (BaÅŸarÄ±sÄ±zlÄ±k) iÃ§in $1-\theta$ sonucunu verir. |
| **Ã–rnek** | Hileli bir madeni paranÄ±n tek bir atÄ±ÅŸta Tura gelme olasÄ±lÄ±ÄŸÄ± $\theta$'dÄ±r. | $X=1$ Tura gelme, $X=0$ YazÄ± gelme durumunu temsil eder. |

## 3. Bernoulli Ã–rneÄŸi Ãœzerinden BayesÃ§i Ã‡Ä±karÄ±m (Beta-Binomial Model)

BayesÃ§i istatistikte, bir madeni paranÄ±n hileli olup olmadÄ±ÄŸÄ±nÄ± (yani $\theta$ parametresini) tahmin etmek iÃ§in Bernoulli daÄŸÄ±lÄ±mÄ± kullanÄ±lÄ±r.

| AÅŸama | Uygulama | Terimsel AÃ§Ä±klama |
| :--- | :--- | :--- |
| **1. Ã–nsel SeÃ§imi** | $\theta \sim \text{Beta}(\alpha, \beta)$ | Bernoulli/Binomial olabilirlik fonksiyonu iÃ§in yaygÄ±n olarak **Konjuge Ã–nsel (Conjugate Prior)** olan **Beta DaÄŸÄ±lÄ±mÄ±** kullanÄ±lÄ±r. Beta daÄŸÄ±lÄ±mÄ±, $\theta$ hakkÄ±ndaki baÅŸlangÄ±Ã§ inancÄ±mÄ±zÄ± temsil eder. $\alpha$ ve $\beta$, sÄ±rasÄ±yla 'baÅŸarÄ±' ve 'baÅŸarÄ±sÄ±zlÄ±k' sayÄ±sÄ± hakkÄ±ndaki Ã¶n bilgimiz gibi dÃ¼ÅŸÃ¼nÃ¼lebilir. |
| **2. Veri (Likelihood) Toplama** | $X = \{x_1, \dots, x_n\}$ | $n$ kez yapÄ±lan madeni para atÄ±ÅŸÄ± verisi toplanÄ±r. Toplam baÅŸarÄ± sayÄ±sÄ± $k = \sum x_i$ olsun. Olabilirlik fonksiyonu, Binomial daÄŸÄ±lÄ±mÄ±n formunu alÄ±r: $P(\text{Veri} \mid \theta) = \theta^k (1-\theta)^{n-k}$. |
| **3. Sonsal Hesaplama** | $\theta \mid \text{Veri} \sim \text{Beta}(\alpha', \beta')$ | Bayes Teoremi uygulandÄ±ÄŸÄ±nda, Sonsal DaÄŸÄ±lÄ±m da yine bir **Beta DaÄŸÄ±lÄ±mÄ±** Ã§Ä±kar. <br>**GÃ¼ncellenmiÅŸ Parametreler:** <br>$\alpha' = \alpha + k$ <br>$\beta' = \beta + (n-k)$ |
| **SonuÃ§** | Yeni sonsal daÄŸÄ±lÄ±m, hem eski inancÄ±mÄ±zÄ± ($\alpha, \beta$) hem de yeni gÃ¶zlemlenen veriyi ($k, n-k$) birleÅŸtirir. $\theta$'nÄ±n artÄ±k **en olasÄ±** deÄŸeri, Sonsal Beta DaÄŸÄ±lÄ±mÄ±nÄ±n modu veya ortalamasÄ± olarak alÄ±nÄ±r. |

<img width="1126" height="502" alt="image" src="https://github.com/user-attachments/assets/d55cd874-50ae-4306-a5a4-932963ef3c4f" />

# ğŸ§  BayesÃ§i Ä°statistik: (Final Summary)

## 1. BayesÃ§iler Ã–nsel Ä°nanÃ§larÄ± GÃ¼nceller (Bayesians update prior beliefs) ğŸ”„

* **AÃ§Ä±klama:** BayesÃ§i yaklaÅŸÄ±mÄ±n temel iÅŸlevi, **Ã–nsel Ä°nanÃ§ (Prior Belief - $P(\theta)$)** olarak adlandÄ±rÄ±lan baÅŸlangÄ±Ã§taki bilgimizi veya varsayÄ±mÄ±mÄ±zÄ±, gÃ¶zlemlenen yeni verilerle (**Olabilirlik / Likelihood**) birleÅŸtirerek **Sonsal Ä°nanÃ§ (Posterior Belief - $P(\theta \mid \text{Veri})$)** elde etmektir.
* **Ã–rnek:** Bir madeni paranÄ±n hileli olduÄŸuna inanÄ±yorsunuz ($\theta \approx 0.7$ Tura). Bu, sizin Ã¶nsel inancÄ±nÄ±zdÄ±r. ParayÄ± 100 kez attÄ±nÄ±z ve sadece 48 kez tura geldi. Bayes Teoremi, sizin ilk inancÄ±nÄ±zÄ± bu yeni veriyle birleÅŸtirerek $\theta$'nÄ±n muhtemelen $0.5$'e daha yakÄ±n olduÄŸunu gÃ¶steren yeni (sonsal) bir daÄŸÄ±lÄ±m oluÅŸturur.

---

## 2. Bilgi Ä°Ã§ermeyen Ã–nsellerle MAP, MLE ile AynÄ±dÄ±r (MAP with uninformative priors is just MLE) âš–ï¸

* **MLE (Maksimum Olabilirlik Tahmincisi / Maximum Likelihood Estimator):** Sadece veriyi esas alÄ±r ve veriyi en olasÄ± kÄ±lan parametreyi bulur.
* **MAP (Maksimum Sonsal Tahmini / Maximum A Posteriori):** BayesÃ§i bir yÃ¶ntemdir. Veri ve Ã–nsel inancÄ±n Ã§arpÄ±mÄ±nÄ± maksimize eder.
* **Bilgi Ä°Ã§ermeyen Ã–nsel (Uninformative Prior):** Bu, parametrenin tÃ¼m olasÄ± deÄŸerlerinin eÅŸit olasÄ±lÄ±ÄŸa sahip olduÄŸunu varsayan bir Ã¶nseldir (Ã–rn: DÃ¼zgÃ¼n DaÄŸÄ±lÄ±m / Uniform Distribution).
* **KarÅŸÄ±laÅŸtÄ±rma:** EÄŸer Ã¶nsel, tÃ¼m olasÄ±lÄ±klara eÅŸit aÄŸÄ±rlÄ±k veriyorsa (yani hiÃ§bir bilgi iÃ§ermiyorsa), MAP formÃ¼lÃ¼nÃ¼n Ã¶nsel kÄ±smÄ± sabit bir sayÄ±ya dÃ¶ner. (Bu durumda MAP'i maksimize etmek, sadece **Olabilirlik (Likelihood)** kÄ±smÄ±nÄ± maksimize etmeye eÅŸittir).
* $$\text{Sonsal} \propto \text{Olabilirlik} \times \text{Ã–nsel (Sabit)}$$
* BÃ¶ylece, MAP tahminleri ($P(\text{Veri} \mid \theta) \cdot P(\theta)$) ile MLE tahminleri ($P(\text{Veri} \mid \theta)$) matematiksel olarak aynÄ± sonucu verir.

---

## 3. Yeterli Veri OlduÄŸunda, MLE ve MAP Tahminleri YakÄ±nsar (With enough data, MLE and MAP estimates usually converge) ğŸ“ˆ

* **AÃ§Ä±klama:** Veri setinin bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) Ã§ok yÃ¼ksek olduÄŸunda, veriden gelen bilgi (Olabilirlik) Ã¶nsel inanca gÃ¶re Ã§ok daha baskÄ±n hale gelir.
* **YakÄ±nsama:** Ã–nselin ne kadar gÃ¼Ã§lÃ¼ olursa olsun, Ã§ok bÃ¼yÃ¼k bir veri seti her iki yÃ¶ntemi de verinin kendisini en iyi aÃ§Ä±klayan parametreye doÄŸru iter. Bu nedenle, ML ve Ä°statistik uygulamalarÄ±nda, bÃ¼yÃ¼k veri setleriyle Ã§alÄ±ÅŸÄ±rken MLE ve MAP tahminleri genellikle **aynÄ± sonuca** ulaÅŸÄ±r.

---

## 4. SÄ±nÄ±rlÄ± Veri veya GÃ¼Ã§lÃ¼ Ã–nsel Ä°nanÃ§ OlduÄŸunda Ä°yidir (Good for instances when you have limited data or strong prior beliefs) ğŸ‘

* **SÄ±nÄ±rlÄ± Veri:** SÄ±klÄ±kÃ§Ä± MLE, az veriyle Ã§alÄ±ÅŸÄ±rken aÅŸÄ±rÄ± uyum (overfitting) eÄŸilimi gÃ¶sterir ve gÃ¼venilmez sonuÃ§lar verebilir. BayesÃ§i yaklaÅŸÄ±m, bu boÅŸluÄŸu **Ã–nsel Bilgiyle** doldurarak daha mantÄ±klÄ± ve stabilize edilmiÅŸ sonuÃ§lar Ã¼retir.
* **GÃ¼Ã§lÃ¼ Ã–nsel Ä°nanÃ§:** EÄŸer elinizde gÃ¼venilir bir uzman gÃ¶rÃ¼ÅŸÃ¼, eski deneyler veya Ã¶n bilgiler varsa, BayesÃ§ilik bu bilgiyi resmi olarak modele dahil etmenin tek yoludur.

---

## 5. YanlÄ±ÅŸ Ã–nseller, YanlÄ±ÅŸ SonuÃ§lar (Wrong priors, wrong conclusions) ğŸ›‘

* **AÃ§Ä±klama:** BayesÃ§i yaklaÅŸÄ±mÄ±n temel riskini ve felsefi eleÅŸtirisini Ã¶zetler.
* EÄŸer bir araÅŸtÄ±rmacÄ±, konuyla ilgili olmayan, hatalÄ± veya aÅŸÄ±rÄ± Ã¶nyargÄ±lÄ± bir Ã¶nsel seÃ§erse, model bu yanlÄ±ÅŸ inancÄ± kabul eder ve veri ne kadar iyi olursa olsun sonuÃ§lar hatalÄ± ve yanÄ±ltÄ±cÄ± olabilir.
* **Ã–rnek:** Bir paranÄ±n adil olduÄŸunu ispatlamaya Ã§alÄ±ÅŸÄ±yorsunuz. Ancak baÅŸlangÄ±cÄ± $\theta \approx 0.99$ (neredeyse her zaman tura) gibi yanlÄ±ÅŸ bir Ã¶nselle belirlerseniz, parayÄ± 100 kez attÄ±ÄŸÄ±nÄ±zda bile (50 tura gelse bile), sonsal inanÃ§ $0.5$'e yakÄ±nsamayacak, inatla $0.9$'un Ã¼zerinde kalacaktÄ±r.

---

##### Asagidaki tablolarda, Maksimum Olabilirlik Tahmini (Maximum Likelihood Estimation - MLE), Maksimum Sonsal Tahmini (Maximum A Posteriori - MAP) ve DÃ¼zenlileÅŸtirme (Regularization) kavramlarÄ±nÄ±n Makine Ã–ÄŸreniminde (Machine Learning) nasÄ±l birleÅŸtiÄŸini ve bu birleÅŸimin arkasÄ±ndaki BayesÃ§i (Bayesian) mantÄ±ÄŸÄ± aÃ§Ä±klamaktadÄ±r.

# ğŸ’¡ Temel Ä°statistiksel Tahmin YÃ¶ntemleri

| Kavram | Ne Ä°ÅŸe Yarar? (Ä°ÅŸlevi) | KullanÄ±m ZamanÄ± ve Yeri |
| :--- | :--- | :--- |
| **Maksimum Olabilirlik Tahmini (Maximum Likelihood Estimation - MLE) ğŸ¯** | Verilen $\theta$ parametrelerinin, **gÃ¶zlemlenen veriyi** oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± ($P(\text{Veri} \mid \theta)$) maksimize eden parametre deÄŸerlerini bulur. | Temel olarak **SÄ±klÄ±kÃ§Ä± (Frequentist)** bir yaklaÅŸÄ±mdÄ±r. Veri setinin bÃ¼yÃ¼k ve Ã¶nsel (prior) bilgiye gerek duyulmadÄ±ÄŸÄ± durumlarda, tahminci (estimator) olarak kullanÄ±lÄ±r. |
| **Maksimum Sonsal Tahmini (Maximum A Posteriori - MAP) ğŸ§ ** | Veri ve **Ã–nsel Ä°nanÃ§** ($P(\theta)$) birleÅŸtiÄŸinde, $\theta$ parametresinin en olasÄ± deÄŸerini bulur. Sonsal daÄŸÄ±lÄ±mÄ±n en yÃ¼ksek noktasÄ±dÄ±r. | Temel olarak **BayesÃ§i (Bayesian)** bir yaklaÅŸÄ±mdÄ±r. SÄ±nÄ±rlÄ± veri olduÄŸunda veya model parametreleri hakkÄ±nda gÃ¼Ã§lÃ¼ bir Ã¶nsel bilgi olduÄŸunda kullanÄ±lÄ±r. |
| **DÃ¼zenlileÅŸtirme (Regularization) âš–ï¸** | **KayÄ±p Fonksiyonuna (Loss Function)** bir ceza terimi ekleyerek modelin katsayÄ±larÄ±nÄ±n (coefficients) mutlak deÄŸerlerini veya karelerini sÄ±nÄ±rlar. Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r. | **AÅŸÄ±rÄ± Uyum (Overfitting)** riskini azaltmak ve modelin genelleÅŸtirme (generalization) yeteneÄŸini artÄ±rmak iÃ§in Regresyon ve Sinir AÄŸlarÄ± (Neural Networks) gibi birÃ§ok ML modelinde kullanÄ±lÄ±r (Ã–rn: Ridge, Lasso). |

# ğŸ¤ MAP ve DÃ¼zenlileÅŸtirmenin (Regularization) BirleÅŸimi

## 2. Logaritma DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve EÅŸitlik ğŸ’¡

Metnin en kritik kÄ±smÄ±, Ã§arpma (Ã§arpÄ±m) iÅŸlemini toplama iÅŸlemine dÃ¶nÃ¼ÅŸtÃ¼rerek (logaritma alarak) BayesÃ§i formÃ¼l ile Regresyon formÃ¼lÃ¼ arasÄ±ndaki eÅŸleÅŸmeyi gÃ¶stermesidir:

### 1. BayesÃ§i Ã‡Ä±karÄ±m (Sol Taraf):
$$\text{AmacÄ±mÄ±z} \rightarrow \text{MaksimumlaÅŸtÄ±rmak}[P(\text{Veri} \mid \text{Model}) \times P(\text{Model})]$$

### 2. Logaritma AlÄ±nmasÄ±:
$$\text{MaksimumlaÅŸtÄ±rmak}[\log(P(\text{Veri} \mid \text{Model})) + \log(P(\text{Model}))]$$

### 3. Regresyon (SaÄŸ Taraf): Logaritma dÃ¶nÃ¼ÅŸÃ¼mÃ¼nden Ã§Ä±kan terimler, bilinen Regresyon terimleriyle eÅŸleÅŸir:

| BayesÃ§i Terim (LogaritmalÄ±) | Regresyon Terimi |
| :--- | :--- |
| $\log(P(\text{Veri} \mid \text{Model}))$ | **MaksimumlaÅŸtÄ±rmak** $\log(P(\text{Veri} \mid \text{Model}))$ aynÄ± zamanda **Kare KaybÄ± (Square Loss / Hata Karesi ToplamÄ±)**'nÄ± minimize etmeye eÅŸittir. |
| $\log(P(\text{Model}))$ | **MaksimumlaÅŸtÄ±rmak** $\log(P(\text{Model}))$), **KatsayÄ±larÄ±n Karelerinin ToplamÄ±nÄ± (Sum of Squares of Coefficients)** minimize etmeye eÅŸittir. Bu terim, **DÃ¼zenlileÅŸtirme Terimi (Regularization Term)** olarak bilinir. |

### 4. Nihai SonuÃ§: ğŸš€
* **Yeni KayÄ±p = Kare KaybÄ± + DÃ¼zenlileÅŸtirme Terimi**
* MAP, Kare KaybÄ±nÄ± (Hata) minimize etmeyi ve katsayÄ±larÄ±n karelerinin toplamÄ±nÄ± (Model KarmaÅŸÄ±klÄ±ÄŸÄ±) minimize etmeyi birleÅŸtirir. Bu, **Ridge Regresyon'un (L2 DÃ¼zenlileÅŸtirme)** maliyet fonksiyonudur.

---

## 3. P(Model)'in AnlamÄ± (The Probability of a Model) ğŸ§ 

Metin, bir modelin olasÄ±lÄ±ÄŸÄ±nÄ±n ($P(\text{Model})$) ne anlama geldiÄŸini aÃ§Ä±klÄ±yor:

* **VarsayÄ±m:** Modelin katsayÄ±larÄ±nÄ±n ($a_1, a_2, \dots$) **Standart Normal DaÄŸÄ±lÄ±mdan (Standard Normal Distribution)** seÃ§ildiÄŸi varsayÄ±lÄ±r.
* **Hesaplama:** Bir modelin olasÄ±lÄ±ÄŸÄ±, tÃ¼m bu katsayÄ±larÄ± seÃ§me olasÄ±lÄ±klarÄ±nÄ±n Ã§arpÄ±mÄ±dÄ±r.
* **Basitlik ve OlasÄ±lÄ±k:** Basit bir modelin (Model 1: 1 katsayÄ±) olasÄ±lÄ±ÄŸÄ±, karmaÅŸÄ±k bir modelden (Model 3: 10 katsayÄ±) daha yÃ¼ksektir, Ã§Ã¼nkÃ¼ katsayÄ±larÄ±n Ã§arpÄ±mÄ± daha azdÄ±r. Bu da, BayesÃ§i yaklaÅŸÄ±mÄ±n, **Basitlik Prensibini (Ockham's Razor)** otomatik olarak model seÃ§imine dahil ettiÄŸini gÃ¶sterir.

# âœï¸ Ã–zet Tablo: BayesÃ§i ve Regresyon KavramlarÄ±nÄ±n EÅŸleÅŸtirilmesi

Bu tablo, BayesÃ§i yaklaÅŸÄ±mÄ±n (Ã¶zellikle MAP) logaritma ve eksi iÅŸareti dÃ¶nÃ¼ÅŸÃ¼mleri sayesinde Regresyon ve DÃ¼zenlileÅŸtirme (Regularization) olarak bilinen kavramlarla nasÄ±l matematiksel olarak eÅŸleÅŸtiÄŸini gÃ¶stermektedir.

| BayesÃ§i Kavram | Matematiksel Ä°ÅŸlem | Regresyon KavramÄ± |
| :--- | :--- | :--- |
| **MaksimumlaÅŸtÄ±r** $P(\text{Veri} \mid \text{Model})$ | Logaritma Almak ve Eksilisini Almak | **Minimize Et** Kare KaybÄ± (Square Loss) ğŸ“‰ |
| **MaksimumlaÅŸtÄ±r** $P(\text{Model})$ | Logaritma Almak ve Eksilisini Almak | **Minimize Et** DÃ¼zenlileÅŸtirme Terimi (Regularization Term) ğŸšï¸ |
| **MAP** ($P(\text{Veri} \mid \text{Model}) \cdot P(\text{Model})$) | Logaritma ToplamÄ± | **Minimize Et** Toplam KayÄ±p Fonksiyonu (Total Loss Function) ğŸ’° |

---
---

### â“ "Bir Modelin OlasÄ±lÄ±ÄŸÄ± Nedir?" (What Is the Probability of a Model?)

"Bir Modelin OlasÄ±lÄ±ÄŸÄ± Nedir?" sorusu, tek baÅŸÄ±na kesin bir istatistiksel tanÄ±ma sahip deÄŸildir. Genellikle bu soru, baÄŸlama gÃ¶re iki ana yoruma gelir:

1.  **BayesÃ§i Ä°statistik Yorumu (AsÄ±l Anlam) ğŸ§ :**
    * **AnlamÄ±:** Modelin, eldeki veriler gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda doÄŸru olma olasÄ±lÄ±ÄŸÄ± nedir?
    * **KullanÄ±m AlanÄ±:** Model KarÅŸÄ±laÅŸtÄ±rma ve SeÃ§imi.

2.  **Klasik Ä°statistik Yorumu (Pratik Anlam) ğŸ¯:**
    * **AnlamÄ±:** Modelin bir tahmininin (Ã¶rneÄŸin bir sÄ±nÄ±flandÄ±rma modelinin) belirli bir sÄ±nÄ±fa ait olma olasÄ±lÄ±ÄŸÄ± nedir?
    * **KullanÄ±m AlanÄ±:** Modelin Ã‡Ä±ktÄ±sÄ± (Tahmin GÃ¼veni).

Bu iki temel yoruma ve ilgili parametrelere detaylÄ± olarak bakmak gereklidir.

| Parametre / Konu | Ne? | Neden KullanÄ±lÄ±r? | NasÄ±l/NiÃ§in KullanÄ±lÄ±r? |
| :--- | :--- | :--- | :--- |
| **1. Modelin Posterior OlasÄ±lÄ±ÄŸÄ± ($P(M|D)$) ğŸ§ ** | **BayesÃ§i Ä°statistik**'te, modelin ($M$), eldeki gÃ¶zlemlenen veriler ($D$) gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda doÄŸru olma olasÄ±lÄ±ÄŸÄ±dÄ±r. | FarklÄ± modelleri (veya model parametrelerini) doÄŸrudan bir olasÄ±lÄ±kla karÅŸÄ±laÅŸtÄ±rmak ve hangi modelin verileri en iyi aÃ§Ä±kladÄ±ÄŸÄ±nÄ± belirlemek iÃ§in. | **Bayes Teoremi** kullanÄ±larak hesaplanÄ±r: $$P(M|D) = \frac{P(D|M) \cdot P(M)}{P(D)}$$ Burada $P(D|M)$ **OlasÄ±lÄ±k (Likelihood)**, $P(M)$ **Ã–nsel (Prior)** olasÄ±lÄ±ktÄ±r. |
| **2. Marjinal OlasÄ±lÄ±k (Model KanÄ±tÄ±)ğŸ“Š** | **$P(D|M)$ (Likelihood):** Modelin, belirli parametreler altÄ±nda gÃ¶zlemlenen verileri Ã¼retme olasÄ±lÄ±ÄŸÄ±dÄ±r. | Modelin, tahmin edilen parametre deÄŸerlerinin ne kadar iyi uyum saÄŸladÄ±ÄŸÄ±nÄ± gÃ¶sterir. | Modeli eÄŸitirken kullanÄ±lan veriye uyumu Ã¶lÃ§mek ve model parametrelerini optimize etmek iÃ§in (Maksimum Olabilirlik Tahmini - MLE). |
| **3. Bayes FaktÃ¶rÃ¼ (BF) âš–ï¸** | Ä°ki rakip modelin ($M_1$ ve $M_2$), veriler tarafÄ±ndan ne kadar desteklendiÄŸini gÃ¶steren oran: $BF = \frac{P(D|M_1)}{P(D|M_2)}$. | Ä°ki modelin birbirine karÅŸÄ± destek dÃ¼zeyini Ã¶lÃ§mek ve hangisinin daha olasÄ± olduÄŸunu belirlemek iÃ§in. | Genellikle $\ln(BF)$ logaritmasÄ± alÄ±narak yorumlanÄ±r; $BF > 1$ ise $M_1$ daha olasÄ±dÄ±r. |
| **4. Tahmin OlasÄ±lÄ±ÄŸÄ± (Model Ã‡Ä±ktÄ±sÄ±)ğŸ¯** | Bir sÄ±nÄ±flandÄ±rma modelinin (Lojistik Regresyon, SÄ±nÄ±flandÄ±rÄ±cÄ±lar vb.) yeni bir veri noktasÄ±nÄ± belirli bir sÄ±nÄ±fa atama olasÄ±lÄ±ÄŸÄ±. | Modelin tahminindeki **belirsizliÄŸi** Ã¶lÃ§mek ve yalnÄ±zca yÃ¼ksek gÃ¼vene sahip tahminleri kabul etmek iÃ§in. | $P(SÄ±nÄ±f|Veri)$. Bu Ã§Ä±ktÄ±, genellikle bir Softmax veya Sigmoid aktivasyon fonksiyonu kullanÄ±larak elde edilir. |
| **5. Model GÃ¼ven AralÄ±ÄŸÄ± (Confidence Interval) ğŸš§** | Bir model parametresinin (Ã¶rneÄŸin, regresyon katsayÄ±sÄ± $\beta$) veya tahmininin gerÃ§ek deÄŸerini belirli bir gÃ¼ven dÃ¼zeyinde (Ã¶rneÄŸin %95) iÃ§erme olasÄ±lÄ±ÄŸÄ±. | Model sonuÃ§larÄ±nÄ±n ne kadar gÃ¼venilir veya hassas olduÄŸunu nicel olarak belirtmek iÃ§in. | Tahmin edilen deÄŸer $\pm$ Hata Marjini ÅŸeklinde ifade edilir. |
| **6. Model DeÄŸerlendirme Metrikleri ğŸ“ˆ** | | | |
| a) $R^2$ (Belirleme KatsayÄ±sÄ±) | Modelin baÄŸÄ±mlÄ± deÄŸiÅŸkendeki varyasyonu ne Ã¶lÃ§Ã¼de aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir. | Regresyon modellerinin genel uyumunu ve aÃ§Ä±klama gÃ¼cÃ¼nÃ¼ Ã¶lÃ§mek iÃ§in. | $0$ ile $1$ arasÄ±nda deÄŸiÅŸir ($1$ en iyi uyumdur). |
| b) AUC-ROC EÄŸrisi AltÄ±ndaki Alan | Modelin rastgele seÃ§ilen pozitif bir Ã¶rneÄŸi, rastgele seÃ§ilen negatif bir Ã¶rnekten daha yÃ¼ksek bir skorla sÄ±ralama olasÄ±lÄ±ÄŸÄ±dÄ±r. | SÄ±nÄ±flandÄ±rma modelinin ayÄ±rma gÃ¼cÃ¼nÃ¼ tÃ¼m olasÄ± eÅŸikler boyunca Ã¶lÃ§mek iÃ§in. | $0.5$ (rastgele tahmin) ile $1.0$ (mÃ¼kemmel tahmin) arasÄ±nda deÄŸiÅŸir. |
| c) P-DeÄŸeri | GÃ¶zlemlenen etkinin (veya daha aÅŸÄ±rÄ± bir etkinin) null hipotezi doÄŸruyken elde edilme olasÄ±lÄ±ÄŸÄ±. | Bir deÄŸiÅŸkenin (Ã¶zellik) model Ã¼zerinde istatistiksel olarak anlamlÄ± bir etkiye sahip olup olmadÄ±ÄŸÄ±nÄ± test etmek iÃ§in. | Genellikle $p < 0.05$ olduÄŸunda null hipotez reddedilir. |

### ğŸ“‹ Ã–zetle Cevap YaklaÅŸÄ±mÄ±

Bu soru bir Data Scientist mÃ¼lakatÄ±nda sorulduÄŸunda, en doÄŸru ve kapsamlÄ± cevap, iki ana perspektife deÄŸinmektir:

1.  **Teorik/BayesÃ§i Perspektif ğŸ§ :** Soru, genellikle **BayesÃ§i Model KarÅŸÄ±laÅŸtÄ±rmasÄ±** baÄŸlamÄ±nda **Modelin Posterior OlasÄ±lÄ±ÄŸÄ±nÄ± ($P(M|D)$)** ifade eder. Bu, modelin kendisinin doÄŸru olma olasÄ±lÄ±ÄŸÄ±dÄ±r.

2.  **Pratik/Uygulama Perspektifi âš™ï¸:** Soru, gÃ¼nlÃ¼k iÅŸ akÄ±ÅŸÄ±nda kullanÄ±lan bir sÄ±nÄ±flandÄ±rma modelinin Ã¼rettiÄŸi **Tahmin OlasÄ±lÄ±ÄŸÄ±nÄ±** ifade ediyor olabilir.

Bu ayrÄ±mlarÄ± yaparak ve her iki alandaki kritik parametreleri (Posterior OlasÄ±lÄ±k, Bayes FaktÃ¶rÃ¼ ve AUC, $R^2$ gibi pratik metrikler) aÃ§Ä±klayarak konuya hakimiyetinizi gÃ¶sterebilirsiniz.

---

<img width="733" height="205" alt="image" src="https://github.com/user-attachments/assets/d608d7e7-ef6f-46df-9385-29459b775943" />

* Sorunun amacÄ±, popÃ¼lasyonun temel parametrelerini (varyans, ortalama, oran) tahmin etmek iÃ§in kullanÄ±lan genel istatistiksel yÃ¶ntemi sormaktadÄ±r:

### ğŸ¯ Point Estimation (Nokta Tahmini)

* **Point Estimation (Nokta Tahmini):** Bir popÃ¼lasyon parametresini (Ã¶rneÄŸin popÃ¼lasyon ortalamasÄ± $\mu$, varyansÄ± $\sigma^2$ veya oranÄ± $p$) tek bir deÄŸerle tahmin etme yÃ¶ntemidir.
    * **Ã–rneklem OrtalamasÄ± ($\bar{x}$)** popÃ¼lasyon ortalamasÄ± ($\mu$) iÃ§in bir nokta tahminidir.
    * **Ã–rneklem VaryansÄ± ($s^2$)** popÃ¼lasyon varyansÄ± ($\sigma^2$) iÃ§in bir nokta tahminidir.
    * **Ã–rneklem OranÄ± ($\hat{p}$)** popÃ¼lasyon oranÄ± ($p$) iÃ§in bir nokta tahminidir.

* Bu nedenle, Nokta Tahmini hem ortalamayÄ±, hem varyansÄ± hem de oranÄ± tahmin etmek iÃ§in kullanÄ±lan genel bir **yÃ¶ntemdir**.

---

<img width="755" height="260" alt="image" src="https://github.com/user-attachments/assets/373b3968-9db1-4e53-bdd3-ff4b80f5b6b6" />

* 
Bu soru, Maksimum Olabilirlik Tahmini (Maximum Likelihood Estimation - MLE) yÃ¶ntemini kullanarak bir Bernoulli denemesindeki (madeni para atÄ±ÅŸÄ±) baÅŸarÄ± olasÄ±lÄ±ÄŸÄ±nÄ± (p) bulmaya iliÅŸkindir.

Sorunun Ã§Ã¶zÃ¼mÃ¼ iÃ§in izlenmesi gereken adÄ±mlar ve kullanÄ±lacak fonksiyon ÅŸunlardÄ±r:

### ğŸª™ Maksimum Olabilirlik Tahmini (MLE) ile Madeni Para AtÄ±ÅŸÄ± Sorununun Ã‡Ã¶zÃ¼mÃ¼

#### 1. Problemi TanÄ±mlama

| Parametre | DeÄŸer | AÃ§Ä±klama |
| :--- | :--- | :--- |
| Toplam Deneme SayÄ±sÄ± ($n$) | 10 | (10 kez yazÄ± tura atÄ±ldÄ±). |
| BaÅŸarÄ± SayÄ±sÄ± ($k$) | 6 | (6 kez tura geldi - "heads"). |
| BaÅŸarÄ±sÄ±zlÄ±k SayÄ±sÄ± ($n-k$) | 4 | (4 kez yazÄ± geldi - "tails"). |
| Tahmin Edilecek Parametre | $p$ | Tura gelme olasÄ±lÄ±ÄŸÄ±. |

#### 2. Olabilirlik Fonksiyonu (Likelihood Function) Kurma ğŸ“

Bir madeni para atÄ±ÅŸÄ± dizisindeki sonuÃ§larÄ±n olasÄ±lÄ±ÄŸÄ±, Binom DaÄŸÄ±lÄ±mÄ± kullanÄ±larak hesaplanÄ±r. Ancak, MLE'de biz sadece belirli bir dizinin (Ã¶rneÄŸimizde 6 tura ve 4 yazÄ±) gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ± maksimize etmeye odaklanÄ±rÄ±z.

$p$ tura gelme olasÄ±lÄ±ÄŸÄ± ve $(1-p)$ yazÄ± gelme olasÄ±lÄ±ÄŸÄ± olmak Ã¼zere, herhangi bir 6 tura ve 4 yazÄ± dizisinin gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ± (olabilirlik fonksiyonu $L(p)$) ÅŸu ÅŸekilde ifade edilir:

$$L(p) = P(\text{veriler}|p) \propto p^k \cdot (1-p)^{n-k}$$

*Burada $\propto$, Binom olasÄ±lÄ±k fonksiyonundaki $\binom{n}{k}$ katsayÄ±sÄ±nÄ± (bu katsayÄ± $p$ parametresine baÄŸlÄ± olmadÄ±ÄŸÄ± iÃ§in MLE sÃ¼recinde genellikle gÃ¶z ardÄ± edilir) iÃ§erdiÄŸini belirtir.*

#### 3. DeÄŸerleri Yerine Koyma

BulduÄŸumuz deÄŸerleri fonksiyonda yerine koyarÄ±z:

* $k=6$
* $n-k=4$

$$L(p) = p^6 \cdot (1-p)^4$$

#### 4. Maksimum Olabilirlik Tahminini Bulma (Ek Bilgi) ğŸ’¡

Soruda sadece maksimize edilmesi gereken fonksiyon sorulsa da, tam MLE deÄŸeri de bu fonksiyondan tÃ¼retilir:

* Bu fonksiyonu maksimize eden $p$ deÄŸeri, $\frac{d(\ln L(p))}{dp} = 0$ denklemi Ã§Ã¶zÃ¼lerek bulunur.
* Bu tÃ¼r Binom durumlarÄ±nda, Maksimum Olabilirlik Tahmini her zaman basitÃ§e gÃ¶zlemlenen oran ($\hat{p}$) olur:

$$\hat{p} = \frac{\text{BaÅŸarÄ± SayÄ±sÄ±}}{\text{Toplam Deneme SayÄ±sÄ±}} = \frac{6}{10} = 0.6$$

---
<img width="777" height="235" alt="image" src="https://github.com/user-attachments/assets/f37673c8-ed85-4d46-8edf-ad374e5528d8" />

* Basit Lineer Regresyon modelinin (En KÃ¼Ã§Ã¼k Kareler YÃ¶ntemi - Ordinary Least Squares, OLS) temel Ã§alÄ±ÅŸma prensibini tam olarak aÃ§Ä±klamaktadÄ±r:
* (Lineer regresyon, noktalar ve uydurulan Ã§izgi arasÄ±ndaki karesel mesafelerin toplamÄ±nÄ± minimize ederek veriye en iyi uyumu saÄŸlar.)
  
---

<img width="724" height="225" alt="image" src="https://github.com/user-attachments/assets/10ef8e7c-81bd-401d-891c-d34b45ba484e" />

### ğŸ›¡ï¸ DÃ¼zenlileÅŸtirme (Regularization) AmacÄ±

**DoÄŸru Ä°fade:**
> Regularization prevents overfitting by penalizing models with large coefficients or weights.
> (DÃ¼zenlileÅŸtirme, bÃ¼yÃ¼k katsayÄ±lara veya aÄŸÄ±rlÄ±klara sahip modelleri cezalandÄ±rarak aÅŸÄ±rÄ± uydurmayÄ± (overfitting) Ã¶nler.)

#### Neden Bu TanÄ±m DoÄŸrudur?

DÃ¼zenlileÅŸtirmenin temel amacÄ± ve mekanizmasÄ± ÅŸunlardÄ±r:

* **Birincil AmaÃ§ ğŸ¯:** DÃ¼zenlileÅŸtirmenin (**L1 - Lasso** veya **L2 - Ridge** gibi) asÄ±l amacÄ±, modelin **karmaÅŸÄ±klÄ±ÄŸÄ±nÄ±** kontrol altÄ±na alarak eÄŸitim verisine aÅŸÄ±rÄ± derecede uyum saÄŸlamasÄ±nÄ± (**overfitting**) engellemektir.
* **Mekanizma âš–ï¸:** Bu, kayÄ±p fonksiyonuna (**loss function**) katsayÄ±larÄ±n (aÄŸÄ±rlÄ±klarÄ±n) bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne baÄŸlÄ± bir **ceza terimi (penalty term)** eklenerek yapÄ±lÄ±r.
* **Etkisi ğŸ“‰:** BÃ¼yÃ¼k katsayÄ±lar, modelin verideki kÃ¼Ã§Ã¼k dalgalanmalara karÅŸÄ± Ã§ok hassas olduÄŸu anlamÄ±na gelir. Bu katsayÄ±larÄ± cezalandÄ±rmak, modeli daha **genelleÅŸtirilebilir** (basit) hale getirir.

---

<img width="768" height="229" alt="image" src="https://github.com/user-attachments/assets/7a35c4b7-8958-4c9d-8c49-a51c50b44ad8" />

### ğŸ“ L2 DÃ¼zenlileÅŸtirme Hata DeÄŸeri HesaplamasÄ± (Ridge Penalty)

Soruda istenen, verilen $M = 4x^4 + 3x^2 + 1$ modeli iÃ§in **L2 DÃ¼zenlileÅŸtirme Hata DeÄŸeri**'ni (L2 regularization error value) hesaplamaktÄ±r.

L2 dÃ¼zenlileÅŸtirme (Ridge Regresyonu), modelin katsayÄ±larÄ±nÄ±n (aÄŸÄ±rlÄ±klarÄ±nÄ±n) karesinin toplamÄ±na eÅŸittir. **Bias terimi** (kesiÅŸim/sabit terim) genellikle dÃ¼zenlileÅŸtirmeye dahil edilmez.

#### 1. Modelin KatsayÄ±larÄ±nÄ± (AÄŸÄ±rlÄ±klarÄ±nÄ±) Belirleme:

Verilen model:
$$M = 4x^4 + 3x^2 + 1$$

Bu modelin katsayÄ±larÄ± (aÄŸÄ±rlÄ±klarÄ±) ÅŸunlardÄ±r:
* $x^4$ teriminin katsayÄ±sÄ± ($w_4$): $4$
* $x^2$ teriminin katsayÄ±sÄ± ($w_2$): $3$
* Sabit terim/Bias ($b$): $1$

#### 2. L2 DÃ¼zenlileÅŸtirme Hata DeÄŸeri FormÃ¼lÃ¼:

L2 cezasÄ± (penalty) katsayÄ±larÄ±n karesinin toplamÄ±dÄ±r:
$$\text{L2 CezasÄ±} = \sum_{i} w_i^2$$

#### 3. Hesaplama ğŸ”¢:

Bias terimini ($1$) hariÃ§ tutarak katsayÄ±larÄ±n karesini toplarÄ±z:

$$\text{L2 CezasÄ±} = (4)^2 + (3)^2$$
$$\text{L2 CezasÄ±} = 16 + 9$$
$$\text{L2 CezasÄ±} = 25$$

---

**SonuÃ§:** L2 dÃ¼zenlileÅŸtirme hata deÄŸeri **25**'tir.

**Not ğŸ“Œ:** Bu deÄŸer, toplam kayÄ±p fonksiyonuna ($\text{Loss}$) $\lambda$ (lambda) hiperparametresi ile Ã§arpÄ±larak eklenir: $\text{Toplam Loss} = \text{MSE} + \lambda \cdot (\text{L2 CezasÄ±})$. Ancak soruda sadece ceza deÄŸerinin kendisi (25) istenmiÅŸtir.

---

### ğŸ” Maksimum Olabilirlik ile DaÄŸÄ±lÄ±m KarÅŸÄ±laÅŸtÄ±rmasÄ±

<img width="821" height="616" alt="image" src="https://github.com/user-attachments/assets/e2d0af40-ec6e-479d-8d23-401e98490320" />


Bu soru, $S = \{-1, 2\}$ Ã¶rneklemini Ã¼retme olasÄ±lÄ±ÄŸÄ± en yÃ¼ksek olan daÄŸÄ±lÄ±mÄ± bulmak iÃ§in **Maksimum Olabilirlik (Maximum Likelihood - ML)** ilkesini kullanmayÄ± gerektirir.

Ã–rneklemdeki gÃ¶zlemler baÄŸÄ±msÄ±z kabul edildiÄŸinden, bir daÄŸÄ±lÄ±mÄ±n ($M$) Olabilirlik Fonksiyonu (L(M)), ÅŸu ÅŸekilde hesaplanÄ±r:

$$L(M) = f(x_1 | M) \cdot f(x_2 | M)$$

$f(x|M)$ deÄŸeri, grafikteki **YoÄŸunluk (Density)** deÄŸerleridir.

---

#### 1. DaÄŸÄ±lÄ±m $N(0, 2^2)$ Ä°Ã§in Olabilirlik HesaplamasÄ± ğŸŸ¢

Bu daÄŸÄ±lÄ±m iÃ§in $\mu=0$ ve $\sigma=2$'dir.

| Nokta ($x$) | YoÄŸunluk $f(x)$ DeÄŸeri |
| :---: | :---: |
| $x_1 = -1$ | $0.18$ |
| $x_2 = 2$ | $0.12$ |

$$\text{L}(N(0, 2^2)) = 0.18 \cdot 0.12 = 0.0216$$

#### 2. DaÄŸÄ±lÄ±m $N(1, 1^2)$ Ä°Ã§in Olabilirlik HesaplamasÄ± ğŸ”´

Bu daÄŸÄ±lÄ±m iÃ§in $\mu=1$ ve $\sigma=1$'dir.

| Nokta ($x$) | YoÄŸunluk $f(x)$ DeÄŸeri |
| :---: | :---: |
| $x_1 = -1$ | $0.05$ |
| $x_2 = 2$ | $0.24$ |

$$\text{L}(N(1, 1^2)) = 0.05 \cdot 0.24 = 0.0120$$

---

#### 3. KarÅŸÄ±laÅŸtÄ±rma Sonucu

| DaÄŸÄ±lÄ±m | Olabilirlik DeÄŸeri ($L$) |
| :--- | :--- |
| $N(0, 2^2)$ | **$0.0216$** |
| $N(1, 1^2)$ | $0.0120$ |

$0.0216 > 0.0120$ olduÄŸundan, **$N(0, 2^2)$ daÄŸÄ±lÄ±mÄ±nÄ±n** verilen Ã¶rneklemi Ã¼retme olasÄ±lÄ±ÄŸÄ± (olabilirliÄŸi) daha yÃ¼ksektir.

**DoÄŸru Cevap: $N(0, 2^2)$**

---

<img width="734" height="239" alt="image" src="https://github.com/user-attachments/assets/72cc4f4d-fa93-427d-9b9f-ae1406a500f4" />

### ğŸ’¡ Ã–nsel (Prior) Ä°nanÃ§larÄ±n Bayes Ä°statistiÄŸindeki RolÃ¼

Kavramsal olarak "Ã¶nsel" (prior) terimi, herhangi bir veri gÃ¶zlemlemeden Ã¶nce bir parametrenin daÄŸÄ±lÄ±mÄ± hakkÄ±ndaki **baÅŸlangÄ±Ã§ inanÃ§larÄ±nÄ±** yansÄ±tÄ±r.

Ancak, Bayes istatistiÄŸinde, bu Ã¶nsel inanÃ§larÄ±n gÃ¼cÃ¼, gÃ¶zlemlenen veriler tarafÄ±ndan yÃ¶nlendirilen sistematik bir **gÃ¼ncelleme sÃ¼reci** aracÄ±lÄ±ÄŸÄ±yla deÄŸiÅŸtirilir:

* **GÃ¼ncelleme MekanizmasÄ± ğŸ”„:** Bu gÃ¼ncelleme, Bayes teoremi aracÄ±lÄ±ÄŸÄ±yla yapÄ±lÄ±r. Bu sÃ¼reÃ§, Ã¶nsel inanÃ§larÄ±n, verilerden elde edilen bilgilerle rafine edilmesini ve hizalanmasÄ±nÄ± saÄŸlar.
* **Verinin Merkezi RolÃ¼ ğŸ“Š:** Bayes analizinde **veri asla gÃ¶z ardÄ± edilmez veya dÄ±ÅŸlanmaz**. Veri, temel gerÃ§ekliÄŸi daha iyi yansÄ±tmak Ã¼zere Ã¶nsel daÄŸÄ±lÄ±mÄ± ayarlamada ve ÅŸekillendirmede merkezi bir rol oynar.

---

<img width="834" height="600" alt="image" src="https://github.com/user-attachments/assets/975ba114-e275-4f23-82c7-435517229ad4" />

### âš–ï¸ MAP (Maximum A Posteriori) Tahmini ve Ã–nsel KarÅŸÄ±laÅŸtÄ±rmasÄ±

Bu Ã§Ã¶zÃ¼m, farklÄ± **Ã–nsel (Prior)** inanÃ§lara sahip iki BayesÃ§i'nin, aynÄ± veriyi gÃ¶zlemledikten sonra ulaÅŸtÄ±klarÄ± **Maksimum ArdsÄ±l OlasÄ±lÄ±k (MAP)** tahminlerini karÅŸÄ±laÅŸtÄ±rmaktadÄ±r.

#### 1. Veri ve Modeli TanÄ±mlama ğŸ“Š

| Parametre | DeÄŸer |
| :--- | :--- |
| Deneme SayÄ±sÄ± ($n$) | 10 |
| Tura SayÄ±sÄ± (BaÅŸarÄ±, $k$) | 3 |
| YazÄ± SayÄ±sÄ± (BaÅŸarÄ±sÄ±zlÄ±k, $n-k$) | 7 |
| Tahmin Edilecek Parametre | Tura gelme olasÄ±lÄ±ÄŸÄ± ($\theta = P(H)$) |

**Olabilirlik (Likelihood) Fonksiyonu:** $\propto \theta^3 (1-\theta)^7$

---

#### 2. BayesÃ§i 2'nin MAP Tahmini (Uniform/ZayÄ±f Ã–nsel)

BayesÃ§i 2, **uniform Ã¶nsel** ($\text{Prior} \propto 1$) kullanÄ±r. Bu, tÃ¼m olasÄ±lÄ±klara eÅŸit aÄŸÄ±rlÄ±k verdiÄŸi iÃ§in bilgi iÃ§ermez.

$$\text{Posterior} \propto \theta^3 (1-\theta)^7 \cdot 1$$

* Bu durumda MAP tahmini, **Maksimum Olabilirlik Tahmini (MLE)** ile aynÄ±dÄ±r:
    $$\text{MAP}_{\text{BayesÃ§i 2}} = \frac{\text{BaÅŸarÄ± SayÄ±sÄ±}}{\text{Toplam Deneme SayÄ±sÄ±}} = \frac{3}{10} = \mathbf{0.30}$$

---

#### 3. BayesÃ§i 1'in MAP Tahmini (GÃ¼Ã§lÃ¼ Ã–nsel)

BayesÃ§i 1, paranÄ±n adil ($\mathbf{P(H)=0.5}$) olduÄŸuna dair **gÃ¼Ã§lÃ¼ bir Ã¶nsel** inanÃ§ kullanÄ±r. Bu gÃ¼Ã§lÃ¼ Ã¶nsel, zayÄ±f veriye raÄŸmen sonucu kendi ortalamasÄ±na yaklaÅŸtÄ±rÄ±r.

$$\text{MAP}_{\text{BayesÃ§i 1}} \text{ deÄŸeri } \in [0.30, 0.50]$$

* **Prensip ğŸ¯:** GÃ¼Ã§lÃ¼ bir Ã¶nsel, zayÄ±f bir veriyi tamamen ezemez, ancak tahminin veri oranÄ± ($0.30$) ile Ã¶nselin ortalamasÄ± ($0.50$) arasÄ±nda kalmasÄ±nÄ± saÄŸlar. Ã–nsel gÃ¼Ã§lÃ¼ olduÄŸu iÃ§in, sonuÃ§ **$0.50$'ye daha yakÄ±n** olacaktÄ±r.
* **Makul DeÄŸerler:** $0.30$ ve $0.50$ arasÄ±ndaki $0.49$ veya $0.51$ seÃ§enekleri, gÃ¼Ã§lÃ¼ Ã¶nselin etkisini yansÄ±tÄ±r.

**Beklenen SonuÃ§:**
* BayesÃ§i 1: GÃ¼Ã§lÃ¼ Ã¶nsel nedeniyle $0.50$'ye yakÄ±n (Ã¶rn., $\mathbf{0.49}$)
* BayesÃ§i 2: Veri oranÄ± olduÄŸu iÃ§in $\mathbf{0.30}$

---

### 4. SeÃ§eneklerin DeÄŸerlendirilmesi (BayesÃ§i MAP KarÅŸÄ±laÅŸtÄ±rmasÄ±) ğŸ§ 

| SeÃ§enek | BayesÃ§i 1 (GÃ¼Ã§lÃ¼ Ã–nsel, 0.5 civarÄ±) | BayesÃ§i 2 (ZayÄ±f Ã–nsel, 0.3 civarÄ±) | Karar |
| :---: | :---: | :---: | :--- |
| **A** | $P(H) = 0.49$ | $P(H) = 0.30$ | **MantÄ±klÄ±** (BayesÃ§i 1, 0.30 ile 0.50 arasÄ±nÄ±, 0.50'ye yakÄ±n seÃ§miÅŸtir.) |
| **B** | $P(H) = 0.51$ | $P(H) = 0.30$ | MantÄ±klÄ± (BayesÃ§i 1, 0.50'nin biraz Ã¼zerine kaymÄ±ÅŸtÄ±r.) |
| **C** | $P(H) = 0.30$ | $P(H) = 0.30$ | YanlÄ±ÅŸ (BayesÃ§i 1'in gÃ¼Ã§lÃ¼ Ã¶nseli yok sayÄ±lmÄ±ÅŸtÄ±r.) |
| **D** | $P(H) = 0.30$ | $P(H) = 0.49$ | YanlÄ±ÅŸ (BayesÃ§i 1'in ve BayesÃ§i 2'nin sonuÃ§larÄ± karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.) |

**Not:** Bu karÅŸÄ±laÅŸtÄ±rmada, BayesÃ§i 2'nin uniform Ã¶nseli nedeniyle MAP deÄŸeri doÄŸrudan gÃ¶zlemlenen orana (0.30) eÅŸit Ã§Ä±karken; BayesÃ§i 1'in gÃ¼Ã§lÃ¼ Ã¶nseli, sonucu 0.50'ye yakÄ±n tutmaktadÄ±r.

### ğŸŒŸ BayesÃ§i SonuÃ§larÄ±n YorumlanmasÄ± (MAP KararÄ±)

BayesÃ§i modellemede, gÃ¼Ã§lÃ¼ bir Ã¶nselin zayÄ±f bir veri kÃ¼mesiyle karÅŸÄ±laÅŸtÄ±ÄŸÄ± durumlar Ã¶nemlidir:

* Hem **$0.49$** hem de **$0.51$** mantÄ±klÄ±dÄ±r, ancak BayesÃ§i modelleme genellikle sonuÃ§larÄ± Ã¶nselin ortalamasÄ±na yakÄ±n tutar.
* Bu tÃ¼r Ã§oktan seÃ§meli sorularda, genellikle gÃ¼Ã§lÃ¼ Ã¶nselin $0.50$'den sadece biraz uzaklaÅŸtÄ±ÄŸÄ± kabul edilir.
* GÃ¶zlemlenen Veri ($0.30$) ve Ã–nsel ($0.50$) arasÄ±nda, $0.50$'ye en yakÄ±n olan deÄŸerler $0.49$ veya $0.51$'dir.

Deneyimli BayesÃ§ilerin Ã§oÄŸu, gÃ¼Ã§lÃ¼ bir Ã¶nselin $10$ atÄ±ÅŸlÄ±k zayÄ±f bir veriyi tamamen ezemeyeceÄŸini bilir, bu yÃ¼zden $0.50$'nin hemen yanÄ±ndaki $0.49$ veya $0.51$ en olasÄ± deÄŸerlerdir.

**Bu baÄŸlamda, ilk seÃ§enek (A) en tipik BayesÃ§i sonucu yansÄ±tÄ±r:**

| BayesÃ§i | Ã–nsel Tipi | SonuÃ§ Yorumu | MAP DeÄŸeri |
| :---: | :---: | :--- | :---: |
| **BayesÃ§i 1** | GÃ¼Ã§lÃ¼ Ã–nsel | $\text{GÃ¼Ã§lÃ¼ Ã–nsel} + \text{ZayÄ±f Veri} \implies$ Ã–nsele yakÄ±n | $\mathbf{0.49}$ |
| **BayesÃ§i 2** | Uniform Ã–nsel | $\text{Uniform Ã–nsel} + \text{ZayÄ±f Veri} \implies$ Veriye eÅŸit | $\mathbf{0.30}$ |

**DoÄŸru cevap ilk seÃ§enektir:** **Bayesian 1: $P(H)=0.49$, Bayesian 2: $P(H)=0.30$.**

---
---
---

### ğŸ“Š Nicel Ä°statistik: GÃ¼ven AralÄ±ÄŸÄ± (Confidence Interval) KavramlarÄ±

* GÃ¶rseldeki maddeler, nicel istatistiÄŸin temel konularÄ±ndan biri olan GÃ¼ven AralÄ±ÄŸÄ± (Confidence Interval) kavramÄ±nÄ± ve bu aralÄ±ÄŸÄ± etkileyen faktÃ¶rleri aÃ§Ä±klamaktadÄ±r.
  
GÃ¼ven aralÄ±ÄŸÄ±, bir popÃ¼lasyon parametresini (Ã¶rneÄŸin ortalama $\mu$) tahmin etmek iÃ§in kullanÄ±lan, Ã¶rneklem verisine dayalÄ± istatistiksel bir aralÄ±ktÄ±r.

| Kavram | AÃ§Ä±klama | Anahtar Ã‡Ä±karÄ±m | Emoji |
| :--- | :--- | :--- | :---: |
| **Confidence Intervals** (GÃ¼ven AralÄ±klarÄ±) | GÃ¼ven aralÄ±klarÄ±, Ã¶rneklem ortalamasÄ±na ($\bar{x}$) her iki taraftan da bir hata payÄ±nÄ±n eklenmesiyle elde edilir. $$\text{GA} = \bar{x} \pm \text{Hata PayÄ±}$$ | PopÃ¼lasyon parametresi iÃ§in tahmini bir aralÄ±k sunar. | ğŸ“ |
| **Confidence Level** (GÃ¼ven Seviyesi) | Bir gÃ¼ven aralÄ±ÄŸÄ±nÄ±n, tahmin edilen gerÃ§ek popÃ¼lasyon ortalamasÄ±nÄ± ($\mu$) iÃ§erme olasÄ±lÄ±ÄŸÄ±dÄ±r (Ã¶rneÄŸin %95). | GÃ¼ven aralÄ±ÄŸÄ±nÄ±n gerÃ§ek parametreyi yakalama olasÄ±lÄ±ÄŸÄ±nÄ± gÃ¶sterir. | ğŸ¯ |
| **Ideally** (Ä°deal Durum) | Ä°deal olarak, hem yÃ¼ksek gÃ¼ven seviyesine hem de dar bir aralÄ±ÄŸa sahip olmak istenir. (YÃ¼ksek kesinlik + YÃ¼ksek doÄŸruluk) | YÃ¼ksek gÃ¼ven ve dar aralÄ±k bir Ã§eliÅŸkidir; bu hedefe ancak daha fazla veriyle ulaÅŸÄ±labilir. | âœ¨ |
| **Larger Samples** (Daha BÃ¼yÃ¼k Ã–rneklemler) | Daha bÃ¼yÃ¼k Ã¶rneklemler (daha fazla veri), hata payÄ±nÄ± dÃ¼ÅŸÃ¼rerek daha dar bir aralÄ±k saÄŸlayacaktÄ±r. | Dar aralÄ±k ve yÃ¼ksek gÃ¼ven elde etmenin en gÃ¼venilir yoludur. | ğŸ“ˆ |
| **Decreasing Confidence Level** (GÃ¼ven Seviyesini DÃ¼ÅŸÃ¼rmek) | GÃ¼ven seviyesini dÃ¼ÅŸÃ¼rmek (%99'dan %95'e gibi) de aralÄ±ÄŸÄ± daraltacaktÄ±r. | AralÄ±ÄŸÄ± daraltÄ±r (kesinliÄŸi artÄ±rÄ±r) ancak gerÃ§ek ortalamayÄ± yakalama olasÄ±lÄ±ÄŸÄ±nÄ± (gÃ¼veni) azaltÄ±r. | ğŸ“‰ |

<img width="1213" height="590" alt="image" src="https://github.com/user-attachments/assets/11d81451-7fa5-4ee5-966c-caa7cb609d9f" />

# ğŸ“ˆ Hata PayÄ± (Margin of Error) Tahmini ve Ä°statistiksel Temeller

Bu bÃ¶lÃ¼mdeki gÃ¶rsel, **Hata PayÄ± (Margin of Error)** kavramÄ±nÄ± ve bir popÃ¼lasyonun **Normal DaÄŸÄ±lÄ±m** Ã¶zelliÄŸini ($X \sim N(\mu, \sigma^2)$) gÃ¶stermektedir. Hata PayÄ±'nÄ± tahmin etmek, yani bir **GÃ¼ven AralÄ±ÄŸÄ±** oluÅŸturmak iÃ§in kullanÄ±lan formÃ¼l, elinizdeki istatistiksel bilgiye ve Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne baÄŸlÄ± olarak deÄŸiÅŸir.

---

## Temel FormÃ¼l ve BileÅŸenler

Hata PayÄ±'nÄ±n temel formÃ¼lÃ¼ ÅŸÃ¶yledir:

$$\text{Hata PayÄ±} = \text{Kritik DeÄŸer} \times \text{Standart Hata}$$

| BileÅŸen | AÃ§Ä±klama |
| :--- | :--- |
| **Kritik DeÄŸer** ($\boldsymbol{Z}$ veya $\boldsymbol{t}$) | SeÃ§tiÄŸiniz GÃ¼ven Seviyesine (Ã¶rneÄŸin, %95) gÃ¶re Normal DaÄŸÄ±lÄ±m ($Z$) veya t-DaÄŸÄ±lÄ±m ($t$) tablosundan belirlenen deÄŸerdir. |
| **Standart Hata** | Ã–rneklem ortalamasÄ±nÄ±n, popÃ¼lasyon ortalamasÄ±ndan ne kadar uzakta olabileceÄŸinin bir Ã¶lÃ§Ã¼sÃ¼dÃ¼r. |

---

## ğŸ”¬ Tahmin DurumlarÄ±

GÃ¶rselde popÃ¼lasyonun Normal daÄŸÄ±lÄ±ma sahip olduÄŸu belirtildiÄŸi iÃ§in (bu, nicel istatistikte sÄ±klÄ±kla varsayÄ±lÄ±r), tahmin formÃ¼lÃ¼ ÅŸu iki ana duruma gÃ¶re belirlenir:

### Durum 1: PopÃ¼lasyon Standart SapmasÄ± ($\sigma$) **BÄ°LÄ°NÄ°YORSA**

Bu durumda (pratikte nadir) Hata PayÄ±'nÄ± hesaplamak iÃ§in **Z-Skoru (Normal DaÄŸÄ±lÄ±m)** kullanÄ±lÄ±r.

$$\text{Hata PayÄ±} = Z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}$$

| Parametre | AÃ§Ä±klama | Ã–rnek DeÄŸerler |
| :--- | :--- | :--- |
| $\boldsymbol{Z_{\alpha/2}}$ (Kritik DeÄŸer) | SeÃ§ilen gÃ¼ven seviyesine karÅŸÄ±lÄ±k gelen $z$-skoru. | %95 GÃ¼ven Seviyesi iÃ§in $Z_{\alpha/2} \approx 1.96$ |
| $\boldsymbol{\sigma}$ | PopÃ¼lasyon Standart SapmasÄ± (GÃ¶rselde $\sigma^2$ (varyans) ile gÃ¶sterilen deÄŸerin karekÃ¶kÃ¼). | |
| $\boldsymbol{n}$ | Ã–rneklem BÃ¼yÃ¼klÃ¼ÄŸÃ¼ (Toplanan veri sayÄ±sÄ±). | |
| $\boldsymbol{\frac{\sigma}{\sqrt{n}}}$ | Standart Hata (Ã–rneklem ortalamasÄ±nÄ±n standart sapmasÄ±). | |

---

### Durum 2: PopÃ¼lasyon Standart SapmasÄ± ($\sigma$) **BÄ°LÄ°NMÄ°YORSA**

Pratikte en yaygÄ±n durum budur. Bu durumda, popÃ¼lasyon standart sapmasÄ± yerine **Ã¶rneklem standart sapmasÄ± ($s$)** kullanÄ±lÄ±r ve genellikle **t-DaÄŸÄ±lÄ±mÄ±** kullanÄ±lÄ±r.

$$\text{Hata PayÄ±} = t_{n-1, \alpha/2} \times \frac{s}{\sqrt{n}}$$

| Parametre | AÃ§Ä±klama |
| :--- | :--- |
| $\boldsymbol{t_{n-1, \alpha/2}}$ (Kritik DeÄŸer) | SeÃ§ilen gÃ¼ven seviyesine ve serbestlik derecesine ($n-1$) karÅŸÄ±lÄ±k gelen $t$-skoru. |
| $\boldsymbol{s}$ | Ã–rneklem Standart SapmasÄ± (TopladÄ±ÄŸÄ±nÄ±z Ã¶rneklem verisinin standart sapmasÄ±). |
| $\boldsymbol{n}$ | Ã–rneklem BÃ¼yÃ¼klÃ¼ÄŸÃ¼ (Toplanan veri sayÄ±sÄ±). |

### âš ï¸ Ã–nemli Not (Merkezi Limit Teoremi)

EÄŸer Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) yeterince bÃ¼yÃ¼kse (genellikle $n \ge 30$ kabul edilir), **Merkezi Limit Teoremi** gereÄŸi popÃ¼lasyonun daÄŸÄ±lÄ±mÄ± ne olursa olsun, Ã¶rneklem ortalamasÄ±nÄ±n daÄŸÄ±lÄ±mÄ± normale yaklaÅŸÄ±r ve bu durumda $t$-DaÄŸÄ±lÄ±mÄ± yerine yine yaklaÅŸÄ±k olarak $Z$-DaÄŸÄ±lÄ±mÄ± kullanÄ±labilir.

---

## ğŸ¤– Makine Ã–ÄŸrenmesi BaÄŸlamÄ±nda Ã–nemi

Hata PayÄ± ve GÃ¼ven AralÄ±ÄŸÄ± kavramÄ±, makine Ã¶ÄŸrenmesi ve veri analizinde Ã¶zellikle kritik noktalarda devreye girer:

1.  **Regresyon Analizi (Tahminler):**
    * Tahmin edilen katsayÄ±lar iÃ§in gÃ¼ven aralÄ±klarÄ± hesaplanÄ±r. Bu, katsayÄ±nÄ±n gerÃ§ek deÄŸerinin bÃ¼yÃ¼k olasÄ±lÄ±kla hangi aralÄ±kta olduÄŸunu gÃ¶sterir.

2.  **A/B Testi (KarÅŸÄ±laÅŸtÄ±rma):**
    * Ä°ki grup arasÄ±ndaki farkÄ±n istatistiksel olarak anlamlÄ± olup olmadÄ±ÄŸÄ±nÄ± test ederken, fark iÃ§in bir gÃ¼ven aralÄ±ÄŸÄ± oluÅŸturulur. EÄŸer bu aralÄ±k sÄ±fÄ±rÄ± iÃ§ermiyorsa, farkÄ±n anlamlÄ± olduÄŸu sonucuna varÄ±lÄ±r.

3.  **Model DeÄŸerlendirme (GÃ¼venilirlik):**
    * Modelinizin performans Ã¶lÃ§Ã¼mlerine (Ã¶rneÄŸin DoÄŸruluk - Accuracy) GÃ¼ven AralÄ±ÄŸÄ± eklemek, tahminlerinizin ne kadar gÃ¼venilir olduÄŸunu daha ÅŸeffaf bir ÅŸekilde gÃ¶sterir.
  
<img width="1183" height="560" alt="image" src="https://github.com/user-attachments/assets/811d0dac-3743-4465-9eb3-65a637c0db8d" />

# âš–ï¸ Veri DaÄŸÄ±lÄ±mlarÄ± Ãœzerine: Merkezi Limit Teoremi (CLT)

Bu gÃ¶rsel, istatistikteki en Ã¶nemli kavram olan **Ã–rneklem OrtalamasÄ±nÄ±n DaÄŸÄ±lÄ±mÄ± (Sampling Distribution of the Mean)** ve **Merkezi Limit Teoremi'ni (CLT)** aÃ§Ä±klamaktadÄ±r.

## Temel Kural

Bir popÃ¼lasyondan alÄ±nan Ã¶rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±, her zaman popÃ¼lasyon ortalamasÄ± ($\mu$) etrafÄ±nda merkezlenir ve varyansÄ± popÃ¼lasyon varyansÄ±nÄ±n ($\sigma^2$) Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne ($n$) bÃ¶lÃ¼nmesiyle bulunur: $\frac{\sigma^2}{n}$.

| Senaryo | PopÃ¼lasyon (Ana Kitle) DaÄŸÄ±lÄ±mÄ± | Uygulama KuralÄ± | Ã–rneklem OrtalamasÄ±nÄ±n DaÄŸÄ±lÄ±mÄ± ($\bar{X}$) | AnlamÄ± |
| :---: | :--- | :--- | :--- | :--- |
| **Senaryo 1** ğŸ¯ | **Normal DaÄŸÄ±lÄ±m** ($\boldsymbol{X \sim N(\mu, \sigma^2)}$) | **DoÄŸrudan Kural:** PopÃ¼lasyon Normal olduÄŸu iÃ§in, Ã¶rneklem daÄŸÄ±lÄ±mÄ± her zaman normaldir. | **Normal DaÄŸÄ±lÄ±m:** $$\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)$$ | Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($n$) ne olursa olsun, istatistiksel Ã§Ä±karÄ±m iÃ§in Normal DaÄŸÄ±lÄ±m kurallarÄ± uygulanabilir. |
| **Senaryo 2** ğŸ’¡ | **Bilinmiyor veya Normal DeÄŸil** ($\boldsymbol{X}$ unknown or not Normal) | **Merkezi Limit Teoremi (CLT):** Yeterince bÃ¼yÃ¼k bir Ã¶rneklem alÄ±nmalÄ±dÄ±r ($n \ge 30$). | **YaklaÅŸÄ±k Normal DaÄŸÄ±lÄ±m:** $$\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)$$ | PopÃ¼lasyonun ÅŸekli Ã¶nemli deÄŸildir. BÃ¼yÃ¼k Ã¶rneklemler sayesinde, istatistiksel analizler iÃ§in Normal DaÄŸÄ±lÄ±mÄ±n gÃ¼Ã§lÃ¼ matematiksel araÃ§larÄ±nÄ± kullanabiliriz. |

## ğŸŒŸ Merkezi Limit Teoremi (CLT) Nedir?

CLT, **veri biliminin ve Ã§Ä±karÄ±msal istatistiÄŸin bel kemiÄŸidir.**

* **TanÄ±m:** Bir popÃ¼lasyonun daÄŸÄ±lÄ±mÄ± ne olursa olsun, o popÃ¼lasyondan alÄ±nan rastgele ve yeterince bÃ¼yÃ¼k Ã¶rneklemlerin ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±, yaklaÅŸÄ±k olarak Normal DaÄŸÄ±lÄ±ma uyar.
* **Pratikteki Ã–nemi:** Veri bilimcileri ve analistler, Ã§oÄŸu zaman popÃ¼lasyonun tamamÄ±nÄ± bilemezler veya Ã¶lÃ§emezler. CLT sayesinde, sadece **bÃ¼yÃ¼k bir Ã¶rneklem** toplayarak, popÃ¼lasyon hakkÄ±nda gÃ¼venilir istatistiksel tahminler yapabilirler.

<img width="1205" height="589" alt="image" src="https://github.com/user-attachments/assets/9564ba7e-eb04-4435-9720-a81e5a77e4b9" />

# ğŸ¯ GÃ¼ven AralÄ±ÄŸÄ± (Confidence Interval) - Hesaplama AdÄ±mlarÄ±

Bu gÃ¶rsel, popÃ¼lasyon standart sapmasÄ±nÄ±n ($\sigma$) bilindiÄŸi veya bÃ¼yÃ¼k Ã¶rneklemle (Z-DaÄŸÄ±lÄ±mÄ±) Ã§alÄ±ÅŸÄ±ldÄ±ÄŸÄ± durumda GÃ¼ven AralÄ±ÄŸÄ± hesaplamasÄ±nÄ± Ã¶zetler.

## ğŸ“ Hesaplama AdÄ±mlarÄ± (STEPS)

GÃ¼ven AralÄ±ÄŸÄ±: $\text{GA} = \bar{x} \pm Z_{1-\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$

| AdÄ±m # | AÃ§Ä±klama | AmacÄ± ve RolÃ¼ | Emoji |
| :---: | :--- | :--- | :---: |
| **1** | **Find the sample mean ($\bar{x}$)** | Ã–rneklem ortalamasÄ±nÄ± bulun. Bu, gÃ¼ven aralÄ±ÄŸÄ±nÄ±n merkezini oluÅŸturur. | ğŸ“‹ |
| **2** | **Define a desired confidence level ($\mathbf{1-\alpha}$)** | Ä°stenen gÃ¼ven seviyesini (%95 gibi) tanÄ±mlayÄ±n. Kritik deÄŸeri belirlemeye yarar. | âœ… |
| **3** | **Get the critical value ($\mathbf{Z_{1-\alpha/2}}$)** | SeÃ§ilen gÃ¼ven seviyesine karÅŸÄ±lÄ±k gelen Z-skorunu bulun (Ã–rn: %95 iÃ§in $\pm 1.96$). | ğŸ“ |
| **4** | **Find the standard error ($\mathbf{\frac{\sigma}{\sqrt{n}}}$)** | Standart hatayÄ± hesaplayÄ±n. Bu, Ã¶rneklem ortalamalarÄ±nÄ±n yayÄ±lÄ±mÄ±nÄ± Ã¶lÃ§er. | ğŸŒ |
| **5** | **Find the margin of error (Hata PayÄ±nÄ± Bulun)** | Kritik deÄŸer ile Standart HatayÄ± Ã§arpÄ±n: $$\text{Hata PayÄ±} = Z_{1-\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$$ | ğŸ§­ |
| **6** | **Add/subtract the margin of error to the sample mean** | Hata payÄ±nÄ± Ã¶rneklem ortalamasÄ±na ($\bar{x}$) ekleyip Ã§Ä±karÄ±n. Bu, aralÄ±ÄŸÄ±n alt ve Ã¼st limitlerini belirler. | â•â– |

## ğŸ“Š GÃ¼ven AralÄ±ÄŸÄ± FormÃ¼lÃ¼ ve BileÅŸenleri

| BileÅŸen | FormÃ¼l/GÃ¶sterim | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **GÃ¼ven AralÄ±ÄŸÄ±** | $$\bar{x} \pm \text{Hata PayÄ±}$$ | PopÃ¼lasyon ortalamasÄ±nÄ±n bÃ¼yÃ¼k ihtimalle iÃ§inde bulunduÄŸu aralÄ±k. |
| **Hata PayÄ± (Margin of Error)** | $$Z_{1-\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$$ | Ã–rneklem ortalamasÄ±nÄ±n, gerÃ§ek popÃ¼lasyon ortalamasÄ±ndan sapabileceÄŸi maksimum deÄŸer. |
| **Kritik DeÄŸer** | $$Z_{1-\alpha/2}$$ | GÃ¼ven seviyesini temsil eden Z-Skoru (Ã–rn: 1.96). |
| **Standart Hata** | $$\frac{\sigma}{\sqrt{n}}$$ | Ã–rneklem ortalamalarÄ±nÄ±n standart sapmasÄ±. |

# âš™ï¸ Ä°statistiksel Ã‡Ä±karÄ±m VarsayÄ±mlarÄ± (Assumptions)

Bu varsayÄ±mlar, GÃ¼ven AralÄ±ÄŸÄ± ve Hipotez Testleri gibi Ã§Ä±karÄ±msal istatistik yÃ¶ntemlerinin geÃ§erli ve gÃ¼venilir olmasÄ± iÃ§in kritik Ã¶neme sahiptir.

| VarsayÄ±m | AÃ§Ä±klama | Neden Ã–nemli? | Emoji |
| :--- | :--- | :--- | :---: |
| **Simple random sample** (Basit Rastgele Ã–rneklem) | Ã–rneklem, popÃ¼lasyondaki her bireyin eÅŸit seÃ§ilme ÅŸansÄ±na sahip olduÄŸu bir yÃ¶ntemle toplanmalÄ±dÄ±r. | ğŸ›¡ï¸ **TarafsÄ±zlÄ±k ve BaÄŸÄ±msÄ±zlÄ±k:** Ã–rneklemin popÃ¼lasyonu tarafsÄ±z (Ã¶nyargÄ±sÄ±z) temsil etmesini ve gÃ¶zlemlerin birbirinden baÄŸÄ±msÄ±z olmasÄ±nÄ± saÄŸlar. | ğŸ² |
| **Sample size > 30 OR** **Population is approximately normal** | **(VEYA)** Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ $n > 30$'dan bÃ¼yÃ¼k olmalÄ±dÄ±r **ya da** popÃ¼lasyonun kendisi yaklaÅŸÄ±k olarak Normal daÄŸÄ±lÄ±ma sahip olmalÄ±dÄ±r. | ğŸ”” **Merkezi Limit Teoremi (CLT):** Bu koÅŸullardan birinin saÄŸlanmasÄ±, Ã¶rneklem ortalamalarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±nÄ±n (yaklaÅŸÄ±k olarak) Normal DaÄŸÄ±lÄ±ma uymasÄ±nÄ± garanti eder. Ä°statistiksel testlerin temelini oluÅŸturur. | ğŸ§  |

---


<img width="1197" height="586" alt="image" src="https://github.com/user-attachments/assets/4e1c9de9-af26-4cba-b240-2f1f8c5a30cd" />

* Bu gÃ¶rsel, istatistikteki en yaygÄ±n yanlÄ±ÅŸ anlaÅŸÄ±lmalardan biri olan GÃ¼ven (Confidence) ve OlasÄ±lÄ±k (Probability) arasÄ±ndaki farkÄ± mÃ¼kemmel bir ÅŸekilde Ã¶zetlemektedir.

# ğŸ¯ GÃ¼ven (Confidence) ve OlasÄ±lÄ±k (Probability) ArasÄ±ndaki Fark

Bu ayrÄ±m, istatistikteki GÃ¼ven AralÄ±ÄŸÄ± kavramÄ±nÄ±n doÄŸru anlaÅŸÄ±lmasÄ± iÃ§in kritik Ã¶neme sahiptir. Temel fark: **Parametre Sabit, AralÄ±k DeÄŸiÅŸken.**

---

## 1. DoÄŸru Yorumlama (GÃ¼ven Seviyesi) - âœ…

**Ä°fade:** The confidence interval contains the true population parameter approximately 95% of the time.
**(TÃ¼rkÃ§e: GÃ¼ven aralÄ±ÄŸÄ±, gerÃ§ek popÃ¼lasyon parametresini yaklaÅŸÄ±k olarak zamanÄ±n %95'inde iÃ§erir.)**

### âœï¸ AÃ§Ä±klama

GÃ¼ven seviyesi, **yÃ¶ntemin gÃ¼venilirliÄŸini** gÃ¶sterir. Bir gÃ¼ven aralÄ±ÄŸÄ± hesaplama sÃ¼recini (Ã¶rneklem alma, aralÄ±k hesaplama) sonsuz kez tekrarladÄ±ÄŸÄ±nÄ±zÄ± varsayÄ±n. OluÅŸturduÄŸunuz aralÄ±klarÄ±n yaklaÅŸÄ±k %95'i, gerÃ§ekten doÄŸru popÃ¼lasyon parametresini (Ã¶rneÄŸin $\mu$) yakalayacaktÄ±r. **GÃ¼ven, bu tekrarlanan sÃ¼reÃ§ten gelir.**

### ğŸ’° Ã–rnek

* Bir ÅŸirketteki Ã§alÄ±ÅŸanlarÄ±n gerÃ§ek ortalama maaÅŸÄ±nÄ± ($X$ TL) tahmin etmek istiyorsunuz.
* Her gÃ¼n 100 farklÄ± Ã§alÄ±ÅŸan grubundan bir Ã¶rneklem alÄ±p, 100 farklÄ± %95 gÃ¼ven aralÄ±ÄŸÄ± hesaplÄ±yorsunuz.
* Bu 100 aralÄ±ktan yaklaÅŸÄ±k **95 tanesi** gerÃ§ek ortalama maaÅŸ olan $X$ deÄŸerini iÃ§erecektir. Geri kalan 5 tanesi ise iÃ§ermeyecektir.

---

## 2. YanlÄ±ÅŸ Yorumlama (OlasÄ±lÄ±k) - âŒ

**Ä°fade:** There's a 95% probability that the population parameter falls within the confidence interval.
**(TÃ¼rkÃ§e: PopÃ¼lasyon parametresinin gÃ¼ven aralÄ±ÄŸÄ± iÃ§ine dÃ¼ÅŸme olasÄ±lÄ±ÄŸÄ± %95'tir.)**

### âœï¸ AÃ§Ä±klama

Tek bir gÃ¼ven aralÄ±ÄŸÄ± hesaplandÄ±ktan ve sÄ±nÄ±rlarÄ± belirlendikten sonra, gerÃ§ek popÃ¼lasyon parametresi **ya o aralÄ±ÄŸÄ±n iÃ§indedir (OlasÄ±lÄ±k 1) ya da dÄ±ÅŸÄ±ndadÄ±r (OlasÄ±lÄ±k 0).** Parametrenin aralÄ±ÄŸa dÃ¼ÅŸme olasÄ±lÄ±ÄŸÄ± %95 olamaz, Ã§Ã¼nkÃ¼ **parametre sabittir.** Sizin yaptÄ±ÄŸÄ±nÄ±z ÅŸey, doÄŸru sonucu yakalayan bir aralÄ±k oluÅŸturma yÃ¶ntemine %95 oranÄ±nda gÃ¼venmektir.

### ğŸ›‘ Ã–rnek

* Sadece **bir kez** %95 gÃ¼ven aralÄ±ÄŸÄ± hesapladÄ±nÄ±z ve aralÄ±k $[4000 \text{ TL}, 4500 \text{ TL}]$ Ã§Ä±ktÄ±.
* GerÃ§ek ortalama maaÅŸ ($\mu$) **ya bu aralÄ±ÄŸÄ±n iÃ§indedir ya da dÄ±ÅŸÄ±ndadÄ±r.** $\mu$'nun bu aralÄ±ÄŸÄ±n iÃ§ine dÃ¼ÅŸme olasÄ±lÄ±ÄŸÄ± %95 **deÄŸildir.**

---

**KÄ±sacasÄ±: Parametre sabit, aralÄ±k deÄŸiÅŸkendir.**

# âš–ï¸ GÃ¼ven (Confidence) ve OlasÄ±lÄ±k (Probability) ArasÄ±ndaki Fark

Bu gÃ¶rsel, istatistikteki %95 GÃ¼ven Seviyesinin (Confidence Level) doÄŸru ve yanlÄ±ÅŸ yorumlanma ÅŸekillerini gÃ¶stererek temel bir kavram hatasÄ±nÄ± dÃ¼zeltmektedir.

## Temel Ä°lke

Ä°statistiksel Ã§Ä±karÄ±mda **popÃ¼lasyon parametresi (Ã¶rneÄŸin $\mu$) sabittir**, **gÃ¼ven aralÄ±ÄŸÄ± ise Ã¶rneÄŸe gÃ¶re deÄŸiÅŸen deÄŸiÅŸkendir**. Bu nedenle, tek bir aralÄ±k iÃ§in olasÄ±lÄ±ktan bahsedilemez.

| Durum | Ä°fade | Yorumlama (Neden DoÄŸru/YanlÄ±ÅŸ?) | Emoji |
| :---: | :--- | :--- | :---: |
| **DOÄRU** âœ… | **The confidence interval contains the true population parameter approximately 95% of the time.** (GÃ¼ven aralÄ±ÄŸÄ±, gerÃ§ek popÃ¼lasyon parametresini yaklaÅŸÄ±k olarak zamanÄ±n %95'inde iÃ§erir.) | **âœ… YÃ¶nteme GÃ¼ven:** Bu ifade, **tekrarlanan deneme sÃ¼recinin** baÅŸarÄ± oranÄ±nÄ± belirtir. YÃ¶ntem, oluÅŸturulan aralÄ±klarÄ±n %95'inin gerÃ§ek parametreyi yakalamasÄ± iÃ§in tasarlanmÄ±ÅŸtÄ±r. | ğŸ¯ |
| **YANLIÅ** âŒ | **There's a 95% probability that the population parameter falls within the confidence interval.** (PopÃ¼lasyon parametresinin gÃ¼ven aralÄ±ÄŸÄ± iÃ§ine dÃ¼ÅŸme olasÄ±lÄ±ÄŸÄ± %95'tir.) | **âŒ Tekil OlasÄ±lÄ±k:** Tek bir aralÄ±k hesaplandÄ±ktan sonra, gerÃ§ek parametre ya aralÄ±ÄŸÄ±n iÃ§indedir (OlasÄ±lÄ±k 1) ya da dÄ±ÅŸÄ±ndadÄ±r (OlasÄ±lÄ±k 0). Parametre sabit olduÄŸu iÃ§in, onun aralÄ±ÄŸa dÃ¼ÅŸme olasÄ±lÄ±ÄŸÄ±ndan sÃ¶z etmek hatalÄ±dÄ±r. | ğŸ›‘ |

## Ã–zet

GÃ¼ven Seviyesi, **"Bu aralÄ±ÄŸÄ± oluÅŸturan yÃ¶nteme %95 oranÄ±nda gÃ¼veniyorum"** demektir. OlasÄ±lÄ±k ise **tek bir olayÄ±n** gerÃ§ekleÅŸme ÅŸansÄ±dÄ±r.

# ğŸ¤– Makine Ã–ÄŸrenmesi (ML) Regresyon Metrikleri

Bu metrikler, bir regresyon modelinin tahminlerinin, gerÃ§ek deÄŸerlere ne kadar yakÄ±n olduÄŸunu (model hatasÄ±nÄ±) Ã¶lÃ§mek iÃ§in kullanÄ±lÄ±r.

| Metrik (KÄ±saltma) | Ä°ngilizce AdÄ± | AÃ§Ä±lÄ±mÄ± ve Hesaplama YÃ¶ntemi | ML'deki Ã–nemi ve KullanÄ±mÄ± | Emoji |
| :---: | :--- | :--- | :--- | :---: |
| **MSE** | **Mean Squared Error** | **Ortalama Karesel Hata.** Hata (gerÃ§ek deÄŸer - tahmin) hesaplanÄ±r, karesi alÄ±nÄ±r ve tÃ¼m Ã¶rnekler iÃ§in ortalamasÄ± bulunur. BÃ¼yÃ¼k hatalarÄ± cezalandÄ±rÄ±r. | âš¡ **KullanÄ±m:** TÃ¼revlenebilir olmasÄ± nedeniyle Gradyan Ä°niÅŸ (Gradient Descent) gibi optimizasyon algoritmalarÄ±nda yaygÄ±n olarak kayÄ±p fonksiyonu (loss function) olarak kullanÄ±lÄ±r. **Dezavantaj:** Birimi, hedef deÄŸiÅŸkenin biriminin karesidir. |  squared |
| **MAE** | **Mean Absolute Error** | **Ortalama Mutlak Hata.** HatanÄ±n mutlak deÄŸeri alÄ±nÄ±r ve tÃ¼m Ã¶rnekler iÃ§in ortalamasÄ± bulunur. | ğŸ›¡ï¸ **KullanÄ±m:** Birim ile aynÄ± birimde olduÄŸu iÃ§in sonuÃ§larÄ±n yorumlanmasÄ± kolaydÄ±r. Ã–zellikle aykÄ±rÄ± deÄŸerlerin (outliers) cezalandÄ±rÄ±lmasÄ±nÄ±n istenmediÄŸi durumlarda tercih edilir. **Dezavantaj:** Mutlak deÄŸer fonksiyonu nedeniyle tÃ¼revi her yerde yoktur, optimizasyonda zorluk Ã§Ä±karabilir. | ğŸ”¢ |
| **MAPE** | **Mean Absolute Percentage Error** | **Ortalama Mutlak YÃ¼zde Hata.** Mutlak hata, gerÃ§ek deÄŸere bÃ¶lÃ¼nerek yÃ¼zdeye Ã§evrilir ve ortalamasÄ± alÄ±nÄ±r. | à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œ **KullanÄ±m:** Tahmin doÄŸruluÄŸunu yÃ¼zde cinsinden ifade ettiÄŸi iÃ§in iÅŸ birimleri ve yÃ¶netim tarafÄ±ndan kolayca anlaÅŸÄ±lÄ±r. **Dezavantaj:** GerÃ§ek deÄŸer sÄ±fÄ±ra yakÄ±n olduÄŸunda tanÄ±msÄ±z olabilir veya Ã§ok bÃ¼yÃ¼k deÄŸerler alabilir. | ğŸ’° |
| **Max Error** | **Maximum Error** | **Maksimum Hata.** Tahmin edilen deÄŸer ile gerÃ§ek deÄŸer arasÄ±ndaki mutlak farkÄ±n, veri setindeki en bÃ¼yÃ¼k deÄŸeridir. | ğŸš¨ **KullanÄ±m:** Modelin **en kÃ¶tÃ¼** durumda ne kadar yanlÄ±ÅŸ tahmin yaptÄ±ÄŸÄ±nÄ± gÃ¶sterir. Ã–zellikle gÃ¼venlik ve kritik sistemlerdeki en bÃ¼yÃ¼k riskin Ã¶lÃ§Ã¼lmesi iÃ§in Ã¶nemlidir. | âŒ |
| **R-squared** | **Coefficient of Determination** | **Belirlilik KatsayÄ±sÄ±.** Modelin, hedef deÄŸiÅŸkendeki varyasyonun ne kadarÄ±nÄ± aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir. 0 ile 1 arasÄ±nda deÄŸer alÄ±r (bazÄ± durumlarda negatif olabilir). | âœ¨ **KullanÄ±m:** Modelin ne kadar iyi oturduÄŸunu ve ne kadar aÃ§Ä±klayÄ±cÄ± olduÄŸunu anlamak iÃ§in en popÃ¼ler metriktir. YÃ¼ksek deÄŸerler (1'e yakÄ±n), modelin veriyi iyi aÃ§Ä±kladÄ±ÄŸÄ±nÄ± gÃ¶sterir. **Dikkat:** Fazla parametre eklendiÄŸinde R-squared her zaman artar, bu nedenle *Adjusted R-squared* (DÃ¼zeltilmiÅŸ R-squared) tercih edilebilir. | ğŸ” |

--

