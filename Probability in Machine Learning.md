# ğŸ§  Makine Ã–ÄŸrenimi'nin OlasÄ±lÄ±k Temelleri (Probability Foundations of Machine Learning)

Makine Ã¶ÄŸrenimi, bÃ¼yÃ¼k Ã¶lÃ§Ã¼de bilinmeyen bir sonucun, elimizdeki verilere dayanarak ortaya Ã§Ä±kma **olasÄ±lÄ±ÄŸÄ±nÄ± hesaplama** sanatÄ±dÄ±r. Temelde ML, bir olasÄ±lÄ±k hesaplama makinesi olarak iÅŸlev gÃ¶rÃ¼r.

---

## 1. KoÅŸullu OlasÄ±lÄ±k (Conditional Probability) ğŸ“Š

Makine Ã¶ÄŸrenimindeki en yaygÄ±n kullanÄ±m alanÄ±, bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± **baÅŸka faktÃ¶rler (features)** verildiÄŸinde hesaplamaktÄ±r.

| Uygulama (Application) | Hesaplanan KoÅŸullu OlasÄ±lÄ±k | AÃ§Ä±klama (Explanation) |
| :--- | :--- | :--- |
| **Spam Tespiti** (Spam Detection) ğŸ“§ | P(Spam \| Kelimeler, AlÄ±cÄ±lar, ...) | Bir e-postanÄ±n **istenmeyen posta (spam)** olma olasÄ±lÄ±ÄŸÄ±, e-postadaki **kelimeler (words)** ve diÄŸer **Ã¶zellikler (features)** verildiÄŸinde. |
| **Duygu Analizi** (Sentiment Analysis) ğŸ˜Š | P(Mutlu \| Metindeki Kelimeler) | Bir metnin **mutlu (happy)** olma olasÄ±lÄ±ÄŸÄ±, iÃ§erdiÄŸi **kelimeler (words)** verildiÄŸinde. |
| **GÃ¶rÃ¼ntÃ¼ TanÄ±ma** (Image Recognition) ğŸ“¸ | P(Kedi \| Pikseller) | Bir resimde **kedi (cat)** olma olasÄ±lÄ±ÄŸÄ±, resimdeki **pikseller (pixels)** verildiÄŸinde. |

**Teknik TanÄ±m:** KoÅŸullu olasÄ±lÄ±k, **P(A \| B)** formÃ¼lÃ¼ ile ifade edilir ve "**B olayÄ± gerÃ§ekleÅŸtiÄŸinde A olayÄ±nÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±**" anlamÄ±na gelir. Makine Ã¶ÄŸrenimi **sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±nÄ±n (classifiers)** yaptÄ±ÄŸÄ± temel iÅŸ budur.

---

## 2. SÃ¼pervizyonlu Ã–ÄŸrenme ve KoÅŸullu OlasÄ±lÄ±k (Supervised Learning and Conditional Probability)

Metinde bahsedilen Spam Tespiti, Duygu Analizi ve GÃ¶rÃ¼ntÃ¼ TanÄ±ma Ã¶rnekleri **SÃ¼pervizyonlu Ã–ÄŸrenme (Supervised Learning)** alanÄ±na aittir.

* Bu alanda, model **etiketli veri (labeled data)** kullanÄ±larak eÄŸitilir ve sorulara cevap arar.
* **AmaÃ§:** Verilen girdi iÃ§in doÄŸru **etiketi (label)** tahmin etmektir.
* **Ã–rnekler:** Resim kedi mi? CÃ¼mle olumlu mu? E-posta spam mi?

---

## 3. Bayes Teoremi ve ArtÃ§Ä± OlasÄ±lÄ±k (Bayes' Theorem and Posterior Probability) ğŸ²

Bayes Teoremi, bir **sÄ±nÄ±flandÄ±rÄ±cÄ± (classifier)** oluÅŸturarak bir ÅŸeyin olasÄ±lÄ±ÄŸÄ±nÄ± baÅŸka bir ÅŸey verildiÄŸinde hesaplamanÄ±zÄ± saÄŸlayan matematiksel bir mekanizma sunar.

$$\mathbf{P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}}$$

OlasÄ±lÄ±ÄŸÄ±n hesaplanmasÄ±nda **Bayes Teoremi'nin** nasÄ±l kullanÄ±ldÄ±ÄŸÄ± aÃ§Ä±klanmÄ±ÅŸtÄ±r:

1.  **Ã–nsel OlasÄ±lÄ±k (Prior Probability):** BaÅŸlangÄ±Ã§ olasÄ±lÄ±ÄŸÄ±dÄ±r (Ã–rn: TÃ¼m e-postalar iÃ§inde spam oranÄ±, P(Spam)).
2.  **KanÄ±t (Evidence) veya Olay (Event):** Yeni bir bilginin ortaya Ã§Ä±kmasÄ± (Ã–rn: E-postada "lottery" kelimesinin geÃ§mesi).
3.  **ArtÃ§Ä± OlasÄ±lÄ±k (Posterior Probability):** Ã–nsel olasÄ±lÄ±ÄŸÄ±n yeni bilgiyle gÃ¼ncellenmiÅŸ halidir (Ã–rn: P(Spam \| Lottery)).

---

## 4. Jeneratif Modeller ve Saf OlasÄ±lÄ±k (Generative Models and Pure Probability) âœ¨

Metin, koÅŸullu olasÄ±lÄ±ÄŸÄ±n yanÄ± sÄ±ra **saf olasÄ±lÄ±klarÄ±n (pure probabilities)** kullanÄ±ldÄ±ÄŸÄ± bÃ¼yÃ¼k bir ML alanÄ±nÄ± daha tanÄ±tÄ±r: **Jeneratif Makine Ã–ÄŸrenimi (Generative Machine Learning)**. Bu, **GÃ¶zetimsiz Ã–ÄŸrenme (Unsupervised Learning)** alanÄ±nÄ±n bir parÃ§asÄ±dÄ±r.

* **AmaÃ§:** Modelin, var olan veriye benzeyen **yeni iÃ§erik** (gÃ¶rÃ¼ntÃ¼, metin, ses) Ã¼retmesini saÄŸlamaktÄ±r.
* **Ä°ÅŸleyiÅŸ:** Burada amaÃ§, koÅŸullu olasÄ±lÄ±k hesaplamak yerine, yeni Ã¼retilen verinin gerÃ§ekÃ§i olma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmektir (maximize probability)**.
    * **GÃ¶rÃ¼ntÃ¼ Ãœretimi (Image Generation - Ã–rn: StyleGAN):** Model, rastgele piksellerin insan yÃ¼zÃ¼ oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
    * **Metin Ãœretimi (Text Generation):** Model, rastgele kelimelerin anlamlÄ± ve baÄŸlamÄ±na uygun bir metin oluÅŸturma olasÄ±lÄ±ÄŸÄ±nÄ± **maksimize etmeye** Ã§alÄ±ÅŸÄ±r.
 
    * # ğŸ’¡ Makine Ã–ÄŸrenmesinde Bayes Teoremi ve Ä°lgili Konular

Bayes Teoremi, makine Ã¶ÄŸrenmesinde **olasÄ±lÄ±ksal sÄ±nÄ±flandÄ±rma** algoritmalarÄ±nÄ±n temelini oluÅŸturur. Ã–zellikle **NaÃ¯ve Bayes SÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±** gibi yaygÄ±n algoritmalarÄ± anlamak iÃ§in bu kavramlara derinlemesine hakim olmak kritiktir.

---

## ğŸ“ Bayes Teoremi'nin FormÃ¼lÃ¼ ve BileÅŸenleri

Bayes Teoremi, bir olayÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ±, Ã¶nceden bilinen koÅŸullu ve marjinal olasÄ±lÄ±klarÄ± kullanarak hesaplar.

$$\huge P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

| Terim | AdÄ± | AÃ§Ä±klama (Makine Ã–ÄŸrenmesi) |
| :--- | :--- | :--- |
| **P(A|B)** | **Sonsal OlasÄ±lÄ±k (Posterior)** | Verilen veri (B) ile Ã¶rneÄŸin belli bir sÄ±nÄ±fa (A) ait olma olasÄ±lÄ±ÄŸÄ±. (Hedefimiz budur!) |
| **P(B|A)** | **Olabilirlik (Likelihood)** | Verilen sÄ±nÄ±f (A) iÃ§in verinin (B) gÃ¶zlemlenme olasÄ±lÄ±ÄŸÄ±. |
| **P(A)** | **Ã–nsel OlasÄ±lÄ±k (Prior)** | B olayÄ± hakkÄ±nda hiÃ§bir bilgi olmadan A sÄ±nÄ±fÄ±nÄ±n genel olasÄ±lÄ±ÄŸÄ±. |
| **P(B)** | **KanÄ±t (Evidence)** | Sonsal olasÄ±lÄ±ÄŸÄ± normalize eden sabittir. |

## ğŸ¤– Naive Bayes SÄ±nÄ±flandÄ±rÄ±cÄ± Ã‡eÅŸitleri ve KarÅŸÄ±laÅŸtÄ±rmasÄ±

Naive Bayes, **Ã¶zelliklerin birbirinden baÄŸÄ±msÄ±z olduÄŸu** (saf varsayÄ±m) varsayÄ±mÄ±na dayanÄ±r ve basitliÄŸi ile Ã¶ne Ã§Ä±kar.

| Algoritma AdÄ± | Temel Ã–zellik VarsayÄ±mÄ± | Uygulama AlanÄ± | Ã–rnek Veri Tipi | AvantajlarÄ± |
| :--- | :--- | :--- | :--- | :--- |
| **Gaussian Naive Bayes** | Ã–zellikler **Normal (Gauss) daÄŸÄ±lÄ±mÄ±na** (Normal/Gaussian Distribution) uyar. | SÃ¼rekli sayÄ±sal verilerin olduÄŸu sÄ±nÄ±flandÄ±rma problemleri. | Boy, kilo, sÄ±caklÄ±k gibi sÃ¼rekli deÄŸerler. | HÄ±zlÄ± ve kÃ¼Ã§Ã¼k veri kÃ¼melerinde baÅŸarÄ±lÄ±dÄ±r. |
| **Multinomial Naive Bayes** | Ã–zellikler **Multinomial daÄŸÄ±lÄ±mÄ±na** (Multinomial Distribution) uyar (sayÄ±m verileri). | Metin sÄ±nÄ±flandÄ±rma (spam, duygu analizi). | Bir belgedeki kelimelerin frekansÄ± (sayÄ±mÄ±). | Metin verilerinde en baÅŸarÄ±lÄ± Ã§eÅŸitlerdendir. |
| **Bernoulli Naive Bayes** | Ã–zellikler **Bernoulli daÄŸÄ±lÄ±mÄ±na** (Bernoulli Distribution) uyar (ikili/Boolean). | Belge varlÄ±ÄŸÄ±/yokluÄŸu, ikili Ã¶zelliklerin olduÄŸu sÄ±nÄ±flandÄ±rmalar. | Kelimenin bir belgede bulunup bulunmamasÄ± (1/0). | Ä°kili Ã¶zellik setleri iÃ§in etkilidir. |

---

## ğŸ§  Bilinmesi Gereken Kritik Ek Konular

Bayes algoritmalarÄ±nÄ± etkili kullanmak iÃ§in anlaÅŸÄ±lmasÄ± gereken temel kavramlar:

### 1. KoÅŸullu BaÄŸÄ±msÄ±zlÄ±k VarsayÄ±mÄ± (Conditional Independence)
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in "saf" kÄ±smÄ± buradan gelir. Hedef sÄ±nÄ±f verildiÄŸinde, girdilerin (Ã¶zelliklerin) birbirlerinden baÄŸÄ±msÄ±z olduÄŸu varsayÄ±lÄ±r. Bu, hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±r.
* **Matematiksel Ä°fade:** $P(x_1, x_2, \dots, x_n | C) = \prod_{i=1}^{n} P(x_i | C)$

### 2. SÄ±fÄ±r Frekans Problemi ve Ã‡Ã¶zÃ¼mÃ¼
* **Problem:** EÄŸitim verisinde hiÃ§ gÃ¶rÃ¼lmeyen bir Ã¶zellik-sÄ±nÄ±f kombinasyonu test verisinde ortaya Ã§Ä±karsa, ilgili **Olabilirlik** ($P(B|A)$) sÄ±fÄ±r olur. Bu durumda, sonuÃ§ **Sonsal OlasÄ±lÄ±k** da sÄ±fÄ±rlanÄ±r.
* **Ã‡Ã¶zÃ¼m:** **Laplace YumuÅŸatmasÄ± (Laplace Smoothing)** kullanÄ±lÄ±r. TÃ¼m sayÄ±mlara kÃ¼Ã§Ã¼k bir pozitif deÄŸer ($\alpha$, genellikle 1) eklenerek sÄ±fÄ±r olasÄ±lÄ±klar engellenir.

### 3. Maksimum Sonsal OlasÄ±lÄ±k (Maximum A Posteriori - MAP) Karar KuralÄ±

* **AÃ§Ä±klama:** SÄ±nÄ±flandÄ±rma yaparken NaÃ¯ve Bayes, olasÄ± tÃ¼m sÄ±nÄ±flar ($C_k$) arasÄ±ndan **en yÃ¼ksek sonsal olasÄ±lÄ±ÄŸa** sahip olan sÄ±nÄ±fÄ± seÃ§er. Bu, modelin tahmin mekanizmasÄ±dÄ±r.


### 4. Bayes AÄŸlarÄ± (Bayesian Networks) ğŸŒ
* **AÃ§Ä±klama:** NaÃ¯ve Bayes'in baÄŸÄ±msÄ±zlÄ±k varsayÄ±mÄ±nÄ±n Ã¶tesine geÃ§en, Ã¶zellikler arasÄ±ndaki **baÄŸÄ±mlÄ±lÄ±klarÄ±** modelleyebilen daha geliÅŸmiÅŸ olasÄ±lÄ±ksal grafik modellerdir.
* **Ã–nemi:** Ã–zellikler arasÄ±ndaki nedensel iliÅŸkileri (YÃ¶nlendirilmiÅŸ Asiklik Grafikler - DAG) kullanarak daha doÄŸru, ancak daha karmaÅŸÄ±k Ã§Ä±karÄ±mlar saÄŸlar.

---

## âœ‰ï¸ Ã–rnek Uygulama: E-posta Spam Tespiti (Multinomial NB)

**AmaÃ§:** "Ãœcretsiz para" e-postasÄ±nÄ± sÄ±nÄ±flandÄ±rmak.

### 1. Ã–nsel OlasÄ±lÄ±klar (Prior)

| SÄ±nÄ±f | SayÄ±m | $P(C)$ |
| :--- | :--- | :--- |
| Spam | 3 | $3/5 = 0.6$ |
| Ham | 2 | $2/5 = 0.4$ |

### 2. Olabilirlikler (Likelihood) - Laplace YumuÅŸatmasÄ± UygulanmÄ±ÅŸ

KullanÄ±lan kelimeler: "Ãœcretsiz" ve "Para".

* $P(\text{Ãœcretsiz}|\text{Spam}) = 3/17$
* $P(\text{Para}|\text{Spam}) = 2/17$

* $P(\text{Ãœcretsiz}|\text{Ham}) = 1/14$
* $P(\text{Para}|\text{Ham}) = 1/14$

### 3. Sonsal OlasÄ±lÄ±k HesabÄ±

MAP kuralÄ±nÄ± uygulayarak:

* **Spam PuanÄ±:** $P(\text{Spam}|D) \propto P(\text{Ãœcretsiz}|\text{Spam}) \cdot P(\text{Para}|\text{Spam}) \cdot P(\text{Spam})$
    * Spam PuanÄ± $\propto (3/17) \cdot (2/17) \cdot 0.6 \approx 0.0124$

* **Ham PuanÄ±:** $P(\text{Ham}|D) \propto P(\text{Ãœcretsiz}|\text{Ham}) \cdot P(\text{Para}|\text{Ham}) \cdot P(\text{Ham})$
    * Ham PuanÄ± $\propto (1/14) \cdot (1/14) \cdot 0.4 \approx 0.0020$

**SonuÃ§:** $0.0124 > 0.0020$ olduÄŸu iÃ§in model, e-postayÄ± **SPAM** olarak sÄ±nÄ±flandÄ±rÄ±r. âœ…

---
---

<img width="930" height="347" alt="image" src="https://github.com/user-attachments/assets/ebf529b1-ea70-4f52-b2cc-874a6a8607f4" />

## â“ OlasÄ±lÄ±k Problemi Ã‡Ã¶zÃ¼mÃ¼: Yetersiz Bilgi

Bu, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ± (Probability of Union)** hesaplama problemidir. Temel kural, iki olayÄ±n **kesiÅŸiminin ($P(A \text{ ve } B)$)** bilinmesini gerektirir.

### ğŸ’Š Deney Verileri

* **Toplam Hasta SayÄ±sÄ±:** 100
* **BaÅŸ AÄŸrÄ±sÄ± YaÅŸayanlar (Headache - H):** 50
* **AteÅŸ YaÅŸayanlar (Fever - F):** 50

**OlasÄ±lÄ±klar (Prior):**
* $P(H) = \frac{50}{100} = 0.5$
* $P(F) = \frac{50}{100} = 0.5$

### ğŸ¯ Ä°stenen OlasÄ±lÄ±k

Doktorlar, bir hastanÄ±n **BaÅŸ AÄŸrÄ±sÄ± VEYA AteÅŸ** yaÅŸama olasÄ±lÄ±ÄŸÄ±nÄ±, yani $\mathbf{P(H \cup F)}$'i bulmak istiyor.

### ğŸ“ OlasÄ±lÄ±klarÄ±n BirleÅŸim KuralÄ± (The Addition Rule)

Genel kural ÅŸudur:
$$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

Bu kuralÄ± uygulamak iÃ§in $\mathbf{P(\text{BaÅŸ AÄŸrÄ±sÄ± ve AteÅŸ})}$ terimine ihtiyacÄ±mÄ±z var. Bu, aynÄ± anda hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ yaÅŸayan hastalarÄ±n oranÄ±nÄ± ifade eder. Soruda bu **kesiÅŸim olasÄ±lÄ±ÄŸÄ±** (yani kaÃ§ hastanÄ±n Ã§ifte semptom yaÅŸadÄ±ÄŸÄ±) **verilmemiÅŸtir**.

### âŒ GeÃ§ersiz VarsayÄ±mlarÄ±n Ä°ncelenmesi

| VarsayÄ±m | Kural | Neden GeÃ§ersiz? |
| :--- | :--- | :--- |
| **AyrÄ±k Olaylar** (Mutually Exclusive) | $P(A \text{ veya } B) = P(A) + P(B) = 1$ | HiÃ§bir hastanÄ±n iki semptomu birden yaÅŸamadÄ±ÄŸÄ± varsayÄ±lÄ±r. Soruda bu bilgi **yoktur**. |
| **BaÄŸÄ±msÄ±z Olaylar** (Independent) | $P(A \text{ ve } B) = P(A) \cdot P(B) = 0.25$ | TÄ±bbi semptomlar genellikle **baÄŸÄ±mlÄ±dÄ±r**. Ä°laÃ§, semptomlardan birini tetikleyebilir veya engelleyebilir. Bu varsayÄ±mÄ±n doÄŸru olduÄŸu **garanti edilemez**. |

### ğŸ›‘ SonuÃ§

$\mathbf{P(\text{H ve F})}$ deÄŸeri (kesiÅŸim) bilinmediÄŸi sÃ¼rece, **BirleÅŸim OlasÄ±lÄ±ÄŸÄ±** doÄŸru bir ÅŸekilde hesaplanamaz.

* **Ek Bilgi Gereksinimi:** KaÃ§ hasta **sadece** baÅŸ aÄŸrÄ±sÄ±, kaÃ§ hasta **sadece** ateÅŸ ve kaÃ§ hasta **hem** baÅŸ aÄŸrÄ±sÄ± **hem de** ateÅŸ yaÅŸadÄ±?

**DoÄŸru Ä°fade:**
> **Not enough information is given to calculate $P(\text{fever or headache})$.**
> (BaÅŸ aÄŸrÄ±sÄ± veya ateÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± hesaplamak iÃ§in yeterli bilgi verilmemiÅŸtir.)
>
> ## ğŸ“Œ AÃ§Ä±klamanÄ±n Ã–zeti: Neden Yetersiz Bilgi?

AÃ§Ä±klama, temel olasÄ±lÄ±k kurallarÄ±ndan yola Ã§Ä±karak ÅŸunlarÄ± sÃ¶ylÃ¼yor:

1.  **Olaylar OrtaktÄ±r (Joint Events) ğŸ¤:** Problem metninde, iki olayÄ±n (**baÅŸ aÄŸrÄ±sÄ±** ve **ateÅŸ**) **ayrÄ±k (disjoint)** olduÄŸu, yani aynÄ± anda gerÃ§ekleÅŸemeyeceÄŸi sÃ¶ylenmiyor. DolayÄ±sÄ±yla, bazÄ± kiÅŸilerin **hem baÅŸ aÄŸrÄ±sÄ± hem de ateÅŸ** yaÅŸama ihtimali vardÄ±r. Bu tÃ¼r Ã§akÄ±ÅŸan olaylara **ortak olaylar (joint events)** denir.

2.  **KesiÅŸim Bilinmeli â“:** Ä°ki olayÄ±n birleÅŸim olasÄ±lÄ±ÄŸÄ±nÄ± ($P(A \text{ veya } B)$) hesaplamak iÃ§in genel **Toplama KuralÄ± (Addition Rule)**'nÄ± kullanmak zorunludur:
    $$\mathbf{P(A \text{ veya } B)} = P(A) + P(B) - P(A \text{ ve } B)$$

3.  **SonuÃ§ ğŸ›‘:** Bu nedenle, $P(\text{ateÅŸ veya baÅŸ aÄŸrÄ±sÄ±})$ olasÄ±lÄ±ÄŸÄ±nÄ± bulmak iÃ§in, olaylarÄ±n kesiÅŸim olasÄ±lÄ±ÄŸÄ± olan **$P(\text{ateÅŸ VE baÅŸ aÄŸrÄ±sÄ±})$** deÄŸerini **bilmeniz gerekir**. Bu bilgi problem metninde verilmediÄŸi iÃ§in, olasÄ±lÄ±k hesaplanamaz.

---

<img width="870" height="357" alt="image" src="https://github.com/user-attachments/assets/f76afc36-6297-45df-9f2a-011289ddd967" />


## ğŸ KoÅŸullu OlasÄ±lÄ±k Ã‡Ã¶zÃ¼mÃ¼: YazÄ±lÄ±m Testi

Bu problem, **Bayes Teoremi'nin temelini oluÅŸturan** bir **KoÅŸullu OlasÄ±lÄ±k (Conditional Probability)** problemidir. Bir kullanÄ±cÄ±nÄ±n hata deneyimlediÄŸi bilgisi **verildiÄŸinde**, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±nÄ± bulmayÄ± amaÃ§lar.

### ğŸ“ 1. Olay TanÄ±mlarÄ± ve FormÃ¼l

* **B:** KullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmesi.
* **H:** KullanÄ±cÄ±nÄ±n **Hata (Bug)** deneyimlemesi.

Aranan olasÄ±lÄ±k: $P(B|H)$ (Hata deneyimlediÄŸine gÃ¶re, Versiyon B'yi test etme olasÄ±lÄ±ÄŸÄ±).

$$\mathbf{P(B|H)} = \frac{P(B \cap H)}{P(H)}$$

### ğŸ“Š 2. Verilerin Ã–zetlenmesi

Toplam kullanÄ±cÄ± sayÄ±sÄ±: $4000 + 5000 = \mathbf{9000}$

| Kategori | DeÄŸer | Olay SayÄ±sÄ± (N) | OlasÄ±lÄ±k (P) |
| :--- | :--- | :--- | :--- |
| **Versiyon B KullanÄ±cÄ±larÄ±** | 5000 | $N(B)$ | $P(B) = 5000/9000$ |
| **Toplam Hata Deneyimleyenler** | 3000 | $N(H)$ | $P(H) = 3000/9000$ |
| **Versiyon B ve Hata KesiÅŸimi** | 1500 | $N(B \cap H)$ | $P(B \cap H) = 1500/9000$ |

### ğŸ§  3. Hesaplama (KullanÄ±cÄ± SayÄ±larÄ±yla)

OlasÄ±lÄ±klar yerine, hesaplamanÄ±n daha basit olmasÄ± iÃ§in doÄŸrudan **kullanÄ±cÄ± sayÄ±larÄ±nÄ±** kullanabiliriz, Ã§Ã¼nkÃ¼ payda ($N(\text{Toplam})$) sadeleÅŸecektir:

$$\mathbf{P(B|H)} = \frac{N(\text{B ve H})}{N(H)} = \frac{\text{Versiyon B ile hata deneyimleyenler}}{\text{Toplam hata deneyimleyenler}}$$

$$P(B|H) = \frac{1500}{3000}$$

$$P(B|H) = \frac{1}{2} = \mathbf{0.5}$$

---

### âœ… SonuÃ§

Bir kullanÄ±cÄ±nÄ±n **hata deneyimlediÄŸi** bilgisi verildiÄŸinde, bu kullanÄ±cÄ±nÄ±n **Versiyon B**'yi test etmiÅŸ olma olasÄ±lÄ±ÄŸÄ± $\mathbf{1/2}$ (**%50**) olarak bulunur.
