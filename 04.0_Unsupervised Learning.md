

<img width="1504" height="780" alt="image" src="https://github.com/user-attachments/assets/df684f88-a47d-44ec-81db-f22c352eac2f" />

* [Picture](https://www.labellerr.com/blog/supervised-vs-unsupervised-learning-whats-the-difference/)

# Supervised vs. Unsupervised Learning: A Technical Comparison ‚öñÔ∏è

This table details the key differences in data, goal, algorithm, and evaluation between Supervised and Unsupervised Learning.

| Feature | Supervised Learning üéØ (Denetimli √ñƒürenme) | Unsupervised Learning üîç (Denetimsiz √ñƒürenme) |
| :--- | :--- | :--- |
| **Data Type** üíæ | **Labeled Data** ($\mathbf{X}$, $\mathbf{y}$). Each input ($\mathbf{X}$) is paired with a corresponding output label ($\mathbf{y}$). | **Unlabeled Data** ($\mathbf{X}$). Only the input features are available; there is no target label ($\mathbf{y}$). |
| **Primary Goal** ‚û°Ô∏è | **Prediction/Mapping.** The model learns a function to map inputs to known outputs ($f(\mathbf{X}) \rightarrow \mathbf{y}$). | **Discovery/Structure.** The model aims to infer the underlying patterns, hidden structures, or natural groupings within the data. |
| **Typical Problems** üß© | **Classification** (predicting discrete labels) and **Regression** (predicting continuous values). | **Clustering** (grouping similar data points), **Dimensionality Reduction** (data compression), and **Association** (finding relationships). |
| **Model Training Process** ‚öôÔ∏è | Training is done using an **"external supervisor"** (the $\mathbf{y}$ labels) to guide the learning process and correct errors during iterative training. | Training is **autonomous** and exploratory. The model seeks similarity and distribution patterns entirely on its own. |
| **Common Algorithms** üßÆ | Linear Regression, Logistic Regression, Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Decision Trees, Random Forest, Neural Networks. | K-Means Clustering, Hierarchical Clustering, DBSCAN, Principal Component Analysis (PCA), t-SNE, Autoencoders, Apriori Algorithm. |
| **Evaluation Metrics** üíØ | Based on a known **Ground Truth**. Metrics include Accuracy, Precision, Recall, F1-Score (for Classification), and RMSE, MAE, $R^2$ (for Regression). | Lacks a Ground Truth for external validation. Metrics focus on **internal consistency** (e.g., Silhouette Score, Davies‚ÄìBouldin Index) or application-specific measures. |
| **Computational Requirement** ‚è±Ô∏è | Can be computationally **intensive** during the training phase due to optimizing complex loss functions against known labels. | Can be computationally **intensive** when dealing with high-dimensional data or complex clustering (e.g., large-scale distance calculations). |
| **Real-World Examples** üåç | Spam detection, image recognition (labeling an object), house price prediction, sentiment analysis. | Customer segmentation, anomaly/fraud detection, data compression for visualization, market basket analysis. |

# üß† Unsupervised Learning: Discovering Structure in Unlabeled Data

## 1. Introduction: The Need for Autonomy

Traditional **Supervised Learning** relies on datasets where each instance contains both **features ($\mathbf{X}$)** (input data, e.g., a passenger's age, class, sex) and a corresponding **target ($\mathbf{y}$)** (the ground-truth label, e.g., 'survived' or 'not survived').

However, in many real-world scenarios, the target labels are unavailable, impractical to obtain, or simply nonexistent. This presents the fundamental challenge: **how can a computational system derive meaningful insights, patterns, or intrinsic structures from raw, unlabeled data entirely on its own?**

This autonomy is the domain of **Unsupervised Learning**.

***

## 2. Technical Definition and Core Objective

**Unsupervised Learning (UL)** is a category of Machine Learning algorithms designed to infer a function to describe hidden structure from **unlabeled data** ($\mathbf{X}$) ‚Äî data that has no associated output values ($\mathbf{y}$).

* **Input:** Only the **feature matrix ($\mathbf{X}$)**.
* **Goal:** To model the underlying distribution, density, or latent variables of the data to facilitate:
    1.  **Pattern Discovery:** Identifying recurring relationships or associations.
    2.  **Dimensionality Reduction:** Compressing the feature space while retaining essential information.
    3.  **Clustering:** Grouping similar data points together.

### Core Analogy (Dimensionality/Similarity)

Consider a large, unsorted dataset of fruits. A UL algorithm doesn't know the human-defined labels ('apple', 'watermelon'). Instead, it analyzes the feature space (e.g., color, size, shape, weight) and identifies **natural separations** where intra-group variance is minimized and inter-group variance is maximized. This process defines the intrinsic structure of the data.

***

## 3. The Rationale: Why Unlabeled Data Persists

The prevalence of Unsupervised Learning is driven by the significant practical constraints associated with data labeling (or **annotation**):

| Constraint | Description | Example |
| :--- | :--- | :--- |
| **Cost & Scale** | Manual labeling of vast datasets (Big Data) is prohibitively expensive and time-consuming, requiring extensive human effort. | Annotating millions of high-resolution images or hours of streaming audio. |
| **Expert Dependency** | Specialized domains require highly skilled subject-matter experts for accurate annotation, which are scarce resources. | Labeling rare astronomical events or complex genetic sequences. |
| **Epistemological Gap** | In exploratory data analysis (e.g., frontier science), the ground truth is **unknown**. If the underlying phenomenon is not yet understood, no meaningful label can be assigned. | Identifying novel cell types or emergent patterns in high-dimensional genomic data. |

UL algorithms bypass this dependency, providing the initial step to structure raw data, which can then potentially be used in a semi-supervised or domain-specific application.

***

## 4. Key Use Cases and Applications

Unsupervised learning excels in tasks aimed at simplification, segmentation, and novelty detection:

### A. Clustering (Segmentation)

The process of grouping a set of data objects into subsets (clusters) so that objects within the same cluster are more similar to each other than to those in other clusters.

* **Application:** **Customer Segmentation** (e.g., *k*-Means, DBSCAN) where a business wants to identify inherent market segments (e.g., "High-Value Spenders," "Occasional Browsers") without pre-defining the categories.

### B. Dimensionality Reduction

Reducing the number of random variables under consideration by obtaining a set of principal variables. This combats the **curse of dimensionality** and aids visualization.

* **Application:** **Feature Extraction** (e.g., Principal Component Analysis **PCA**, t-distributed Stochastic Neighbor Embedding **t-SNE**). Used to simplify complex genomic or image datasets for faster processing and better model interpretability.

### C. Association Rule Mining

Discovering interesting relationships between variables in large databases.

* **Application:** **Market Basket Analysis** (e.g., Apriori algorithm). Identifying products frequently purchased together (e.g., *if customer buys A and B, they are likely to buy C*).

# Real-World Applications of Unsupervised Learning (Clustering, Association, and Reduction)

| Application Area (Emoji) | Core Unsupervised Technique | Detailed Real-World Example / Benefit |
| :--- | :--- | :--- |
| **Marketing & E-commerce** üõçÔ∏è | **Clustering** (K-Means, Hierarchical) | **Customer Segmentation:** Automatically groups customers into distinct segments (e.g., "High Spenders," "Discount Seekers") based on their purchasing history, demographics, and web behavior. *Benefit:* Allows businesses to tailor personalized marketing campaigns and product offerings, significantly boosting conversion rates and customer loyalty. |
| **Finance & Security** üí∞ | **Anomaly Detection** (Isolation Forest, DBSCAN) | **Fraud and Intrusion Detection:** Identifies rare, unusual data points or patterns that deviate significantly from the established 'normal' behavior within financial transactions or network traffic logs. *Benefit:* Crucial for flagging potential credit card fraud, money laundering activities, or unauthorized network access in real-time. |
| **Media & Content** üì∫ | **Clustering** & **Association** | **Recommendation Systems:** Groups users with similar consumption habits (e.g., movie genres watched, products bought) to provide highly relevant suggestions. **Matrix Factorization** (a form of dimensionality reduction) is often used to discover latent features. *Example:* Netflix suggesting new titles based on similar viewers' preferences. |
| **Retail & Inventory** üõí | **Association Rule Mining** (Apriori) | **Market Basket Analysis (MBA):** Discovers strong relationships and "if-then" rules between items frequently purchased together in large transaction datasets. *Benefit:* Optimizes store layout, product bundling, and cross-selling strategies (e.g., placing bread next to butter). |
| **Data Preprocessing** üìä | **Dimensionality Reduction** (PCA, t-SNE) | **Data Simplification & Visualization:** Reduces the number of variables (features) in high-dimensional datasets while preserving their most critical variance. *Benefit:* Eliminates redundant data, speeds up subsequent supervised learning models, and makes complex data understandable for human visualization (e.g., plotting high-dimensional clusters on a 2D map). |
| **Natural Language Processing** üì∞ | **Topic Modeling** (LDA) & **Clustering** | **Document Organization and Thematic Grouping:** Automatically categorizes massive, unstructured text collections (e.g., news articles, research papers) into overarching themes or topics without needing predefined tags. *Example:* Google News aggregating related stories from thousands of sources under a single category. |
| **Healthcare & Genomics** üß¨ | **Clustering** & **Dimensionality Reduction** | **Patient Subgroup Identification:** Analyzes complex genetic, proteomic, or clinical data to identify natural, previously unknown subgroups of patients with similar disease characteristics. *Benefit:* Aids in discovering new disease subtypes and moving toward personalized medicine and targeted drug development. |


# Why Unsupervised Learning Matters (Denetimsiz √ñƒürenme Neden √ñnemlidir) üß†

## Why Do We Need Unsupervised Learning?

Unsupervised learning is essential because it allows us to make sense of **data without labels**. It provides a powerful set of tools when human intervention for labeling is impractical or impossible. Here are some key reasons why it matters:

### 1. Exploratory Analysis ‚ú®

When we don‚Äôt know much about a dataset, unsupervised learning helps us discover hidden **relationships, patterns, or groups**. It acts as a guide in the early stages of data analysis.

* **Example:** Finding natural **clusters** of customers in sales data, revealing market segments you didn't know existed.

### 2. Handling Massive, Unlabeled Data üíæ

Most of the data generated today‚Äîsocial media posts, healthcare records, financial transactions‚Äîis **unlabeled** and far too large to tag manually.

* Unsupervised algorithms can organize and simplify these overwhelming datasets through techniques like **Dimensionality Reduction** (e.g., PCA) and **Clustering**, making them manageable for further analysis.

### 3. Unlocking Hidden Insights üîì

Sometimes the most valuable findings are the ones we didn‚Äôt expect. Unsupervised learning can uncover **anomalies, trends, and structures** we didn‚Äôt even know to look for, leading to genuine discovery (e.g., detecting novel fraud patterns).

---

## Challenges in Unsupervised Learning ‚ö†Ô∏è

While powerful, unsupervised learning isn‚Äôt perfect. Because the data has no ground truth, it presents unique challenges:

| Challenge (Emoji) | Explanation |
| :--- | :--- |
| **Interpreting the Results** ü§î | Without labels, it‚Äôs up to humans (domain experts) to decide whether the discovered clusters, components, or patterns are actually meaningful and actionable in a business or scientific context. |
| **No Clear Evaluation Metric** üìâ | Unlike supervised learning (where we use accuracy or error rates), there‚Äôs no single **ground truth** to compare against. Evaluation often relies on subjective measures or internal metrics like silhouette scores. |
| **Ambiguity** ü§∑ | Different algorithms (or even different runs of the same stochastic algorithm) may produce slightly different groupings. The output is often less deterministic than in supervised models. |
| **Computational Complexity** ‚öôÔ∏è | Especially in high-dimensional space, algorithms like Hierarchical Clustering can be very **resource-intensive** to process, making them slow for massive datasets. |

---

## Summary üéâ

* **Unsupervised learning** = **no labels** ($y$ is missing).
* It‚Äôs about finding **patterns, groups, or structures** in data.
* Very useful in the real world when labels are **unavailable or too costly** to acquire.



# Key Differences: Supervised vs. Unsupervised Learning

In the previous lesson, we introduced the concept of **Unsupervised Learning** ($U$) where models operate on **unlabeled data** ($X$) to extract inherent patterns. Now, let's establish a technical comparison with **Supervised Learning** ($S$), which you are already familiar with.

---

## 1. Data Type: Labeled vs. Unlabeled üè∑Ô∏è

The most fundamental distinction between $S$ and $U$ learning lies in the nature of the training data:

| Feature | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Data Structure** | Requires **labeled data** ($X, y$), where each input vector $X_i$ has a corresponding output label or value $y_i$. | Operates solely on **unlabeled input data** ($X$), where the target variable $y$ is unknown. |
| **Model Function** | Learns a mapping function $f: X \rightarrow Y$. | Aims to model the underlying structure or distribution of the data $P(X)$. |

Let's visualize the raw data difference:

| Supervised Data (Classification) | Unsupervised Data (Clustering) |
| :---: | :---: |
| Data points are distinctly color-coded, representing known classes (e.g., Cat vs. Dog).  | All data points are uniformly colored, challenging the model to discover separation.  |

---

## 2. Goal: Prediction vs. Discovery üéØ

The end objective dictates the choice of the learning paradigm:

| Goal Type | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Primary Aim** | **Prediction** or **Estimation**. The goal is to accurately predict known outcomes ($y$) for new, unseen inputs ($X$). | **Discovery** or **Inference**. The goal is to find hidden structures, groupings, or representations within the data. |
| **Metric Focus** | Minimizing prediction error (e.g., $L_2$ loss for Regression). | Optimizing a measure of data structure quality (e.g., cluster compactness/separation). |

Here's an illustrative example contrasting the goals:

| Supervised Goal: House Price Prediction (Regression) | Unsupervised Goal: Customer Clustering |
| :---: | :---: |
| A red line shows the learned linear relationship between house size ($X$) and predicted price ($y$) based on labeled data.  | Data points are grouped into distinct segments (colors), reflecting underlying customer patterns without prior labels.  |

---

## 3. Problem Types and Applications üõ†Ô∏è

The absence or presence of a target variable $y$ inherently limits the types of problems each approach can address:

| Problem Type | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Common Tasks** | **Classification** (predicting discrete labels) and **Regression** (predicting continuous values). | **Clustering** (grouping similar data points) and **Dimensionality Reduction** (simplifying feature space). |
| **Examples** | Spam Detection, Image Recognition, Stock Price Forecasting. | Customer Segmentation, Anomaly Detection, Feature Extraction. |

A visual contrast of the problem output:

| Supervised Task: Classification | Unsupervised Task: Dimensionality Reduction & Clustering |
| :---: | :---: |
| A decision boundary separates data points into two distinct, predefined categories (e.g., Spam vs. Non-Spam).  | Data is reduced (e.g., using PCA) and grouped into natural, emergent clusters.  |

---

## 4. Model Complexity and Evaluation üìà

Model architecture and validation methods also differ significantly:

### Model Complexity and Architecture

* **Supervised Learning:** Often employs more **complex models** (e.g., Deep Neural Networks, Random Forests, Support Vector Machines) designed for precise, high-dimensional **mapping** and robust generalization.
* **Unsupervised Learning:** Typically relies on **simpler algorithms** (e.g., K-Means, PCA, Autoencoders) focused on finding an optimal internal **representation** or latent structure of the data.

### Evaluation Metrics

* **Supervised Learning:** Performance is evaluated against the **ground truth ($y$)** using objective external metrics: **Accuracy, Precision, Recall, F1 Score** (for classification), or **RMSE** (Root Mean Square Error, for regression).
* **Unsupervised Learning:** Due to the lack of $y$, evaluation relies on **intrinsic metrics** that measure the quality of the discovered structure: **Silhouette Score** (cluster separation and compactness), **Davies‚ÄìBouldin Index**, or Reconstruction Error (for dimensionality reduction). Interpretation often requires **human domain expertise** to validate the utility of the discovered patterns.




---
# T√úRKCE:

# Denetimsiz √ñƒürenme Nedir? (Unsupervised Learning)

## 1. Giri≈ü ve Baƒülam

≈ûu ana kadar, verinin hem **√ñzniteliklere** ($X$) hem de **Hedefe** ($y$) sahip olduƒüu **denetimli √∂ƒürenme** (supervised learning) konseptiyle √ßalƒ±≈ütƒ±k.

* **√ñznitelikler ($X$)**: Giri≈ü verisi (√∂rn. Titanic veri setindeki yolcunun ya≈üƒ±, sƒ±nƒ±fƒ±, cinsiyeti).
* **Hedef ($y$)**: Tahmin etmek istediƒüimiz √ßƒ±ktƒ± (√∂rn. hayatta kalƒ±p kalmadƒ±ƒüƒ±).

Peki ya elimizde etiketler yoksa? Ya sadece ham verimiz varsa ve bilgisayarƒ±n **kendi ba≈üƒ±na** √∂r√ºnt√ºleri, yapƒ±larƒ± veya gruplarƒ± ke≈üfetmesini istiyorsak?

ƒ∞≈üte tam bu noktada **Denetimsiz √ñƒürenme** devreye girer.

***

## 2. Teknik Tanƒ±m

**Denetimsiz √∂ƒürenme**, yalnƒ±zca **√∂zniteliklere** ($X$) sahip olduƒüumuz, ancak **etiketlerin** ($y$) bulunmadƒ±ƒüƒ± bir makine √∂ƒürenimi t√ºr√ºd√ºr.

Temel ama√ß, bilinen bir sonucu tahmin etmek deƒüil, veri i√ßerisindeki **gizli √∂r√ºnt√ºleri**, **gruplandƒ±rmalarƒ±** (k√ºmelemeyi) veya **i√ßsel yapƒ±larƒ±** ortaya √ßƒ±karmaktƒ±r.

Bu, ≈üu soruyu sormak gibidir:
üëâ "Bilgisayar, benim ne aradƒ±ƒüƒ±mƒ± s√∂ylememe gerek kalmadan bu kaostaki bir d√ºzeni bulabilir mi?"

### √ñrnek Senaryo: Meyve Ayƒ±rma
Bir sepette hem karpuz hem de elma olduƒüunu d√º≈ü√ºn√ºn, ancak bunlar etiketlenmemi≈ü. Denetimsiz √∂ƒürenme algoritmasƒ±, t√ºm karpuzlar ve t√ºm elmalar arasƒ±nda ortak olanƒ± bulmanƒ±n bir yolunu bulacaktƒ±r. Bu, boyut, renk, ≈üekil ve diƒüer ayƒ±rt edici niteliklere dayanabilir. Algoritma, bu niteliklere g√∂re verileri **k√ºmelendirerek** ayƒ±rma i≈ülemini ger√ßekle≈ütirecektir.

### √ñrnek Senaryo: K√ºt√ºphane Organizasyonu
Bir k√ºt√ºphanedeki b√ºy√ºk bir kitap yƒ±ƒüƒ±nƒ±nƒ± d√º≈ü√ºn√ºn; kitaplarda kategori veya yazar etiketi yok. Denetimsiz bir √∂ƒürenme algoritmasƒ±, kaosu d√ºzene sokmak i√ßin kapak rengi, sayfa uzunluƒüu veya hatta kullanƒ±lan kaƒüƒ±t t√ºr√ºne g√∂re kitaplarƒ± gruplandƒ±rarak ba≈ülayabilir.

***

## 3. Neden Veriyi Etiketlemiyoruz?

Denetimsiz √∂ƒürenme algoritmalarƒ±, insan tarafƒ±ndan saƒülanmƒ±≈ü etiketleri veya etiketlemeyi kullanmaz. Peki bu neden √∂nemlidir?

Bunun en b√ºy√ºk nedeni, veriyi **etiketlemenin** genellikle **zor, pahalƒ±** veya hatta **imkansƒ±z** olmasƒ±dƒ±r.

### Etiketleme (Veri A√ßƒ±klamasƒ± - Data Annotation) Nedir?
Makine √∂ƒüreniminde, veri etiketleme, ham verinin (tablodaki satƒ±rlar, g√∂rseller, ses kayƒ±tlarƒ± vb.) alƒ±nƒ±p buna bir veya daha fazla etiket atanmasƒ± anlamƒ±na gelir. Bu etiketler baƒülam ve anlam saƒülayarak makine √∂ƒürenimi modelinin √∂ƒürenmesini saƒülar.

### Zorluklar
1.  **Maliyet ve Zaman:** Tƒ±bbi ara≈ütƒ±rma gibi alanlarda o kadar √ßok veri vardƒ±r ki, her ≈üeyi etiketlemek yƒ±llar s√ºrebilir ve uzmanlardan muazzam bir √ßaba gerektirir.
2.  **Bilgi Eksikliƒüi:** Genetik veya astronomi gibi alanlarda, bazen ne aradƒ±ƒüƒ±mƒ±zƒ± bile hen√ºz bilmiyoruz. Eƒüer fenomeni anlamƒ±yorsak, onu nasƒ±l etiketleyebiliriz?

ƒ∞≈üte denetimsiz √∂ƒürenme bu noktada parlar: Ham, etiketsiz verideki **yapƒ±yƒ± ke≈üfetmemize** yardƒ±mcƒ± olur.

### Uygulama Alanlarƒ±
| Uygulama | A√ßƒ±klama | Denetimsiz Y√∂ntem |
| :--- | :--- | :--- |
| **M√º≈üteri Segmentasyonu** | Bir ≈üirketin milyonlarca satƒ±n alma kaydƒ± olabilir ancak m√º≈üteri tipleri i√ßin net etiketleri yoktur. Algoritma, insanlarƒ± "b√ºt√ße odaklƒ±" veya "sadƒ±k m√º≈üteriler" olarak manuel etiketlemek yerine, verideki doƒüal gruplandƒ±rmalarƒ± bulur. | **K√ºmeleme (Clustering)** |
| **Doƒüal Dil ƒ∞≈üleme** | Temalarƒ± bilinmeyen b√ºy√ºk bir metin koleksiyonunu analiz etmek. Denetimsiz √∂ƒürenme, √∂nceden tanƒ±mlanmƒ±≈ü kategorilere ihtiya√ß duymadan belgeleri konularƒ±na, kelime √∂r√ºnt√ºlerine veya yazƒ±m stillerine g√∂re gruplandƒ±rabilir. | **Konu Modellemesi (Topic Modeling)** |
| **Boyut Azaltma** | √áok fazla √∂zniteliƒüe sahip verinin (√∂rneƒüin binlerce) temel, daha az sayƒ±da deƒüi≈ükene indirgenmesi. | **Temel Bile≈üen Analizi (PCA)** |


# üåç Ger√ßek D√ºnya Denetimsiz √ñƒürenme √ñrnekleri (Real-World Examples of Unsupervised Learning)

Denetimsiz √∂ƒürenme, etiketsiz veriden doƒüal yapƒ±larƒ±, √∂r√ºnt√ºleri ve ili≈ükileri ke≈üfetme yeteneƒüi sayesinde bir√ßok sekt√∂rde kritik rol oynamaktadƒ±r. ƒ∞≈üte en yaygƒ±n ve etkili uygulamalardan bazƒ±larƒ±:

| Alan (Domain) | Uygulama Adƒ± ve Amacƒ± | Temel Denetimsiz Teknikler | Nasƒ±l √áalƒ±≈üƒ±r (Denetimsiz Doƒüa) |
| :--- | :--- | :--- | :--- |
| **Pazarlama ve E-Ticaret** üõçÔ∏è | **M√º≈üteri Segmentasyonu (Customer Segmentation)**: M√º≈üterileri ortak √∂zelliklerine (satƒ±n alma ge√ßmi≈üi, demografi, gezinme davranƒ±≈üƒ±) g√∂re gruplamak. | **K√ºmeleme (Clustering)**: K-Means, DBSCAN, Hiyerar≈üik K√ºmeleme. | Algoritma, hangi m√º≈üteri gruplarƒ±nƒ±n var olduƒüunu *√∂nceden bilmez*. Veriyi analiz ederek doƒüal olarak benzer davranƒ±≈ü g√∂steren gruplarƒ± **otomatik olarak ke≈üfeder**. |
| **Finans ve Siber G√ºvenlik** üõ°Ô∏è | **Anormallik/Sahtekarlƒ±k Tespiti (Anomaly/Fraud Detection)**: Normal √∂r√ºnt√ºden belirgin ≈üekilde sapan sƒ±ra dƒ±≈üƒ± veri noktalarƒ±nƒ± (i≈ülemler, aƒü trafiƒüi) belirlemek. | **K√ºmeleme (Clustering)**: DBSCAN; **Boyut Azaltma (Dimensionality Reduction)**: Autoencoders, Isolation Forest. | Algoritma, verideki 'normal' davranƒ±≈ü modelini √∂ƒürenir. Yeni bir i≈ülem bu normal modelin **dƒ±≈üƒ±na √ßƒ±ktƒ±ƒüƒ±nda** onu anomali olarak i≈üaretler, yani anormali etiketleyen bir **eƒüitmen yoktur**. |
| **Perakende ve Lojistik** üõí | **Pazar Sepeti Analizi (Market Basket Analysis)**: M√º≈üterilerin birlikte satƒ±n alma olasƒ±lƒ±ƒüƒ± en y√ºksek olan √ºr√ºn gruplarƒ±nƒ± bulmak. | **Birliktelik Kuralƒ± Madenciliƒüi (Association Rule Mining)**: Apriori, Eclat. | Sistem, "X √ºr√ºn√º satƒ±n alƒ±ndƒ±ysa, Y √ºr√ºn√º de satƒ±n alƒ±nƒ±r" gibi **gizli ili≈ükileri** bulmak i√ßin milyonlarca i≈ülemi tarar. Bu ili≈ükiler **√∂nceden etiketlenmemi≈ütir**. |
| **Tavsiye Sistemleri** üé¨ | **ƒ∞√ßerik/√úr√ºn √ñnerileri (Recommendation Systems)**: Kullanƒ±cƒ±nƒ±n ge√ßmi≈ü davranƒ±≈üƒ±na g√∂re diƒüer kullanƒ±cƒ±larla (Collaborative Filtering) veya √ºr√ºnlerle (Content-Based Filtering) olan benzerliƒüini kullanarak √∂neri sunmak. | **Boyut Azaltma (Dimensionality Reduction)**: SVD, Matris Ayrƒ±≈ütƒ±rma (Matrix Factorization). | Model, kullanƒ±cƒ± ve √∂ƒüe etkile≈üimlerinin karma≈üƒ±k yapƒ±sƒ±nƒ± daha d√º≈ü√ºk boyutlu bir g√∂sterime indirir. Bu, **etiketsiz etkile≈üim verisinden** kullanƒ±cƒ± tercihlerini **√ßƒ±karƒ±r**. |
| **Doƒüal Dil ƒ∞≈üleme (NLP)** üì∞ | **Konu Modellemesi (Topic Modeling)**: B√ºy√ºk bir metin koleksiyonundaki (haber makaleleri, e-postalar) ana, gizli konularƒ± otomatik olarak √ßƒ±karmak. | **Konu Modellemesi**: Latent Dirichlet Allocation (LDA), NMF. | Algoritma, belgeleri gruplandƒ±rmak i√ßin **konu etiketlerine** sahip deƒüildir. Metinlerdeki kelimelerin **birlikte ge√ßme √∂r√ºnt√ºlerine** bakarak konularƒ± **kendisi tanƒ±mlar**. |
| **G√∂r√ºnt√º/Video ƒ∞≈üleme** üñºÔ∏è | **G√∂r√ºnt√º Sƒ±kƒ±≈ütƒ±rma (Image Compression)**: G√∂r√ºnt√ºn√ºn kalitesini kaybetmeden boyutunu azaltmak (veri boyutunu d√º≈ü√ºrmek). | **Boyut Azaltma (Dimensionality Reduction)**: Temel Bile≈üen Analizi (PCA). | Model, g√∂r√ºnt√ºdeki en fazla bilgiyi ta≈üƒ±yan temel √∂znitelikleri (boyutlarƒ±) bularak veriyi sƒ±kƒ±≈ütƒ±rƒ±r. Bu, **g√∂r√ºnt√º etiketleri olmadan** verinin i√ßsel yapƒ±sƒ±nƒ± analiz etmeye dayanƒ±r. |
| **Genetik ve Tƒ±p** üß¨ | **Gen ƒ∞fadesi Verilerinin K√ºmelenmesi (Gene Expression Data Clustering)**: Farklƒ± genetik bozukluklarƒ± veya h√ºcre tiplerini temsil eden gen ifadesi √∂r√ºnt√ºlerini gruplamak. | **K√ºmeleme (Clustering)**: Hiyerar≈üik K√ºmeleme, K-Means. | Ara≈ütƒ±rmacƒ±lar, hangi bozukluklarƒ±n var olduƒüunu **bilmediklerinde** algoritma, veri setinde doƒüal olarak olu≈üan genetik alt gruplarƒ± (√∂rneƒüin, yeni bir kanser alt tipi) ke≈üfeder. |

# Denetimsiz √ñƒürenme Neden √ñnemlidir? (Why Unsupervised Learning Matters) üí°

Denetimsiz √∂ƒürenme, etiketler olmadan veriyi anlamlandƒ±rmamƒ±za olanak tanƒ±dƒ±ƒüƒ± i√ßin hayati √∂neme sahiptir. ƒ∞≈üte neden √∂nemli olduƒüuna dair temel nedenler:

## Denetimsiz √ñƒürenmeye Neden ƒ∞htiya√ß Duyarƒ±z?

### Ke≈üifsel Analiz (Exploratory Analysis) üßê
Bir veri k√ºmesi hakkƒ±nda √ßok az ≈üey bildiƒüimizde, denetimsiz √∂ƒürenme **gizli ili≈ükileri, √∂r√ºnt√ºleri veya gruplarƒ±** ke≈üfetmemize yardƒ±mcƒ± olur.
* **√ñrnek:** Satƒ±≈ü verilerinde m√º≈üterilerin doƒüal k√ºmelerini bulmak.

### B√ºy√ºk, Etiketsiz Veriyle Ba≈üa √áƒ±kma üí™
Sosyal medya g√∂nderileri, saƒülƒ±k kayƒ±tlarƒ±, finansal i≈ülemler gibi verilerin √ßoƒüu etiketsizdir ve manuel olarak etiketlemek i√ßin √ßok b√ºy√ºkt√ºr.
* Denetimsiz algoritmalar bu devasa veri k√ºmelerini d√ºzenleyebilir ve basitle≈ütirebilir.

### Gizli ƒ∞√ßg√∂r√ºlerin Kilidini A√ßma üîë
Bazen en deƒüerli bulgular, hi√ß beklemediƒüimiz bulgulardƒ±r.
* Denetimsiz √∂ƒürenme, aramayƒ± bile bilmediƒüimiz **anormallikleri, eƒüilimleri** ve **yapƒ±larƒ±** ortaya √ßƒ±karabilir.

---

## Denetimsiz √ñƒürenmedeki Zorluklar (Challenges) üöß

G√º√ßl√º olmasƒ±na raƒümen, denetimsiz √∂ƒürenme kusursuz deƒüildir. Temel zorluklar ≈üunlardƒ±r:

* **Sonu√ßlarƒ± Yorumlama:** Etiketler olmadan, k√ºmelerin veya √∂r√ºnt√ºlerin ger√ßekten anlamlƒ± olup olmadƒ±ƒüƒ±na karar vermek insanlara kalmƒ±≈ütƒ±r.
* **A√ßƒ±k Bir Deƒüerlendirme Metriƒüi Yok:** Denetimli √∂ƒürenmenin aksine, kar≈üƒ±la≈ütƒ±rma yapabileceƒüimiz **temel ger√ßek** (ground truth) bulunmaz.
* **Belirsizlik (Ambiguity):** Farklƒ± algoritmalar (veya aynƒ± algoritmanƒ±n farklƒ± √ßalƒ±≈ütƒ±rma sonu√ßlarƒ±) biraz farklƒ± gruplandƒ±rmalar √ºretebilir.
* **Hesaplama Karma≈üƒ±klƒ±ƒüƒ± (Computational Complexity):** B√ºy√ºk veri k√ºmelerini i≈ülemek √ßok fazla kaynak gerektirebilir.

---

## √ñzet üéâ

* **Denetimsiz √ñƒürenme = Etiket Yok.**
* Ama√ß, verideki **√∂r√ºnt√ºleri, gruplarƒ± veya yapƒ±larƒ±** bul

# Denetimli √ñƒürenmeden Temel Farklar (Key Differences From Supervised Learning) ‚öñÔ∏è

√ñnceki derste, modellerin etiketlenmemi≈ü veri √ºzerinde √ßalƒ±≈üarak anlamlƒ± √∂r√ºnt√ºler √ßƒ±kardƒ±ƒüƒ± **Denetimsiz √ñƒürenme** kavramƒ±nƒ± tanƒ±ttƒ±k. ≈ûimdi, bu yakla≈üƒ±mƒ±n daha √∂nce √∂ƒürendiƒüiniz **Denetimli √ñƒürenme** ile nasƒ±l kar≈üƒ±la≈ütƒ±rƒ±ldƒ±ƒüƒ±nƒ± inceleyelim.

---

## 1. Veri Tipi: Etiketli vs. Etiketsiz Veri

Denetimli ve denetimsiz √∂ƒürenme arasƒ±ndaki en √∂nemli fark, √ßalƒ±≈ütƒ±klarƒ± **veri t√ºr√ºd√ºr**.

| √ñzellik | Denetimli √ñƒürenme (Supervised) üéØ | Denetimsiz √ñƒürenme (Unsupervised) üîç |
| :--- | :--- | :--- |
| **Gereken Veri** | **Etiketli Veri** (Labeled Data) | **Etiketsiz Veri** (Unlabeled Data) |
| **Veri Noktasƒ± Yapƒ±sƒ±** | Her veri noktasƒ±, tahmin edilecek kar≈üƒ±lƒ±k gelen bir **√ßƒ±ktƒ±/hedef** ($y$) i√ßerir. | Model, √∂nceden "doƒüru" cevabƒ± bilmeden √∂r√ºnt√º veya yapƒ± bulmalƒ±dƒ±r. |

### G√∂rsel Kar≈üƒ±la≈ütƒ±rma: Veri G√∂r√ºn√ºm√º üìä

| Etiketli Veri (Denetimli) | Etiketsiz Veri (Denetimsiz) |
| :---: | :---: |
| Farklƒ± renklerle ayrƒ±lmƒ±≈ü "kedi" ve "k√∂pek" gibi sƒ±nƒ±flarƒ±n olduƒüu veri noktalarƒ±. Bu, modelin belirli kategorilerden √∂ƒürenmesine olanak tanƒ±r. | T√ºm noktalarƒ±n aynƒ± renkte olduƒüu veri noktalarƒ±. Bu, denetimsiz modellerin etiketler olmadan √∂r√ºnt√º ke≈üfetme zorluƒüunu temsil eder. |
|  |  |

---

## 2. Nihai Hedef: Tahmin vs. Ke≈üif

Dikkate alƒ±nmasƒ± gereken bir diƒüer √∂nemli husus da, elimizdeki verilere g√∂re ula≈ümak istediƒüimiz **nihai hedeftir**.

| Hedef | Denetimli √ñƒürenme (Prediction) ‚û°Ô∏è | Denetimsiz √ñƒürenme (Discovery) üó∫Ô∏è |
| :--- | :--- | :--- |
| **Temel Ama√ß** | Etiketli verilere dayanarak **√ßƒ±ktƒ±larƒ± tahmin etmektir**. Model, girdi ($X$) ile bilinen √ßƒ±ktƒ± ($y$) arasƒ±ndaki **e≈ülemeyi (mapping)** √∂ƒürenir. | Veri i√ßindeki **gizli yapƒ±larƒ±** ke≈üfetmektir (√∂rn. k√ºmeleri bulmak veya boyut sayƒ±sƒ±nƒ± azaltmak). |

### G√∂rsel √ñrnek: Ama√ß Netliƒüi

| Denetimli G√∂rev: Ev Fiyatƒ± Tahmini (Regresyon) | Denetimsiz G√∂rev: M√º≈üteri K√ºmelemesi (Clustering) |
| :---: | :---: |
| Sol alt grafik, ev b√ºy√ºkl√ºƒü√º ile tahmin edilen fiyat arasƒ±ndaki ili≈ükiyi g√∂steren kƒ±rmƒ±zƒ± bir regresyon doƒürusu i√ßerir. Bu, net bir i≈ü hedefidir. | Saƒü alt grafik, veri noktalarƒ±nƒ±n √∂nceden etiket olmadan k√ºmelere ayrƒ±ldƒ±ƒüƒ±nƒ± g√∂sterir. Farklƒ± renkler, ke≈üfedilen m√º≈üteri segmentlerini temsil eder. |
|  |  |

---

## 3. Problem T√ºrleri ve Algoritmalar

Her iki yakla≈üƒ±mƒ±n ele alabileceƒüi problemler doƒüasƒ± gereƒüi farklƒ±dƒ±r.

* **Denetimli √ñƒürenme:** Genellikle **Sƒ±nƒ±flandƒ±rma** (Classification) ve **Regresyon** (Regression) problemlerine uygulanƒ±r. Hedef, belirli etiketleri veya deƒüerleri tahmin etmektir (√∂rn. Spam/Spam Deƒüil).
* **Denetimsiz √ñƒürenme:** **K√ºmeleme** (Clustering), **Boyut Azaltma** (Dimensionality Reduction) ve **Birliktelik Kuralƒ± Madenciliƒüi** (Association Rule Mining) gibi problem t√ºrleri i√ßin kullanƒ±lƒ±r.

### G√∂rsel ƒ∞ll√ºstrasyon: G√∂rev Farklƒ±lƒ±klarƒ±

| Denetimli G√∂rev: Sƒ±nƒ±flandƒ±rma | Denetimsiz G√∂rev: K√ºmeleme ve Boyut Azaltma |
| :---: | :---: |
| Veri noktalarƒ± iki ayrƒ± kategoriye (√∂rn. spam vs. spam olmayan e-postalar) ayrƒ±lƒ±r. | Verinin boyutunu azaltmak ve noktalarƒ± farklƒ± k√ºmelere ayƒ±rmak i√ßin **Temel Bile≈üen Analizi (PCA)** kullanƒ±larak k√ºmeleme g√∂revi g√∂sterilir. |
|  |  |

---

## 4. Model Karma≈üƒ±klƒ±ƒüƒ± ve Deƒüerlendirme

### Model Karma≈üƒ±klƒ±ƒüƒ±

| √ñzellik | Denetimli √ñƒürenme (Supervised) üß† | Denetimsiz √ñƒürenme (Unsupervised) üß© |
| :--- | :--- | :--- |
| **Tipik Modeller** | Girdi ile √ßƒ±ktƒ± arasƒ±nda kesin e≈ülemeler √∂ƒürenmesi gereken **Derin Sinir Aƒülarƒ±** (Neural Networks) veya **Rastgele Ormanlar** (Random Forests) gibi daha **karma≈üƒ±k** modeller i√ßerir. | Ama√ß √∂r√ºnt√º ke≈üfi olduƒüu i√ßin genellikle **K-Ortalamalar** (K-means) veya **PCA** gibi daha **basit** modeller kullanƒ±lƒ±r. |

### Deƒüerlendirme Metrikleri (Evaluation Metrics)

Modelimizin ne kadar iyi performans g√∂sterdiƒüini bilmek i√ßin kullandƒ±ƒüƒ±mƒ±z metrikler de farklƒ±dƒ±r:

| √ñzellik | Denetimli √ñƒürenme (Supervised) üíØ | Denetimsiz √ñƒürenme (Unsupervised) ‚ùì |
| :--- | :--- | :--- |
| **Metrikler** | Performans; **Doƒüruluk** (Accuracy), **Hassasiyet** (Precision), **Geri √áaƒüƒ±rma** (Recall) veya **RMSE** (Regresyon i√ßin Hata Karelerinin Karek√∂k√º) gibi **temel ger√ßeƒüe** dayalƒ± metriklerle deƒüerlendirilir. | √ñnceden tanƒ±mlanmƒ±≈ü etiketler olmadƒ±ƒüƒ± i√ßin **Siluet Skoru** (Silhouette Score - k√ºmelerin ne kadar iyi ayrƒ±ldƒ±ƒüƒ±nƒ± g√∂sterir) gibi **i√ßsel (intrinsic)** metrikler kullanƒ±lƒ±r. | 

# Denetimli ve Denetimsiz √ñƒürenme Kar≈üƒ±la≈ütƒ±rmasƒ± üÜö

Bu tablo, makine √∂ƒüreniminin iki ana paradigmasƒ± olan Denetimli ve Denetimsiz √ñƒürenme arasƒ±ndaki temel teknik farklƒ±lƒ±klarƒ± detaylƒ± olarak g√∂stermektedir.

| Kar≈üƒ±la≈ütƒ±rma Kriteri | Denetimli √ñƒürenme (Supervised Learning) üéØ | Denetimsiz √ñƒürenme (Unsupervised Learning) üîç |
| :--- | :--- | :--- |
| **Girdi Verisi (Input Data)** | **Etiketli Veri** (Labeled Data). Veri k√ºmesi, girdi ($X$) ve kar≈üƒ±lƒ±k gelen √ßƒ±ktƒ± ($y$) hedefleri i√ßerir. | **Etiketsiz Veri** (Unlabeled Data). Veri k√ºmesi yalnƒ±zca girdi √∂zniteliklerini ($X$) i√ßerir; √ßƒ±ktƒ± hedefi ($y$) bilinmez. |
| **Temel Ama√ß (Goal)** | Girdi ile √ßƒ±ktƒ± arasƒ±ndaki e≈ülemeyi (mapping) √∂ƒürenerek **√ßƒ±ktƒ±yƒ± tahmin etmek** (Prediction). | Veri i√ßindeki **gizli yapƒ±larƒ±, √∂r√ºnt√ºleri veya gruplarƒ± ke≈üfetmek** (Discovery). |
| **√ñƒürenme S√ºreci** | Model, etiketleri bir **√∂ƒüretmen** (insan uzmanƒ± veya √∂nceden belirlenmi≈ü etiket) gibi kullanarak girdi-√ßƒ±ktƒ± ili≈ükisini √∂ƒürenir. | Model, herhangi bir rehberlik olmadan verinin i√ßsel √∂zelliklerine (benzerlik, farklƒ±lƒ±k vb.) dayanarak kendi kendine √∂ƒürenir. |
| **Ortak Problemler** | **Sƒ±nƒ±flandƒ±rma** (Classification: ikili/√ßok sƒ±nƒ±flƒ±) ve **Regresyon** (Regression: s√ºrekli deƒüer tahmini). | **K√ºmeleme** (Clustering), **Boyut Azaltma** (Dimensionality Reduction), **Birliktelik Kuralƒ± Madenciliƒüi** (Association Rule Mining). |
| **Yaygƒ±n Algoritmalar** | Lineer Regresyon, Lojistik Regresyon, Destek Vekt√∂r Makineleri (SVM), Karar Aƒüa√ßlarƒ±, Rastgele Ormanlar, Sinir Aƒülarƒ±. | K-Ortalamalar (K-Means), DBSCAN, Hiyerar≈üik K√ºmeleme, Temel Bile≈üen Analizi (PCA), Otomatik Kodlayƒ±cƒ±lar (Autoencoders), Birliktelik Kuralƒ± (Apriori). |
| **Modelin √áƒ±ktƒ±sƒ±** | Belirli bir **etiket** (kategori) veya **sayƒ±sal deƒüer** (tahmin). | **K√ºme ID'leri**, **indirgenmi≈ü boyutlu √∂zellikler** veya **birliktelik kurallarƒ±**. |
| **Deƒüerlendirme Metrikleri** | **Temel Ger√ßeƒüe** (Ground Truth) dayalƒ±dƒ±r: Doƒüruluk (Accuracy), Kesinlik (Precision), Geri √áaƒüƒ±rma (Recall), F1 Skoru, RMSE (Hata Karelerinin Karek√∂k√º). | **ƒ∞√ßsel Metrikler** kullanƒ±lƒ±r: Siluet Skoru (Silhouette Score), K√ºme ƒ∞√ßi Kareler Toplamƒ± (Within-Cluster Sum of Squares - WCSS), Rand ƒ∞ndeksi. |
| **Veri Maliyeti** | **Y√ºksek.** Etiketli veri elde etmek ve korumak zaman alƒ±cƒ± ve pahalƒ±dƒ±r. | **D√º≈ü√ºk.** Etiketsiz ham veri genellikle daha kolay ve ucuza toplanƒ±r. |
| **Risk ve Belirsizlik** | Tahminler genellikle daha **kesin** ve doƒürudan i≈ü hedefleriyle ili≈ükilidir. | Sonu√ßlarƒ±n (k√ºmelerin) **yorumlanmasƒ±** insan uzmanlƒ±ƒüƒ± gerektirir ve farklƒ± algoritma √ßalƒ±≈ütƒ±rmalarƒ± belirsiz sonu√ßlar √ºretebilir. |

