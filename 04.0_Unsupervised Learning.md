

<img width="1504" height="780" alt="image" src="https://github.com/user-attachments/assets/df684f88-a47d-44ec-81db-f22c352eac2f" />

* [Picture](https://www.labellerr.com/blog/supervised-vs-unsupervised-learning-whats-the-difference/)

# Supervised vs. Unsupervised Learning: A Technical Comparison âš–ï¸

This table details the key differences in data, goal, algorithm, and evaluation between Supervised and Unsupervised Learning.

| Feature | Supervised Learning ğŸ¯ (Denetimli Ã–ÄŸrenme) | Unsupervised Learning ğŸ” (Denetimsiz Ã–ÄŸrenme) |
| :--- | :--- | :--- |
| **Data Type** ğŸ’¾ | **Labeled Data** ($\mathbf{X}$, $\mathbf{y}$). Each input ($\mathbf{X}$) is paired with a corresponding output label ($\mathbf{y}$). | **Unlabeled Data** ($\mathbf{X}$). Only the input features are available; there is no target label ($\mathbf{y}$). |
| **Primary Goal** â¡ï¸ | **Prediction/Mapping.** The model learns a function to map inputs to known outputs ($f(\mathbf{X}) \rightarrow \mathbf{y}$). | **Discovery/Structure.** The model aims to infer the underlying patterns, hidden structures, or natural groupings within the data. |
| **Typical Problems** ğŸ§© | **Classification** (predicting discrete labels) and **Regression** (predicting continuous values). | **Clustering** (grouping similar data points), **Dimensionality Reduction** (data compression), and **Association** (finding relationships). |
| **Model Training Process** âš™ï¸ | Training is done using an **"external supervisor"** (the $\mathbf{y}$ labels) to guide the learning process and correct errors during iterative training. | Training is **autonomous** and exploratory. The model seeks similarity and distribution patterns entirely on its own. |
| **Common Algorithms** ğŸ§® | Linear Regression, Logistic Regression, Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Decision Trees, Random Forest, Neural Networks. | K-Means Clustering, Hierarchical Clustering, DBSCAN, Principal Component Analysis (PCA), t-SNE, Autoencoders, Apriori Algorithm. |
| **Evaluation Metrics** ğŸ’¯ | Based on a known **Ground Truth**. Metrics include Accuracy, Precision, Recall, F1-Score (for Classification), and RMSE, MAE, $R^2$ (for Regression). | Lacks a Ground Truth for external validation. Metrics focus on **internal consistency** (e.g., Silhouette Score, Daviesâ€“Bouldin Index) or application-specific measures. |
| **Computational Requirement** â±ï¸ | Can be computationally **intensive** during the training phase due to optimizing complex loss functions against known labels. | Can be computationally **intensive** when dealing with high-dimensional data or complex clustering (e.g., large-scale distance calculations). |
| **Real-World Examples** ğŸŒ | Spam detection, image recognition (labeling an object), house price prediction, sentiment analysis. | Customer segmentation, anomaly/fraud detection, data compression for visualization, market basket analysis. |

# ğŸ§  Unsupervised Learning: Discovering Structure in Unlabeled Data

## 1. Introduction: The Need for Autonomy

Traditional **Supervised Learning** relies on datasets where each instance contains both **features ($\mathbf{X}$)** (input data, e.g., a passenger's age, class, sex) and a corresponding **target ($\mathbf{y}$)** (the ground-truth label, e.g., 'survived' or 'not survived').

However, in many real-world scenarios, the target labels are unavailable, impractical to obtain, or simply nonexistent. This presents the fundamental challenge: **how can a computational system derive meaningful insights, patterns, or intrinsic structures from raw, unlabeled data entirely on its own?**

This autonomy is the domain of **Unsupervised Learning**.

***

## 2. Technical Definition and Core Objective

**Unsupervised Learning (UL)** is a category of Machine Learning algorithms designed to infer a function to describe hidden structure from **unlabeled data** ($\mathbf{X}$) â€” data that has no associated output values ($\mathbf{y}$).

* **Input:** Only the **feature matrix ($\mathbf{X}$)**.
* **Goal:** To model the underlying distribution, density, or latent variables of the data to facilitate:
    1.  **Pattern Discovery:** Identifying recurring relationships or associations.
    2.  **Dimensionality Reduction:** Compressing the feature space while retaining essential information.
    3.  **Clustering:** Grouping similar data points together.

### Core Analogy (Dimensionality/Similarity)

Consider a large, unsorted dataset of fruits. A UL algorithm doesn't know the human-defined labels ('apple', 'watermelon'). Instead, it analyzes the feature space (e.g., color, size, shape, weight) and identifies **natural separations** where intra-group variance is minimized and inter-group variance is maximized. This process defines the intrinsic structure of the data.

***

## 3. The Rationale: Why Unlabeled Data Persists

The prevalence of Unsupervised Learning is driven by the significant practical constraints associated with data labeling (or **annotation**):

| Constraint | Description | Example |
| :--- | :--- | :--- |
| **Cost & Scale** | Manual labeling of vast datasets (Big Data) is prohibitively expensive and time-consuming, requiring extensive human effort. | Annotating millions of high-resolution images or hours of streaming audio. |
| **Expert Dependency** | Specialized domains require highly skilled subject-matter experts for accurate annotation, which are scarce resources. | Labeling rare astronomical events or complex genetic sequences. |
| **Epistemological Gap** | In exploratory data analysis (e.g., frontier science), the ground truth is **unknown**. If the underlying phenomenon is not yet understood, no meaningful label can be assigned. | Identifying novel cell types or emergent patterns in high-dimensional genomic data. |

UL algorithms bypass this dependency, providing the initial step to structure raw data, which can then potentially be used in a semi-supervised or domain-specific application.

***

## 4. Key Use Cases and Applications

Unsupervised learning excels in tasks aimed at simplification, segmentation, and novelty detection:

### A. Clustering (Segmentation)

The process of grouping a set of data objects into subsets (clusters) so that objects within the same cluster are more similar to each other than to those in other clusters.

* **Application:** **Customer Segmentation** (e.g., *k*-Means, DBSCAN) where a business wants to identify inherent market segments (e.g., "High-Value Spenders," "Occasional Browsers") without pre-defining the categories.

### B. Dimensionality Reduction

Reducing the number of random variables under consideration by obtaining a set of principal variables. This combats the **curse of dimensionality** and aids visualization.

* **Application:** **Feature Extraction** (e.g., Principal Component Analysis **PCA**, t-distributed Stochastic Neighbor Embedding **t-SNE**). Used to simplify complex genomic or image datasets for faster processing and better model interpretability.

### C. Association Rule Mining

Discovering interesting relationships between variables in large databases.

* **Application:** **Market Basket Analysis** (e.g., Apriori algorithm). Identifying products frequently purchased together (e.g., *if customer buys A and B, they are likely to buy C*).

# Real-World Applications of Unsupervised Learning (Clustering, Association, and Reduction)

| Application Area (Emoji) | Core Unsupervised Technique | Detailed Real-World Example / Benefit |
| :--- | :--- | :--- |
| **Marketing & E-commerce** ğŸ›ï¸ | **Clustering** (K-Means, Hierarchical) | **Customer Segmentation:** Automatically groups customers into distinct segments (e.g., "High Spenders," "Discount Seekers") based on their purchasing history, demographics, and web behavior. *Benefit:* Allows businesses to tailor personalized marketing campaigns and product offerings, significantly boosting conversion rates and customer loyalty. |
| **Finance & Security** ğŸ’° | **Anomaly Detection** (Isolation Forest, DBSCAN) | **Fraud and Intrusion Detection:** Identifies rare, unusual data points or patterns that deviate significantly from the established 'normal' behavior within financial transactions or network traffic logs. *Benefit:* Crucial for flagging potential credit card fraud, money laundering activities, or unauthorized network access in real-time. |
| **Media & Content** ğŸ“º | **Clustering** & **Association** | **Recommendation Systems:** Groups users with similar consumption habits (e.g., movie genres watched, products bought) to provide highly relevant suggestions. **Matrix Factorization** (a form of dimensionality reduction) is often used to discover latent features. *Example:* Netflix suggesting new titles based on similar viewers' preferences. |
| **Retail & Inventory** ğŸ›’ | **Association Rule Mining** (Apriori) | **Market Basket Analysis (MBA):** Discovers strong relationships and "if-then" rules between items frequently purchased together in large transaction datasets. *Benefit:* Optimizes store layout, product bundling, and cross-selling strategies (e.g., placing bread next to butter). |
| **Data Preprocessing** ğŸ“Š | **Dimensionality Reduction** (PCA, t-SNE) | **Data Simplification & Visualization:** Reduces the number of variables (features) in high-dimensional datasets while preserving their most critical variance. *Benefit:* Eliminates redundant data, speeds up subsequent supervised learning models, and makes complex data understandable for human visualization (e.g., plotting high-dimensional clusters on a 2D map). |
| **Natural Language Processing** ğŸ“° | **Topic Modeling** (LDA) & **Clustering** | **Document Organization and Thematic Grouping:** Automatically categorizes massive, unstructured text collections (e.g., news articles, research papers) into overarching themes or topics without needing predefined tags. *Example:* Google News aggregating related stories from thousands of sources under a single category. |
| **Healthcare & Genomics** ğŸ§¬ | **Clustering** & **Dimensionality Reduction** | **Patient Subgroup Identification:** Analyzes complex genetic, proteomic, or clinical data to identify natural, previously unknown subgroups of patients with similar disease characteristics. *Benefit:* Aids in discovering new disease subtypes and moving toward personalized medicine and targeted drug development. |


# Why Unsupervised Learning Matters (Denetimsiz Ã–ÄŸrenme Neden Ã–nemlidir) ğŸ§ 

## Why Do We Need Unsupervised Learning?

Unsupervised learning is essential because it allows us to make sense of **data without labels**. It provides a powerful set of tools when human intervention for labeling is impractical or impossible. Here are some key reasons why it matters:

### 1. Exploratory Analysis âœ¨

When we donâ€™t know much about a dataset, unsupervised learning helps us discover hidden **relationships, patterns, or groups**. It acts as a guide in the early stages of data analysis.

* **Example:** Finding natural **clusters** of customers in sales data, revealing market segments you didn't know existed.

### 2. Handling Massive, Unlabeled Data ğŸ’¾

Most of the data generated todayâ€”social media posts, healthcare records, financial transactionsâ€”is **unlabeled** and far too large to tag manually.

* Unsupervised algorithms can organize and simplify these overwhelming datasets through techniques like **Dimensionality Reduction** (e.g., PCA) and **Clustering**, making them manageable for further analysis.

### 3. Unlocking Hidden Insights ğŸ”“

Sometimes the most valuable findings are the ones we didnâ€™t expect. Unsupervised learning can uncover **anomalies, trends, and structures** we didnâ€™t even know to look for, leading to genuine discovery (e.g., detecting novel fraud patterns).

---

## Challenges in Unsupervised Learning âš ï¸

While powerful, unsupervised learning isnâ€™t perfect. Because the data has no ground truth, it presents unique challenges:

| Challenge (Emoji) | Explanation |
| :--- | :--- |
| **Interpreting the Results** ğŸ¤” | Without labels, itâ€™s up to humans (domain experts) to decide whether the discovered clusters, components, or patterns are actually meaningful and actionable in a business or scientific context. |
| **No Clear Evaluation Metric** ğŸ“‰ | Unlike supervised learning (where we use accuracy or error rates), thereâ€™s no single **ground truth** to compare against. Evaluation often relies on subjective measures or internal metrics like silhouette scores. |
| **Ambiguity** ğŸ¤· | Different algorithms (or even different runs of the same stochastic algorithm) may produce slightly different groupings. The output is often less deterministic than in supervised models. |
| **Computational Complexity** âš™ï¸ | Especially in high-dimensional space, algorithms like Hierarchical Clustering can be very **resource-intensive** to process, making them slow for massive datasets. |

---

## Summary ğŸ‰

* **Unsupervised learning** = **no labels** ($y$ is missing).
* Itâ€™s about finding **patterns, groups, or structures** in data.
* Very useful in the real world when labels are **unavailable or too costly** to acquire.



# Key Differences: Supervised vs. Unsupervised Learning

In the previous lesson, we introduced the concept of **Unsupervised Learning** ($U$) where models operate on **unlabeled data** ($X$) to extract inherent patterns. Now, let's establish a technical comparison with **Supervised Learning** ($S$), which you are already familiar with.

---

## 1. Data Type: Labeled vs. Unlabeled ğŸ·ï¸

The most fundamental distinction between $S$ and $U$ learning lies in the nature of the training data:

| Feature | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Data Structure** | Requires **labeled data** ($X, y$), where each input vector $X_i$ has a corresponding output label or value $y_i$. | Operates solely on **unlabeled input data** ($X$), where the target variable $y$ is unknown. |
| **Model Function** | Learns a mapping function $f: X \rightarrow Y$. | Aims to model the underlying structure or distribution of the data $P(X)$. |

Let's visualize the raw data difference:

| Supervised Data (Classification) | Unsupervised Data (Clustering) |
| :---: | :---: |
| Data points are distinctly color-coded, representing known classes (e.g., Cat vs. Dog).  | All data points are uniformly colored, challenging the model to discover separation.  |

---

## 2. Goal: Prediction vs. Discovery ğŸ¯

The end objective dictates the choice of the learning paradigm:

| Goal Type | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Primary Aim** | **Prediction** or **Estimation**. The goal is to accurately predict known outcomes ($y$) for new, unseen inputs ($X$). | **Discovery** or **Inference**. The goal is to find hidden structures, groupings, or representations within the data. |
| **Metric Focus** | Minimizing prediction error (e.g., $L_2$ loss for Regression). | Optimizing a measure of data structure quality (e.g., cluster compactness/separation). |

Here's an illustrative example contrasting the goals:

| Supervised Goal: House Price Prediction (Regression) | Unsupervised Goal: Customer Clustering |
| :---: | :---: |
| A red line shows the learned linear relationship between house size ($X$) and predicted price ($y$) based on labeled data.  | Data points are grouped into distinct segments (colors), reflecting underlying customer patterns without prior labels.  |

---

## 3. Problem Types and Applications ğŸ› ï¸

The absence or presence of a target variable $y$ inherently limits the types of problems each approach can address:

| Problem Type | Supervised Learning ($S$) | Unsupervised Learning ($U$) |
| :--- | :--- | :--- |
| **Common Tasks** | **Classification** (predicting discrete labels) and **Regression** (predicting continuous values). | **Clustering** (grouping similar data points) and **Dimensionality Reduction** (simplifying feature space). |
| **Examples** | Spam Detection, Image Recognition, Stock Price Forecasting. | Customer Segmentation, Anomaly Detection, Feature Extraction. |

A visual contrast of the problem output:

| Supervised Task: Classification | Unsupervised Task: Dimensionality Reduction & Clustering |
| :---: | :---: |
| A decision boundary separates data points into two distinct, predefined categories (e.g., Spam vs. Non-Spam).  | Data is reduced (e.g., using PCA) and grouped into natural, emergent clusters.  |

---

## 4. Model Complexity and Evaluation ğŸ“ˆ

Model architecture and validation methods also differ significantly:

### Model Complexity and Architecture

* **Supervised Learning:** Often employs more **complex models** (e.g., Deep Neural Networks, Random Forests, Support Vector Machines) designed for precise, high-dimensional **mapping** and robust generalization.
* **Unsupervised Learning:** Typically relies on **simpler algorithms** (e.g., K-Means, PCA, Autoencoders) focused on finding an optimal internal **representation** or latent structure of the data.

### Evaluation Metrics

* **Supervised Learning:** Performance is evaluated against the **ground truth ($y$)** using objective external metrics: **Accuracy, Precision, Recall, F1 Score** (for classification), or **RMSE** (Root Mean Square Error, for regression).
* **Unsupervised Learning:** Due to the lack of $y$, evaluation relies on **intrinsic metrics** that measure the quality of the discovered structure: **Silhouette Score** (cluster separation and compactness), **Daviesâ€“Bouldin Index**, or Reconstruction Error (for dimensionality reduction). Interpretation often requires **human domain expertise** to validate the utility of the discovered patterns.




---
# TÃœRKCE:

# Denetimsiz Ã–ÄŸrenme Nedir? (Unsupervised Learning)

## 1. GiriÅŸ ve BaÄŸlam

Åu ana kadar, verinin hem **Ã–zniteliklere** ($X$) hem de **Hedefe** ($y$) sahip olduÄŸu **denetimli Ã¶ÄŸrenme** (supervised learning) konseptiyle Ã§alÄ±ÅŸtÄ±k.

* **Ã–znitelikler ($X$)**: GiriÅŸ verisi (Ã¶rn. Titanic veri setindeki yolcunun yaÅŸÄ±, sÄ±nÄ±fÄ±, cinsiyeti).
* **Hedef ($y$)**: Tahmin etmek istediÄŸimiz Ã§Ä±ktÄ± (Ã¶rn. hayatta kalÄ±p kalmadÄ±ÄŸÄ±).

Peki ya elimizde etiketler yoksa? Ya sadece ham verimiz varsa ve bilgisayarÄ±n **kendi baÅŸÄ±na** Ã¶rÃ¼ntÃ¼leri, yapÄ±larÄ± veya gruplarÄ± keÅŸfetmesini istiyorsak?

Ä°ÅŸte tam bu noktada **Denetimsiz Ã–ÄŸrenme** devreye girer.

***

## 2. Teknik TanÄ±m

**Denetimsiz Ã¶ÄŸrenme**, yalnÄ±zca **Ã¶zniteliklere** ($X$) sahip olduÄŸumuz, ancak **etiketlerin** ($y$) bulunmadÄ±ÄŸÄ± bir makine Ã¶ÄŸrenimi tÃ¼rÃ¼dÃ¼r.

Temel amaÃ§, bilinen bir sonucu tahmin etmek deÄŸil, veri iÃ§erisindeki **gizli Ã¶rÃ¼ntÃ¼leri**, **gruplandÄ±rmalarÄ±** (kÃ¼melemeyi) veya **iÃ§sel yapÄ±larÄ±** ortaya Ã§Ä±karmaktÄ±r.

Bu, ÅŸu soruyu sormak gibidir:
ğŸ‘‰ "Bilgisayar, benim ne aradÄ±ÄŸÄ±mÄ± sÃ¶ylememe gerek kalmadan bu kaostaki bir dÃ¼zeni bulabilir mi?"

### Ã–rnek Senaryo: Meyve AyÄ±rma
Bir sepette hem karpuz hem de elma olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n, ancak bunlar etiketlenmemiÅŸ. Denetimsiz Ã¶ÄŸrenme algoritmasÄ±, tÃ¼m karpuzlar ve tÃ¼m elmalar arasÄ±nda ortak olanÄ± bulmanÄ±n bir yolunu bulacaktÄ±r. Bu, boyut, renk, ÅŸekil ve diÄŸer ayÄ±rt edici niteliklere dayanabilir. Algoritma, bu niteliklere gÃ¶re verileri **kÃ¼melendirerek** ayÄ±rma iÅŸlemini gerÃ§ekleÅŸtirecektir.

### Ã–rnek Senaryo: KÃ¼tÃ¼phane Organizasyonu
Bir kÃ¼tÃ¼phanedeki bÃ¼yÃ¼k bir kitap yÄ±ÄŸÄ±nÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼n; kitaplarda kategori veya yazar etiketi yok. Denetimsiz bir Ã¶ÄŸrenme algoritmasÄ±, kaosu dÃ¼zene sokmak iÃ§in kapak rengi, sayfa uzunluÄŸu veya hatta kullanÄ±lan kaÄŸÄ±t tÃ¼rÃ¼ne gÃ¶re kitaplarÄ± gruplandÄ±rarak baÅŸlayabilir.

***

## 3. Neden Veriyi Etiketlemiyoruz?

Denetimsiz Ã¶ÄŸrenme algoritmalarÄ±, insan tarafÄ±ndan saÄŸlanmÄ±ÅŸ etiketleri veya etiketlemeyi kullanmaz. Peki bu neden Ã¶nemlidir?

Bunun en bÃ¼yÃ¼k nedeni, veriyi **etiketlemenin** genellikle **zor, pahalÄ±** veya hatta **imkansÄ±z** olmasÄ±dÄ±r.

### Etiketleme (Veri AÃ§Ä±klamasÄ± - Data Annotation) Nedir?
Makine Ã¶ÄŸreniminde, veri etiketleme, ham verinin (tablodaki satÄ±rlar, gÃ¶rseller, ses kayÄ±tlarÄ± vb.) alÄ±nÄ±p buna bir veya daha fazla etiket atanmasÄ± anlamÄ±na gelir. Bu etiketler baÄŸlam ve anlam saÄŸlayarak makine Ã¶ÄŸrenimi modelinin Ã¶ÄŸrenmesini saÄŸlar.

### Zorluklar
1.  **Maliyet ve Zaman:** TÄ±bbi araÅŸtÄ±rma gibi alanlarda o kadar Ã§ok veri vardÄ±r ki, her ÅŸeyi etiketlemek yÄ±llar sÃ¼rebilir ve uzmanlardan muazzam bir Ã§aba gerektirir.
2.  **Bilgi EksikliÄŸi:** Genetik veya astronomi gibi alanlarda, bazen ne aradÄ±ÄŸÄ±mÄ±zÄ± bile henÃ¼z bilmiyoruz. EÄŸer fenomeni anlamÄ±yorsak, onu nasÄ±l etiketleyebiliriz?

Ä°ÅŸte denetimsiz Ã¶ÄŸrenme bu noktada parlar: Ham, etiketsiz verideki **yapÄ±yÄ± keÅŸfetmemize** yardÄ±mcÄ± olur.

### Uygulama AlanlarÄ±
| Uygulama | AÃ§Ä±klama | Denetimsiz YÃ¶ntem |
| :--- | :--- | :--- |
| **MÃ¼ÅŸteri Segmentasyonu** | Bir ÅŸirketin milyonlarca satÄ±n alma kaydÄ± olabilir ancak mÃ¼ÅŸteri tipleri iÃ§in net etiketleri yoktur. Algoritma, insanlarÄ± "bÃ¼tÃ§e odaklÄ±" veya "sadÄ±k mÃ¼ÅŸteriler" olarak manuel etiketlemek yerine, verideki doÄŸal gruplandÄ±rmalarÄ± bulur. | **KÃ¼meleme (Clustering)** |
| **DoÄŸal Dil Ä°ÅŸleme** | TemalarÄ± bilinmeyen bÃ¼yÃ¼k bir metin koleksiyonunu analiz etmek. Denetimsiz Ã¶ÄŸrenme, Ã¶nceden tanÄ±mlanmÄ±ÅŸ kategorilere ihtiyaÃ§ duymadan belgeleri konularÄ±na, kelime Ã¶rÃ¼ntÃ¼lerine veya yazÄ±m stillerine gÃ¶re gruplandÄ±rabilir. | **Konu Modellemesi (Topic Modeling)** |
| **Boyut Azaltma** | Ã‡ok fazla Ã¶zniteliÄŸe sahip verinin (Ã¶rneÄŸin binlerce) temel, daha az sayÄ±da deÄŸiÅŸkene indirgenmesi. | **Temel BileÅŸen Analizi (PCA)** |


# ğŸŒ GerÃ§ek DÃ¼nya Denetimsiz Ã–ÄŸrenme Ã–rnekleri (Real-World Examples of Unsupervised Learning)

Denetimsiz Ã¶ÄŸrenme, etiketsiz veriden doÄŸal yapÄ±larÄ±, Ã¶rÃ¼ntÃ¼leri ve iliÅŸkileri keÅŸfetme yeteneÄŸi sayesinde birÃ§ok sektÃ¶rde kritik rol oynamaktadÄ±r. Ä°ÅŸte en yaygÄ±n ve etkili uygulamalardan bazÄ±larÄ±:

| Alan (Domain) | Uygulama AdÄ± ve AmacÄ± | Temel Denetimsiz Teknikler | NasÄ±l Ã‡alÄ±ÅŸÄ±r (Denetimsiz DoÄŸa) |
| :--- | :--- | :--- | :--- |
| **Pazarlama ve E-Ticaret** ğŸ›ï¸ | **MÃ¼ÅŸteri Segmentasyonu (Customer Segmentation)**: MÃ¼ÅŸterileri ortak Ã¶zelliklerine (satÄ±n alma geÃ§miÅŸi, demografi, gezinme davranÄ±ÅŸÄ±) gÃ¶re gruplamak. | **KÃ¼meleme (Clustering)**: K-Means, DBSCAN, HiyerarÅŸik KÃ¼meleme. | Algoritma, hangi mÃ¼ÅŸteri gruplarÄ±nÄ±n var olduÄŸunu *Ã¶nceden bilmez*. Veriyi analiz ederek doÄŸal olarak benzer davranÄ±ÅŸ gÃ¶steren gruplarÄ± **otomatik olarak keÅŸfeder**. |
| **Finans ve Siber GÃ¼venlik** ğŸ›¡ï¸ | **Anormallik/SahtekarlÄ±k Tespiti (Anomaly/Fraud Detection)**: Normal Ã¶rÃ¼ntÃ¼den belirgin ÅŸekilde sapan sÄ±ra dÄ±ÅŸÄ± veri noktalarÄ±nÄ± (iÅŸlemler, aÄŸ trafiÄŸi) belirlemek. | **KÃ¼meleme (Clustering)**: DBSCAN; **Boyut Azaltma (Dimensionality Reduction)**: Autoencoders, Isolation Forest. | Algoritma, verideki 'normal' davranÄ±ÅŸ modelini Ã¶ÄŸrenir. Yeni bir iÅŸlem bu normal modelin **dÄ±ÅŸÄ±na Ã§Ä±ktÄ±ÄŸÄ±nda** onu anomali olarak iÅŸaretler, yani anormali etiketleyen bir **eÄŸitmen yoktur**. |
| **Perakende ve Lojistik** ğŸ›’ | **Pazar Sepeti Analizi (Market Basket Analysis)**: MÃ¼ÅŸterilerin birlikte satÄ±n alma olasÄ±lÄ±ÄŸÄ± en yÃ¼ksek olan Ã¼rÃ¼n gruplarÄ±nÄ± bulmak. | **Birliktelik KuralÄ± MadenciliÄŸi (Association Rule Mining)**: Apriori, Eclat. | Sistem, "X Ã¼rÃ¼nÃ¼ satÄ±n alÄ±ndÄ±ysa, Y Ã¼rÃ¼nÃ¼ de satÄ±n alÄ±nÄ±r" gibi **gizli iliÅŸkileri** bulmak iÃ§in milyonlarca iÅŸlemi tarar. Bu iliÅŸkiler **Ã¶nceden etiketlenmemiÅŸtir**. |
| **Tavsiye Sistemleri** ğŸ¬ | **Ä°Ã§erik/ÃœrÃ¼n Ã–nerileri (Recommendation Systems)**: KullanÄ±cÄ±nÄ±n geÃ§miÅŸ davranÄ±ÅŸÄ±na gÃ¶re diÄŸer kullanÄ±cÄ±larla (Collaborative Filtering) veya Ã¼rÃ¼nlerle (Content-Based Filtering) olan benzerliÄŸini kullanarak Ã¶neri sunmak. | **Boyut Azaltma (Dimensionality Reduction)**: SVD, Matris AyrÄ±ÅŸtÄ±rma (Matrix Factorization). | Model, kullanÄ±cÄ± ve Ã¶ÄŸe etkileÅŸimlerinin karmaÅŸÄ±k yapÄ±sÄ±nÄ± daha dÃ¼ÅŸÃ¼k boyutlu bir gÃ¶sterime indirir. Bu, **etiketsiz etkileÅŸim verisinden** kullanÄ±cÄ± tercihlerini **Ã§Ä±karÄ±r**. |
| **DoÄŸal Dil Ä°ÅŸleme (NLP)** ğŸ“° | **Konu Modellemesi (Topic Modeling)**: BÃ¼yÃ¼k bir metin koleksiyonundaki (haber makaleleri, e-postalar) ana, gizli konularÄ± otomatik olarak Ã§Ä±karmak. | **Konu Modellemesi**: Latent Dirichlet Allocation (LDA), NMF. | Algoritma, belgeleri gruplandÄ±rmak iÃ§in **konu etiketlerine** sahip deÄŸildir. Metinlerdeki kelimelerin **birlikte geÃ§me Ã¶rÃ¼ntÃ¼lerine** bakarak konularÄ± **kendisi tanÄ±mlar**. |
| **GÃ¶rÃ¼ntÃ¼/Video Ä°ÅŸleme** ğŸ–¼ï¸ | **GÃ¶rÃ¼ntÃ¼ SÄ±kÄ±ÅŸtÄ±rma (Image Compression)**: GÃ¶rÃ¼ntÃ¼nÃ¼n kalitesini kaybetmeden boyutunu azaltmak (veri boyutunu dÃ¼ÅŸÃ¼rmek). | **Boyut Azaltma (Dimensionality Reduction)**: Temel BileÅŸen Analizi (PCA). | Model, gÃ¶rÃ¼ntÃ¼deki en fazla bilgiyi taÅŸÄ±yan temel Ã¶znitelikleri (boyutlarÄ±) bularak veriyi sÄ±kÄ±ÅŸtÄ±rÄ±r. Bu, **gÃ¶rÃ¼ntÃ¼ etiketleri olmadan** verinin iÃ§sel yapÄ±sÄ±nÄ± analiz etmeye dayanÄ±r. |
| **Genetik ve TÄ±p** ğŸ§¬ | **Gen Ä°fadesi Verilerinin KÃ¼melenmesi (Gene Expression Data Clustering)**: FarklÄ± genetik bozukluklarÄ± veya hÃ¼cre tiplerini temsil eden gen ifadesi Ã¶rÃ¼ntÃ¼lerini gruplamak. | **KÃ¼meleme (Clustering)**: HiyerarÅŸik KÃ¼meleme, K-Means. | AraÅŸtÄ±rmacÄ±lar, hangi bozukluklarÄ±n var olduÄŸunu **bilmediklerinde** algoritma, veri setinde doÄŸal olarak oluÅŸan genetik alt gruplarÄ± (Ã¶rneÄŸin, yeni bir kanser alt tipi) keÅŸfeder. |

# Denetimsiz Ã–ÄŸrenme Neden Ã–nemlidir? (Why Unsupervised Learning Matters) ğŸ’¡

Denetimsiz Ã¶ÄŸrenme, etiketler olmadan veriyi anlamlandÄ±rmamÄ±za olanak tanÄ±dÄ±ÄŸÄ± iÃ§in hayati Ã¶neme sahiptir. Ä°ÅŸte neden Ã¶nemli olduÄŸuna dair temel nedenler:

## Denetimsiz Ã–ÄŸrenmeye Neden Ä°htiyaÃ§ DuyarÄ±z?

### KeÅŸifsel Analiz (Exploratory Analysis) ğŸ§
Bir veri kÃ¼mesi hakkÄ±nda Ã§ok az ÅŸey bildiÄŸimizde, denetimsiz Ã¶ÄŸrenme **gizli iliÅŸkileri, Ã¶rÃ¼ntÃ¼leri veya gruplarÄ±** keÅŸfetmemize yardÄ±mcÄ± olur.
* **Ã–rnek:** SatÄ±ÅŸ verilerinde mÃ¼ÅŸterilerin doÄŸal kÃ¼melerini bulmak.

### BÃ¼yÃ¼k, Etiketsiz Veriyle BaÅŸa Ã‡Ä±kma ğŸ’ª
Sosyal medya gÃ¶nderileri, saÄŸlÄ±k kayÄ±tlarÄ±, finansal iÅŸlemler gibi verilerin Ã§oÄŸu etiketsizdir ve manuel olarak etiketlemek iÃ§in Ã§ok bÃ¼yÃ¼ktÃ¼r.
* Denetimsiz algoritmalar bu devasa veri kÃ¼melerini dÃ¼zenleyebilir ve basitleÅŸtirebilir.

### Gizli Ä°Ã§gÃ¶rÃ¼lerin Kilidini AÃ§ma ğŸ”‘
Bazen en deÄŸerli bulgular, hiÃ§ beklemediÄŸimiz bulgulardÄ±r.
* Denetimsiz Ã¶ÄŸrenme, aramayÄ± bile bilmediÄŸimiz **anormallikleri, eÄŸilimleri** ve **yapÄ±larÄ±** ortaya Ã§Ä±karabilir.

---

## Denetimsiz Ã–ÄŸrenmedeki Zorluklar (Challenges) ğŸš§

GÃ¼Ã§lÃ¼ olmasÄ±na raÄŸmen, denetimsiz Ã¶ÄŸrenme kusursuz deÄŸildir. Temel zorluklar ÅŸunlardÄ±r:

* **SonuÃ§larÄ± Yorumlama:** Etiketler olmadan, kÃ¼melerin veya Ã¶rÃ¼ntÃ¼lerin gerÃ§ekten anlamlÄ± olup olmadÄ±ÄŸÄ±na karar vermek insanlara kalmÄ±ÅŸtÄ±r.
* **AÃ§Ä±k Bir DeÄŸerlendirme MetriÄŸi Yok:** Denetimli Ã¶ÄŸrenmenin aksine, karÅŸÄ±laÅŸtÄ±rma yapabileceÄŸimiz **temel gerÃ§ek** (ground truth) bulunmaz.
* **Belirsizlik (Ambiguity):** FarklÄ± algoritmalar (veya aynÄ± algoritmanÄ±n farklÄ± Ã§alÄ±ÅŸtÄ±rma sonuÃ§larÄ±) biraz farklÄ± gruplandÄ±rmalar Ã¼retebilir.
* **Hesaplama KarmaÅŸÄ±klÄ±ÄŸÄ± (Computational Complexity):** BÃ¼yÃ¼k veri kÃ¼melerini iÅŸlemek Ã§ok fazla kaynak gerektirebilir.

---

## Ã–zet ğŸ‰

* **Denetimsiz Ã–ÄŸrenme = Etiket Yok.**
* AmaÃ§, verideki **Ã¶rÃ¼ntÃ¼leri, gruplarÄ± veya yapÄ±larÄ±** bul

# Denetimli Ã–ÄŸrenmeden Temel Farklar (Key Differences From Supervised Learning) âš–ï¸

Ã–nceki derste, modellerin etiketlenmemiÅŸ veri Ã¼zerinde Ã§alÄ±ÅŸarak anlamlÄ± Ã¶rÃ¼ntÃ¼ler Ã§Ä±kardÄ±ÄŸÄ± **Denetimsiz Ã–ÄŸrenme** kavramÄ±nÄ± tanÄ±ttÄ±k. Åimdi, bu yaklaÅŸÄ±mÄ±n daha Ã¶nce Ã¶ÄŸrendiÄŸiniz **Denetimli Ã–ÄŸrenme** ile nasÄ±l karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nÄ± inceleyelim.

---

## 1. Veri Tipi: Etiketli vs. Etiketsiz Veri

Denetimli ve denetimsiz Ã¶ÄŸrenme arasÄ±ndaki en Ã¶nemli fark, Ã§alÄ±ÅŸtÄ±klarÄ± **veri tÃ¼rÃ¼dÃ¼r**.

| Ã–zellik | Denetimli Ã–ÄŸrenme (Supervised) ğŸ¯ | Denetimsiz Ã–ÄŸrenme (Unsupervised) ğŸ” |
| :--- | :--- | :--- |
| **Gereken Veri** | **Etiketli Veri** (Labeled Data) | **Etiketsiz Veri** (Unlabeled Data) |
| **Veri NoktasÄ± YapÄ±sÄ±** | Her veri noktasÄ±, tahmin edilecek karÅŸÄ±lÄ±k gelen bir **Ã§Ä±ktÄ±/hedef** ($y$) iÃ§erir. | Model, Ã¶nceden "doÄŸru" cevabÄ± bilmeden Ã¶rÃ¼ntÃ¼ veya yapÄ± bulmalÄ±dÄ±r. |

### GÃ¶rsel KarÅŸÄ±laÅŸtÄ±rma: Veri GÃ¶rÃ¼nÃ¼mÃ¼ ğŸ“Š

| Etiketli Veri (Denetimli) | Etiketsiz Veri (Denetimsiz) |
| :---: | :---: |
| FarklÄ± renklerle ayrÄ±lmÄ±ÅŸ "kedi" ve "kÃ¶pek" gibi sÄ±nÄ±flarÄ±n olduÄŸu veri noktalarÄ±. Bu, modelin belirli kategorilerden Ã¶ÄŸrenmesine olanak tanÄ±r. | TÃ¼m noktalarÄ±n aynÄ± renkte olduÄŸu veri noktalarÄ±. Bu, denetimsiz modellerin etiketler olmadan Ã¶rÃ¼ntÃ¼ keÅŸfetme zorluÄŸunu temsil eder. |
|  |  |

---

## 2. Nihai Hedef: Tahmin vs. KeÅŸif

Dikkate alÄ±nmasÄ± gereken bir diÄŸer Ã¶nemli husus da, elimizdeki verilere gÃ¶re ulaÅŸmak istediÄŸimiz **nihai hedeftir**.

| Hedef | Denetimli Ã–ÄŸrenme (Prediction) â¡ï¸ | Denetimsiz Ã–ÄŸrenme (Discovery) ğŸ—ºï¸ |
| :--- | :--- | :--- |
| **Temel AmaÃ§** | Etiketli verilere dayanarak **Ã§Ä±ktÄ±larÄ± tahmin etmektir**. Model, girdi ($X$) ile bilinen Ã§Ä±ktÄ± ($y$) arasÄ±ndaki **eÅŸlemeyi (mapping)** Ã¶ÄŸrenir. | Veri iÃ§indeki **gizli yapÄ±larÄ±** keÅŸfetmektir (Ã¶rn. kÃ¼meleri bulmak veya boyut sayÄ±sÄ±nÄ± azaltmak). |

### GÃ¶rsel Ã–rnek: AmaÃ§ NetliÄŸi

| Denetimli GÃ¶rev: Ev FiyatÄ± Tahmini (Regresyon) | Denetimsiz GÃ¶rev: MÃ¼ÅŸteri KÃ¼melemesi (Clustering) |
| :---: | :---: |
| Sol alt grafik, ev bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ile tahmin edilen fiyat arasÄ±ndaki iliÅŸkiyi gÃ¶steren kÄ±rmÄ±zÄ± bir regresyon doÄŸrusu iÃ§erir. Bu, net bir iÅŸ hedefidir. | SaÄŸ alt grafik, veri noktalarÄ±nÄ±n Ã¶nceden etiket olmadan kÃ¼melere ayrÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir. FarklÄ± renkler, keÅŸfedilen mÃ¼ÅŸteri segmentlerini temsil eder. |
|  |  |

---

## 3. Problem TÃ¼rleri ve Algoritmalar

Her iki yaklaÅŸÄ±mÄ±n ele alabileceÄŸi problemler doÄŸasÄ± gereÄŸi farklÄ±dÄ±r.

* **Denetimli Ã–ÄŸrenme:** Genellikle **SÄ±nÄ±flandÄ±rma** (Classification) ve **Regresyon** (Regression) problemlerine uygulanÄ±r. Hedef, belirli etiketleri veya deÄŸerleri tahmin etmektir (Ã¶rn. Spam/Spam DeÄŸil).
* **Denetimsiz Ã–ÄŸrenme:** **KÃ¼meleme** (Clustering), **Boyut Azaltma** (Dimensionality Reduction) ve **Birliktelik KuralÄ± MadenciliÄŸi** (Association Rule Mining) gibi problem tÃ¼rleri iÃ§in kullanÄ±lÄ±r.

### GÃ¶rsel Ä°llÃ¼strasyon: GÃ¶rev FarklÄ±lÄ±klarÄ±

| Denetimli GÃ¶rev: SÄ±nÄ±flandÄ±rma | Denetimsiz GÃ¶rev: KÃ¼meleme ve Boyut Azaltma |
| :---: | :---: |
| Veri noktalarÄ± iki ayrÄ± kategoriye (Ã¶rn. spam vs. spam olmayan e-postalar) ayrÄ±lÄ±r. | Verinin boyutunu azaltmak ve noktalarÄ± farklÄ± kÃ¼melere ayÄ±rmak iÃ§in **Temel BileÅŸen Analizi (PCA)** kullanÄ±larak kÃ¼meleme gÃ¶revi gÃ¶sterilir. |
|  |  |

---

## 4. Model KarmaÅŸÄ±klÄ±ÄŸÄ± ve DeÄŸerlendirme

### Model KarmaÅŸÄ±klÄ±ÄŸÄ±

| Ã–zellik | Denetimli Ã–ÄŸrenme (Supervised) ğŸ§  | Denetimsiz Ã–ÄŸrenme (Unsupervised) ğŸ§© |
| :--- | :--- | :--- |
| **Tipik Modeller** | Girdi ile Ã§Ä±ktÄ± arasÄ±nda kesin eÅŸlemeler Ã¶ÄŸrenmesi gereken **Derin Sinir AÄŸlarÄ±** (Neural Networks) veya **Rastgele Ormanlar** (Random Forests) gibi daha **karmaÅŸÄ±k** modeller iÃ§erir. | AmaÃ§ Ã¶rÃ¼ntÃ¼ keÅŸfi olduÄŸu iÃ§in genellikle **K-Ortalamalar** (K-means) veya **PCA** gibi daha **basit** modeller kullanÄ±lÄ±r. |

### DeÄŸerlendirme Metrikleri (Evaluation Metrics)

Modelimizin ne kadar iyi performans gÃ¶sterdiÄŸini bilmek iÃ§in kullandÄ±ÄŸÄ±mÄ±z metrikler de farklÄ±dÄ±r:

| Ã–zellik | Denetimli Ã–ÄŸrenme (Supervised) ğŸ’¯ | Denetimsiz Ã–ÄŸrenme (Unsupervised) â“ |
| :--- | :--- | :--- |
| **Metrikler** | Performans; **DoÄŸruluk** (Accuracy), **Hassasiyet** (Precision), **Geri Ã‡aÄŸÄ±rma** (Recall) veya **RMSE** (Regresyon iÃ§in Hata Karelerinin KarekÃ¶kÃ¼) gibi **temel gerÃ§eÄŸe** dayalÄ± metriklerle deÄŸerlendirilir. | Ã–nceden tanÄ±mlanmÄ±ÅŸ etiketler olmadÄ±ÄŸÄ± iÃ§in **Siluet Skoru** (Silhouette Score - kÃ¼melerin ne kadar iyi ayrÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir) gibi **iÃ§sel (intrinsic)** metrikler kullanÄ±lÄ±r. | 

# Denetimli ve Denetimsiz Ã–ÄŸrenme KarÅŸÄ±laÅŸtÄ±rmasÄ± ğŸ†š

Bu tablo, makine Ã¶ÄŸreniminin iki ana paradigmasÄ± olan Denetimli ve Denetimsiz Ã–ÄŸrenme arasÄ±ndaki temel teknik farklÄ±lÄ±klarÄ± detaylÄ± olarak gÃ¶stermektedir.

| KarÅŸÄ±laÅŸtÄ±rma Kriteri | Denetimli Ã–ÄŸrenme (Supervised Learning) ğŸ¯ | Denetimsiz Ã–ÄŸrenme (Unsupervised Learning) ğŸ” |
| :--- | :--- | :--- |
| **Girdi Verisi (Input Data)** | **Etiketli Veri** (Labeled Data). Veri kÃ¼mesi, girdi ($X$) ve karÅŸÄ±lÄ±k gelen Ã§Ä±ktÄ± ($y$) hedefleri iÃ§erir. | **Etiketsiz Veri** (Unlabeled Data). Veri kÃ¼mesi yalnÄ±zca girdi Ã¶zniteliklerini ($X$) iÃ§erir; Ã§Ä±ktÄ± hedefi ($y$) bilinmez. |
| **Temel AmaÃ§ (Goal)** | Girdi ile Ã§Ä±ktÄ± arasÄ±ndaki eÅŸlemeyi (mapping) Ã¶ÄŸrenerek **Ã§Ä±ktÄ±yÄ± tahmin etmek** (Prediction). | Veri iÃ§indeki **gizli yapÄ±larÄ±, Ã¶rÃ¼ntÃ¼leri veya gruplarÄ± keÅŸfetmek** (Discovery). |
| **Ã–ÄŸrenme SÃ¼reci** | Model, etiketleri bir **Ã¶ÄŸretmen** (insan uzmanÄ± veya Ã¶nceden belirlenmiÅŸ etiket) gibi kullanarak girdi-Ã§Ä±ktÄ± iliÅŸkisini Ã¶ÄŸrenir. | Model, herhangi bir rehberlik olmadan verinin iÃ§sel Ã¶zelliklerine (benzerlik, farklÄ±lÄ±k vb.) dayanarak kendi kendine Ã¶ÄŸrenir. |
| **Ortak Problemler** | **SÄ±nÄ±flandÄ±rma** (Classification: ikili/Ã§ok sÄ±nÄ±flÄ±) ve **Regresyon** (Regression: sÃ¼rekli deÄŸer tahmini). | **KÃ¼meleme** (Clustering), **Boyut Azaltma** (Dimensionality Reduction), **Birliktelik KuralÄ± MadenciliÄŸi** (Association Rule Mining). |
| **YaygÄ±n Algoritmalar** | Lineer Regresyon, Lojistik Regresyon, Destek VektÃ¶r Makineleri (SVM), Karar AÄŸaÃ§larÄ±, Rastgele Ormanlar, Sinir AÄŸlarÄ±. | K-Ortalamalar (K-Means), DBSCAN, HiyerarÅŸik KÃ¼meleme, Temel BileÅŸen Analizi (PCA), Otomatik KodlayÄ±cÄ±lar (Autoencoders), Birliktelik KuralÄ± (Apriori). |
| **Modelin Ã‡Ä±ktÄ±sÄ±** | Belirli bir **etiket** (kategori) veya **sayÄ±sal deÄŸer** (tahmin). | **KÃ¼me ID'leri**, **indirgenmiÅŸ boyutlu Ã¶zellikler** veya **birliktelik kurallarÄ±**. |
| **DeÄŸerlendirme Metrikleri** | **Temel GerÃ§eÄŸe** (Ground Truth) dayalÄ±dÄ±r: DoÄŸruluk (Accuracy), Kesinlik (Precision), Geri Ã‡aÄŸÄ±rma (Recall), F1 Skoru, RMSE (Hata Karelerinin KarekÃ¶kÃ¼). | **Ä°Ã§sel Metrikler** kullanÄ±lÄ±r: Siluet Skoru (Silhouette Score), KÃ¼me Ä°Ã§i Kareler ToplamÄ± (Within-Cluster Sum of Squares - WCSS), Rand Ä°ndeksi. |
| **Veri Maliyeti** | **YÃ¼ksek.** Etiketli veri elde etmek ve korumak zaman alÄ±cÄ± ve pahalÄ±dÄ±r. | **DÃ¼ÅŸÃ¼k.** Etiketsiz ham veri genellikle daha kolay ve ucuza toplanÄ±r. |
| **Risk ve Belirsizlik** | Tahminler genellikle daha **kesin** ve doÄŸrudan iÅŸ hedefleriyle iliÅŸkilidir. | SonuÃ§larÄ±n (kÃ¼melerin) **yorumlanmasÄ±** insan uzmanlÄ±ÄŸÄ± gerektirir ve farklÄ± algoritma Ã§alÄ±ÅŸtÄ±rmalarÄ± belirsiz sonuÃ§lar Ã¼retebilir. |

