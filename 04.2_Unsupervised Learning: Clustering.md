# Unsupervised Learning: Clustering

# Types of Clustering Algorithms 🧩

There are several fundamental approaches to grouping data in unsupervised learning:

### 1. K-Means Clustering ⚙️
* **Principle:** A prototype-based, partitioned clustering technique. It aims to partition $N$ observations into $K$ clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid).
* **Key Feature:** Requires the user to define the number of clusters ($K$) beforehand.

### 2. Hierarchical Clustering 🌳
* **Principle:** Creates a tree-like hierarchy of clusters, known as a dendrogram. It can be **Agglomerative** (starting with single points and merging) or **Divisive** (starting with one big cluster and splitting).
* **Key Feature:** Does not require the number of clusters to be specified initially; the number of clusters is chosen by cutting the dendrogram at a desired level.

### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 💡
* **Principle:** Clusters are defined as areas of higher density than the remainder of the dataset. It can find arbitrarily shaped clusters and is robust to noise (outliers).
* **Key Feature:** Does not require a fixed number of clusters and is excellent at identifying outliers (points that do not belong to any cluster).


----

# TÜRKCE

# Kümeleme Algoritması Türleri 🧩

Kümeleme (Clustering) yapmak için kullanılan birkaç temel yaklaşım vardır:

### 🌟 K-Ortalamalar Kümeleme (K-Means Clustering)
* **Temel Yapı:** Veriyi, önceden belirlenen $k$ (küme sayısı) adet merkeze (centroid) göre gruplandırır. Her veri noktası, kendisine en yakın olan merkezin kümesine atanır.
* **Kullanım:** Hızlı ve basit olması nedeniyle büyük veri setlerinde yaygın olarak kullanılır.

### 🌳 Hiyerarşik Kümeleme (Hierarchical Clustering)
* **Temel Yapı:** Kümeleme hiyerarşik bir ağaç yapısı (dendrogram) oluşturarak yapılır. İki ana yaklaşımı vardır: birleştirici (agglomerative - alttan yukarı) veya bölücü (divisive - üstten aşağı).
* **Kullanım:** Küme sayısının önceden bilinmediği ve küme hiyerarşisinin önemli olduğu durumlar için idealdir.

### 📍 Gürültülü Uygulamalar için Yoğunluk Tabanlı Mekansal Kümeleme (DBSCAN - Density-Based Spatial Clustering of Applications with Noise)
* **Temel Yapı:** Veri noktalarını, yüksek yoğunluklu bölgeler oluşturarak gruplandırır. Düşük yoğunluklu bölgelerdeki noktaları "gürültü" veya "aykırı değer" (outlier) olarak işaretler.
* **Kullanım:** Küme şekillerinin düzensiz olduğu ve gürültülü verilerin bulunduğu durumlar için çok etkilidir.



# Kümeleme Nedir? (Clustering) 🧩

Kümeleme, denetimsiz öğrenmedeki en önemli tekniklerden biridir.

Temel olarak, benzer veri noktalarını gruplara ayırmakla ilgilidir — bu grupların (etiketlerin) ne olduğu size **söylenmeden**.

---

## Kümeleme Nedir?

👉 Bir kutunun üzerinde resmi olmayan bir yapboz yığınına sahip olduğunuzu hayal edin.

Kümeleme, parçaları otomatik olarak benzer görünen gruplara ayırmaya benzer, böylece onları anlamlandırabilirsiniz.

* Hiçbir etiket verilmez — algoritma grupları **kendi başına** keşfeder.
* Aynı küme içindeki veri noktaları, diğer kümelerdeki noktalara göre birbirine **daha fazla benzerdir**.

---

## Kümeleme, Sınıflandırmadan Nasıl Farklıdır? 🤔

<img width="761" height="379" alt="image" src="https://github.com/user-attachments/assets/9297ce01-d3b7-4e3c-89e3-dfc6722fe01a" />

[Picture](https://techdifferences.com/difference-between-classification-and-clustering.html)

İlk bakışta, kümeleme, sınıflandırmaya benzer gelebilir; her ikisi de veriyi gruplara ayırmayı içerir. Ancak:

* **Sınıflandırma:** Grupları **önceden biliriz** ve modelin onları tahmin etmesini isteriz (Denetimli Öğrenme).
* **Kümeleme:** Grupları **bilmeyiz** ve algoritmanın onları keşfetmesini isteriz (Denetimsiz Öğrenme).

### Örnek 1: Grup Keşfi vs. Tahmin 🐕🦈

| Kümeleme (Keşif) 🗺️ | Sınıflandırma (Tahmin) 🎯 |
| :--- | :--- |
| Algoritma etiketleri (kedi, köpek, balık, köpek balığı) bilmez. Sadece benzer şeyleri özelliklerine göre gruplar. | Burada etiketleri zaten biliyoruz ("Kedi = Sınıf 1", "Köpek = Sınıf 2", vb.). Model bu etiketlerden öğrenir ve yeni veri için doğru olanı tahmin eder. |
| 👉 **Örnek:** Kediyi ve köpeği bir araya (belki ikisi de karada yürüyor), balığı ve köpek balığını bir araya (ikisi de yüzüyor) yerleştirir. | 👉 **Örnek:** Modele yeni bir köpek balığı gösterirsek, onu "Sınıf 4: Köpek Balığı" olarak atayacaktır. |

### Örnek 2: Sınır Keşfi vs. Sınır Öğrenimi 🔴🔵

| Kümeleme (Veri Benzerliğine Odaklanma) ✨ | Sınıflandırma (Sınır Çizgisine Odaklanma) 📏 |
| :--- | :--- |
| Etiket yoktur. Algoritma, yalnızca **benzerliğe** dayanarak yakındaki noktaları kümelere (Küme 1, Küme 2) ayırır. | Gruplar zaten etiketlenmiştir (Sınıf 1, Sınıf 2). Modelin görevi, sınıfları ayıran bir **sınır çizgisi** öğrenmektir. |
| 👉 **Örnek:** Yalnızca bazı noktaların birbirine yakın olduğunu "fark eder" ve onları tek bir grup olarak ele alır. | 👉 **Örnek:** Yeni bir nokta çizginin üstüne düşerse, Sınıf 2'ye aittir. Altına düşerse, Sınıf 1'e aittir. |


# Kümeleme İçin Temel Tanımlar 🔑

Belirli algoritmalara dalmadan önce, kümelemenin nasıl çalıştığını anlamanıza yardımcı olacak birkaç temel kavramı tanımlayalım:

### 🌐 Küme (Cluster)

<img width="1092" height="421" alt="image" src="https://github.com/user-attachments/assets/2c819659-0004-4137-bfcb-c4975457b8a0" />

[Picture](https://dev.to/anurag629/centroid-based-clustering-a-powerful-machine-learning-technique-for-partitioning-datasets-41im)

* **Tanım:** Birbirine **benzer** olan veri noktalarının oluşturduğu bir gruptur.
* **Önemi:** Her küme, veri içindeki **ayrık bir grubu** veya **segmenti** temsil eder. Örneğin, müşteri kümelemesinde her küme, farklı bir müşteri tipine karşılık gelir.

### 🎯 Merkez Noktası (Centroid)

<img width="682" height="251" alt="image" src="https://github.com/user-attachments/assets/1fcce4cf-6260-44e3-a7d2-cf3fddcd1120" />

* **Tanım:** Bir küme içindeki tüm veri noktalarının "ortalama" konumunu temsil eden **merkezi noktadır**.
* **Önemi:** Kümenin konumunu ve sınırlarını tanımlamak için referans noktası görevi görür. Özellikle **K-Ortalamalar (K-Means)** gibi algoritmalar bu merkez noktalarını kullanarak kümelemeyi gerçekleştirir.


### 📏 Mesafe Metriği (Distance Metric)

<img width="514" height="604" alt="image" src="https://github.com/user-attachments/assets/45073d12-ea5a-40c8-9305-148631274de7" />

* **Tanım:** İki noktanın birbirine ne kadar uzak olduğunu ölçmek için kullanılan bir yöntemdir.
* **Yaygın Metrikler:**
    * **Öklid Mesafesi (Euclidean distance):** İki nokta arasındaki düz çizgi mesafesidir (uzayın iki noktası arasındaki en kısa mesafe).
    * **Manhattan Mesafesi (Manhattan distance):** İki nokta arasındaki mesafenin, bir ızgaranın eksenleri boyunca ölçülen toplamı (şehir blokları arasında yürümeye benzer).

### ⬇️ Eylemsizlik (Inertia)
* **Tanım:** Kümelemede, eylemsizlik (inertia), veri noktalarının ait oldukları kümelere ne kadar iyi uyduğunu ölçer.
* **Önemi:** **Daha düşük eylemsizlik**, kümelerin daha **kompakt** olduğu ve noktaların kendi merkez noktalarına (**centroid**) daha yakın olduğu anlamına gelir. Başka bir deyişle, düşük eylemsizlik daha iyi bir kümeleme kalitesine işaret eder.

---

# 📊 Clustering: KMeans

---

## 1. What is K-Means? 🧐

**K-Means**, makine öğrenmesinde en popüler **kümeleme algoritmalarından** biridir.

* Veri noktalarını **benzerliğe** göre $K$ kümeye ayırır.
* **Amaç**: Aynı kümedeki veri noktalarının, diğer kümelerdeki noktalara göre birbirine daha benzer olmasını sağlamaktır.
* 👉 Sınıflandırmanın aksine, K-Means'in etiketlere ihtiyacı yoktur; grupları **kendi başına** bulur (Gözetimsiz Öğrenme - Unsupervised Learning).

### Örnek Kullanım 🛍️

* Bir çevrimiçi perakende şirketi, müşterilerini **satın alma alışkanlıklarına** göre segmentlere ayırmak ister.
* Müşteri verileri (harcanan miktar, satın alma sıklığı) üzerinde K-Means kümelemesi çalıştırılarak, müşteriler **yüksek harcama yapanlar**, **ara sıra alıcılar** ve **fiyat hassasiyeti olanlar** gibi farklı segmentlere ayrılabilir.

### Ana Problem ⚠️

* K-Means algoritmasının temel problemi, bizden **küme sayısını ($K$) önceden belirtmemizi** istemesidir. Bekleyebileceğiniz gibi, bu her zaman uygun ve hatta mümkün değildir.

---

## 2. How Does K-Means Work? ⚙️

K-Means, kümeleme hedefine ulaşmak için yinelenen (iteratif) adımlar kullanır:

1.  **Küme Sayısını ($K$) Seçin.**
    * *Örnek*: $K = 3$ seçmek, verileri $3$ gruba ayırmak istediğimiz anlamına gelir.

2.  **Rastgele Merkezleri (Centroids) Başlatın.**
    * Veri alanına $K$ adet noktayı rastgele yerleştirin (bunlar ilk küme "**merkezleri**"dir).
    * 

3.  **Her Veri Noktasını En Yakın Merkeze Atayın.**
    * Bu adım geçici kümeleri oluşturur.

4.  **Merkezleri Güncelleyin.**
    * Her kümedeki noktaların **ortalama konumunu** hesaplayın ve merkezi (centroid) oraya taşıyın.

5.  **Yakınsayana Kadar Tekrarlayın (3-4. Adımlar).**
    * Merkezler çok fazla hareket etmeyi bırakana kadar (dengeye ulaşana kadar) adımları tekrarlayın.

✨ **Sonuç**: Benzer veri noktalarından oluşan $K$ adet grup elde edilir.

---

## Example 1: 2 clusters (K=2) 🎯

Aşağıda, 2 kümenin net bir şekilde görüldüğü bir örnek üzerinden K-means'in adım adım nasıl çalıştığını görelim:

<img width="571" height="615" alt="image" src="https://github.com/user-attachments/assets/10c29e5d-3998-41a9-a015-82c029c0e051" />


## Example 2: 3 clusters (K=3) 🌟

Let’s see how K-means works step by step.

Below, there is an example where we clearly want **3 clusters**. Here is how K-means will perform its job, iteratively refining the groups until convergence:

<img width="410" height="361" alt="image" src="https://github.com/user-attachments/assets/7cd2285f-6031-4d15-b8db-1f8d8c187589" />


### Step-by-Step Process:

1.  **Initial State (Random Centroids):**
    * $K=3$ centroids are placed randomly (e.g., Red, Blue, Green dots).
      

2.  **Assignment Phase:**
    * Every data point is assigned to the **nearest** centroid. This forms the initial, often rough, clusters.
      

3.  **Update Phase (Iteration 1):**
    * The centroids are moved to the **mean (average) position** of the points currently assigned to them.
      

4.  **Re-Assignment & Update (Iteration N):**
    * Steps 2 and 3 are repeated. Points are re-assigned to the *new* nearest centroid, and the centroids are updated again.
    * This continues until the centroid positions stabilize (i.e., they stop moving significantly).

### Final Result (Convergence) ✅

The algorithm converges, yielding three tightly grouped clusters of similar data points.

* This example perfectly illustrates the **iterative refinement** that is central to the K-Means algorithm.

# 4. Pros & Cons of K-Means (K-Means'in Artıları ve Eksileri) ⚖️

---

## K-Means Avantaj ve Dezavantajları Karşılaştırması 📋

| Kategori | ✅ Avantajlar (Pros) | ❌ Dezavantajlar (Cons) |
| :--- | :--- | :--- |
| **Hız & Basitlik** | Basit ve hızlıdır. | - |
| **Veri Boyutu** | Büyük veri setleriyle iyi çalışır. | - |
| **Anlaşılabilirlik** | Anlaması ve görselleştirmesi kolaydır. | - |
| **Parametre Seçimi** | - | $K$ (küme sayısı) önceden seçilmelidir. |
| **Veri Hassasiyeti** | - | Aykırı değerlere (outliers) karşı hassastır (tek bir aşırı değer, bir merkez noktasını kaydırabilir). |
| **Küme Şekli** | - | Yalnızca kümeler kabaca **küresel** (spherical) ve boyutları **dengeli** ise iyi çalışır. |

---

## 📌 Summary (Özet)

* **K-Means**, basit ve güçlü bir kümeleme algoritmasıdır.
* Noktaları en yakın merkeze atayarak ve merkezleri denge sağlanana kadar güncelleyerek çalışır.
* **Temel Zorluk**: Doğru $K$ (küme sayısı) değerini seçmektir.
* ```

---
# 💡 K-Means'te Optimal Küme Sayısını ($K$) Belirleme Yöntemleri 🎯

K-Means'in en büyük zorluğu olan $K$ sayısının önceden seçilmesi problemini çözmek için kullanılan iki temel yöntem aşağıdadır.

---

## 1. Dirsek Yöntemi (Elbow Method) 📉

**Prensip:**
* Bu yöntem, her $K$ değeri için **Küme İçi Kareler Toplamı (WCSS - Within-Cluster Sum of Squares)** metriğini hesaplamayı ve minimize etmeyi amaçlar.
* WCSS, bir kümedeki her noktanın, o kümenin merkezine olan uzaklığının karelerinin toplamıdır.

**İşleyiş:**
1.  $K=1$'den başlayarak belirli bir üst sınıra ($K=10$ gibi) kadar her bir $K$ değeri için K-Means çalıştırılır.
2.  Her bir çalışmada elde edilen **WCSS** değeri kaydedilir.
3.  $K$ değerine karşılık WCSS değerleri grafiğe dökülür.

**Çözüm:**
* Grafik üzerinde, WCSS değerinin düşüş hızının aniden yavaşladığı bir nokta aranır. Bu noktaya **"dirsek noktası" (elbow point)** denir.
* Dirsek noktası, eklenen her yeni kümenin getirdiği faydanın (WCSS'teki düşüşün) azalmaya başladığı $K$ değerini gösterir ve bu, genellikle **optimal $K$ sayısı** olarak kabul edilir.
* 

---

## 2. Silüet Skoru (Silhouette Score / Analysis) 📊

**Prensip:**
* Bu yöntem, hem küme **içi uyumu (cohesion)** hem de kümeler **arası ayrımı (separation)** aynı anda ölçerek daha net bir değerlendirme sunar.

**Skor Aralığı:**
* Silüet Skoru, **$-1$ ile $+1$** arasında bir değer alır.

| Skor Değeri | Anlamı |
| :--- | :--- |
| **$+1$'e yakın** | Veri noktası kendi kümesine çok iyi atanmış ve diğerlerinden uzaktır (**Mükemmel Kümeleme**). |
| **$0$'a yakın** | Veri noktası iki kümenin sınırındadır, doğru kümede olduğundan emin değildir. |
| **$-1$'e yakın** | Veri noktası **yanlış kümededir** (**Kötü Kümeleme**). |

**İşleyiş:**
1.  $K$ değerleri bir aralıkta (örneğin $K=2$'den başlayarak) denenir.
2.  Her bir $K$ değeri için tüm veri noktalarının **ortalama Silüet Skoru** hesaplanır.

**Çözüm:**
* **En yüksek** ortalama Silüet Skoru'nu veren $K$ değeri, genellikle en iyi küme sayısını gösterir.


# 🥋 Yöntemlerin Karşılaştırmalı Özeti

| Kriter | Dirsek Yöntemi (Elbow Method) 📉 | Silüet Skoru (Silhouette Score) 📊 |
| :--- | :--- | :--- |
| **Ölçülen Metrik** | Küme İçi Kareler Toplamı (WCSS) | Küme İçi Uyum ve Kümeler Arası Ayrım Dengesi |
| **Aranan Değer** | Düşüşün yavaşladığı "**Dirsek Noktası**" | En yüksek skoru veren $K$ değeri ($\approx +1$) |
| **Odak Noktası** | Kümelerin ne kadar **sıkı** (kompakt) olduğu. | Kümelerin hem **sıkı** hem de **ayrık** olduğu. |
| **Dezavantaj** | Bazen "**dirsek**" net olmayabilir, yoruma açıktır. | Hesaplaması, WCSS'e göre daha maliyetlidir (özellikle büyük veri setlerinde). |
| **Ne Zaman Kullanılır?** | Hızlı bir ilk tahmin ve görsel sezgi gerektiğinde. | Daha kesin bir sonuç istendiğinde ve küme örtüşmesi olup olmadığı kontrol edilmek istendiğinde. |


---

## Ek Çözüm: Alan Bilgisi (Domain Knowledge) 🧠

Bazı durumlarda $K$ değerini belirlemek için istatistiksel yöntemlere ihtiyaç duyulmaz:

* **Örnek:** Bir e-ticaret şirketi müşterilerini **"Gold", "Silver", "Bronze"** olarak gruplamak istiyorsa, bu durumda $K$ değeri alan bilgisine göre direkt olarak **$K=3$** olarak belirlenir.

Bu yöntemler, "You must choose K beforehand" ($K$'yı önceden seçmelisiniz) zorunluluğunu bilimsel bir yaklaşımla çözerek, modelin gerçek veri yapısına en uygun $K$ değerini bulmasını sağlar.



# 🔬 Clustering: K-Means Değerlendirmesi (Evaluation)

Kümeleme (Clustering) algoritmaları doğası gereği **gözetimsizdir** (Unsupervised). Bu, elimizde verinin doğru küme atamalarını (ground-truth labels) gösteren etiketlerin **olmadığı** anlamına gelir. Bu yüzden kümeleme modelini değerlendirirken:

* Sınıflandırmada kullandığımız doğruluk (accuracy) veya kesinlik (precision) gibi **harici (external) metriklere değil**,
* Kümelerin yapısını kendi içinde ölçen **dahili (internal) metriklere** güvenmek zorundayız.

Bu dahili metriklerin en popüler ikisi **Inertia** ve **Silhouette Skoru**'dur.

---

## Inertia (Eylemsizlik): Küme Sıkılığının Ölçüsü 🎯

**Inertia**, bir K-Means kümelemesinin ne kadar **kompakt** ve **sıkı** olduğunu ölçen temel metriktir.

### Inertia Nedir?

Basitçe ifade etmek gerekirse:
> **Inertia**, veri noktalarının kendi küme merkezlerinden (centroid) ne kadar uzakta olduğunun bir ölçüsüdür.

Daha teknik ifadeyle:
* Inertia, her bir veri noktasının ait olduğu kümenin merkezine olan uzaklığının **karelerinin toplamıdır** (*Sum of Squared Distances - SSD* veya *Within-Cluster Sum of Squares - WCSS* olarak da bilinir).

### Neden Inertia'yı Kullanırız?

K-Means'in ana amacı mesafeyi minimize etmektir. Inertia, bu amaca ne kadar ulaşıldığını gösterir:

* **Daha Düşük Inertia** 👇: Kümeler daha **sıkı** ve daha **kompakt** oluşmuştur. Küme içi benzerlik yüksektir. (**İyi Kümeleme**)
* **Daha Yüksek Inertia** 👆: Noktalar merkezden uzaktadır. Kümeler muhtemelen **dağınık** ve **kötü şekillidir**. (**Kötü Kümeleme**)

👉 **Akılda Kalması İçin**: Inertia, her bir kümenin ne kadar **eş merkezli (cohesive)** olduğunu ölçer.

### 💡 Güncel Piyasa Örneği: Müşteri Segmentasyonu

* Bir e-ticaret şirketi müşterilerini $K=4$ gruba ayırdı.
* **K=4 Modeli Inertia = 50.000**
* **K=3 Modeli Inertia = 90.000**
* **Yorum**: $K=4$ modeli, $K=3$'e göre daha düşük Inertia'ya sahiptir. Bu, 4 kümenin müşterileri **daha sıkı (homojen)** gruplara ayırdığı anlamına gelir.

---

## Inertia'nın Kritik Kısıtlaması ve Çözümü ⚠️

Inertia, tek başına **optimal $K$ sayısını seçmek** için doğrudan kullanılamaz. Bunun temel nedeni:

* **K Artarsa, Inertia Daima Azalır.**
* Mantık: Küme sayısını artırmak, her noktayı merkezine daha da yaklaştırır. (Extreme durumda, $K=N$ olduğunda Inertia sıfırdır.)

Bu nedenle, sadece en düşük Inertia'yı seçmek, bizi her zaman yüksek bir $K$ değerine (yani aşırı öğrenmeye - **overfitting'e**) götürür.

Bu kısıtlamayı aşmak için kullanılan yöntemler:

* **Dirsek Yöntemi (Elbow Method):** Inertia değerinin düşüşünün aniden yavaşladığı, yani eklenen her kümenin getirdiği faydanın azalmaya başladığı "**dirsek**" noktasını bularak en iyi $K$ sayısını önerir.

---

## Silhouette Skoru (İpuçları) 🌟

Diğer önemli metrik olan Silüet Skoru:

* Hem küme **sıkılığını** hem de kümeler **arası ayrımı** (ne kadar uzakta olduklarını) aynı anda ölçer.
* **Çifte Rolü**: Silüet, hem farklı $K$ değerlerini karşılaştırarak **en iyi $K$ sayısını seçmek** için (Model Seçimi) hem de seçilen son modelin kalitesini **raporlamak** için (Değerlendirme) kullanılır.

Bu metrikleri anlamak, yaptığınız kümelemenin iş hedeflerinize ne kadar uygun olduğunu **ispat etmeniz** için hayati önem taşır.

# 💡 K-Means: Optimal $K$ Değerini Belirleme Zorluğu

Şimdiye kadar K-Means'in nasıl çalıştığını gördük: veri noktalarını $K$ adet kümeye ayırmaya çalışır.

Ancak büyük bir soru var: 👉 **Doğru $K$ değerini nasıl seçeriz?**

* Eğer **$K$ çok küçük** seçilirse, çok farklı gruplar bir araya toplanabilir.
* Eğer **$K$ çok büyük** seçilirse, algoritma veriyi "aşırı bölebilir" (**over-split**), bu da anlamsız kümelere yol açabilir.

---

## 1. Dirsek Yöntemi (The Elbow Method) 📉

Dirsek Yöntemi, daha fazla küme eklemenin çok fazla iyileşme sağlamadığı noktayı bulmamıza yardımcı olur.

### Temel Metrik: Inertia (WCSS)

* **Inertia**: Kümelerin ne kadar **dağınık** olduğunu ölçer (daha düşük olması daha iyidir).
* **İşleyiş**: Inertia değerini, $K$ (küme sayısı) değerine karşı grafiklendiririz.

### Çözüm

Grafikte, eğrinin düzleşmeye başladığı veya görsel olarak bir **"dirseğe"** benzediği noktayı ararız. Bu nokta, eklenen her kümenin getirdiği faydanın (Inertia'daki düşüşün) azalmaya başladığı **optimal $K$ değeri** olarak kabul edilir.

<img width="578" height="382" alt="image" src="https://github.com/user-attachments/assets/6965a3a8-9cc5-4188-b8b7-bf5ee41fa563" />

---

## 2. Silüet Skoru (The Silhouette Score) 🌟

Optimal $K$ değerini belirlemenin bir başka yolu da **Silüet Skoru**'dur. Bu metrik, bir veri noktasının kendi kümesine ne kadar iyi uyduğunu, diğer kümelere kıyasla ölçer. Bu, aynı zamanda en uygun $K$ sayısını seçmemize de yardımcı olur.

### Değer Aralığı

Silüet Skoru, **$-1$ ile $+1$** arasında değişir.

* **$+1$'e yakın** 🚀: Veri noktaları kendi kümeleri içinde çok iyi kümelenmiş ve diğer kümelerden uzaktır. (**Yüksek Kaliteli Kümeleme**)
* **$0$'a yakın** 🧭: Kümeler çok fazla **örtüşüyor** veya veri noktası iki kümenin sınırında.
* **Negatif değerler** ❌: Veri noktalarının **yanlış kümede** olabileceği anlamına gelir.

**Amaç**: Farklı $K$ değerlerini denerken **en yüksek** ortalama Silüet Skorunu elde eden $K$'yı seçmektir.

<img width="524" height="337" alt="image" src="https://github.com/user-attachments/assets/3e33fcc3-30a6-442d-95b3-c30d92c06dc5" />

# 📊 Silüet Skoru Analizi ve K Seçimi (Model Selection)

---

## Silüet Skoru (The Silhouette Score) İncelemesi 🌟

**Silüet Skoru**, kümelerin ne kadar **iyi ayrılmış (well-separated)** ve ne kadar **kompakt (compact)** olduğunu aynı anda ölçer.

| Skor | Anlamı | Değerlendirme |
| :--- | :--- | :--- |
| **Daha Yüksek** | Kümeler belirgin ve iyi şekillendirilmiş. | **Daha İyi** ✅ |
| **Daha Düşük** | Kümeler örtüşüyor veya dağınık (messy). | **Daha Kötü** ❌ |

### Grafik Yorumlama Örneği 📈

* **$K=2$:** Silüet Skoru en yüksek ($\approx 0.52$). Bu, 2 küme ile verinin **en temiz** şekilde ayrıldığı anlamına gelir.
* **$K=3$:** Skor hala oldukça yüksek ($\approx 0.50$). Bu, 3 kümenin de **makul** bir seçim olduğunu gösterir.
* **$K > 3$:** $K$, 3'ün üzerine çıktıkça Silüet Skoru sürekli düşer. Bu, kümelerin **daha az belirginleştiği** ve daha fazla örtüştüğü anlamına gelir.

**✅ Sonuç:** Bu analize göre, grupların en net ayrımını sağladıkları için $K$ için en iyi seçimler **2 veya 3 kümedir**.

---

## ☝🏽 Özet: Optimal K Değerini Seçmek

**$K$ sayısını seçmek** bir miktar sanat, bir miktar bilimdir. En iyi sonucu almak için farklı yaklaşımların birleştirilmesi gerekir:

* **✅ Dirsek Yöntemi (Elbow Method):** Inertia eğrisindeki **bükülme (bend)** noktasını arayın.
* **✅ Silüet Skoru (Silhouette Score):** En yüksek **ortalama küme ayrımına** sahip olan $K$'yı bulun.

Gerçek projelerde, genellikle her iki yöntemi (ve diğerlerini) dener ve bu sonuçları **alan bilginizle** (domain knowledge) birleştirirsiniz (Örn: "Penguen veri setinde 3 penguen türü olduğunu biliyoruz").

# 🌳 Hierarchical Clustering (Hiyerarşik Kümeleme)

Şimdiye kadar, algoritmayı çalıştırmadan önce **küme sayısı $K$**’yı seçmeniz gereken **K-Means**'e baktık.

Peki, küme sayısının kaç olması gerektiğini **bilmiyorsanız** ne olur? Veya kümelerin farklı **ayrıntılılık (granularity)** seviyelerinde nasıl birleştiğini ve bölündüğünü görmek isterseniz?

İşte bu noktada **Hiyerarşik Kümeleme** devreye girer. Hiyerarşik Kümeleme, kullanıcının **önceden küme sayısını belirtmesini gerektirmez**.

---

## Hierarchical Clustering Nedir?

Hiyerarşik Kümeleme, adından da anlaşılacağı gibi, bir küme **ağacı** oluşturur. Tek bir sabit küme sayısına karar vermek yerine, veri noktalarının adım adım nasıl gruplandırılabileceğini size gösterir.

### İki Ana Yaklaşım

| Yaklaşım | Yön | Başlangıç Durumu | İşlem Adımı | Son Durum |
| :--- | :--- | :--- | :--- | :--- |
| **Agglomerative** (Bottom-up) | ⬆️ Aşağıdan Yukarı | Her veri noktası kendi kümesidir. | En yakın iki kümeyi **birleştir**. | Her şey tek bir büyük kümede birleşir. |
| **Divisive** (Top-down) | ⬇️ Yukarıdan Aşağı | Tüm noktalar tek bir kümededir. | Kümeyi adım adım daha küçük gruplara **böl**. | Her nokta kendi kümesidir. |

👉 **Pratikte:** Uygulamada **Agglomerative (Aşağıdan Yukarı) kümeleme** çok daha sık kullanılır.

<img width="674" height="308" alt="image" src="https://github.com/user-attachments/assets/7ad3c28c-1d9a-47e3-af5a-4503c0cb55bb" />

# 🌳 Dendrogram: Hiyerarşik Kümelemenin Görselleştirilmesi

Hiyerarşik Kümeleme'nin sonucu, kümelerin nasıl birleştiğini gösteren ağaç benzeri bir diyagram olan **dendrogram** ile görselleştirilir.

### Dendrogram Yapısı

* **Dikey Eksen (Vertical Axis)** 📏: **Mesafe** (veya benzemezlik/dissimilarity). Yüksekten kesmek, kümeler arası büyük bir mesafeyi kabul etmek demektir.
* **Yatay Eksen (Horizontal Axis)** 📊: Veri noktaları veya kümeler.

### Küme Sayısının Belirlenmesi

* Ağacı seçilen bir yükseklikte **"keserek"**, kaç küme tutacağınıza karar verirsiniz.
* **Örnek**: Dendrogramı belirli bir seviyede kesmek size 2 büyük küme verebilirken, daha aşağıdan kesmek size 5 daha küçük küme verebilir.
    * 

---

## Kümeler Arası Mesafeyi Ölçme (Linkage Criteria) 🔗

Kümeler arasındaki mesafe, farklı şekillerde tanımlanabilir. Bu yöntemlere **bağlantı kriterleri (linkage criteria)** denir:

| Kriter | Açıklama | Odak Noktası |
| :--- | :--- | :--- |
| **Single Linkage** | Her kümedeki **en yakın iki nokta** arasındaki mesafe. | En kısa mesafeyle birleşme. |
| **Complete Linkage** | Her kümedeki **en uzak iki nokta** arasındaki mesafe. | Kümelerin maksimum yayılımını kontrol etme. |
| **Average Linkage** | Tüm nokta **çiftleri** arasındaki mesafelerin ortalaması. | Dengeli bir ortalama mesafe. |
| **Ward's Method** | Küme içindeki **varyansı (dağılımı) minimize etmeyi** amaçlar. | Küme sıkılığını artırma (Çok popülerdir). |

<img width="346" height="471" alt="image" src="https://github.com/user-attachments/assets/ae673f36-d073-4e70-ac5a-dde118e9dffa" />


# 🌲 Hiyerarşik Kümelemenin (HC) Avantajları ve Kısıtlamaları

Hiyerarşik Kümeleme, K-Means'in zorlandığı durumlar için güçlü bir alternatif sunar, özellikle veride doğal bir hiyerarşi olduğundan şüphelenildiğinde tercih edilir.

# ✅ Avantajlar (Pros) of Hierarchical Clustering

| Avantaj | Açıklama | K-Means'ten Üstünlüğü | Piyasa Örneği 💡 |
| :--- | :--- | :--- | :--- |
| **Önceden $K$ Belirleme Zorunluluğu Yok** | Algoritmayı çalıştırmadan önce kaç küme olacağını söylemeniz gerekmez. | **K-Means'in Ana Zorluğunu Giderir** | **Biyoloji/Genetik:** Bilim insanları, gen ifadesi verilerinde kaç genetik alt grubun olduğunu bilmeden analize başlayabilir. |
| **Görsel Yorumlama (Dendrogram)** | Sonuçları, kümelerin nasıl birleştiğini gösteren net bir ağaç diyagramı (**dendrogram**) ile sunar. | **Modelin Sezgisel Anlaşılırlığını Artırır** | **Müşteri Segmentasyonu:** Pazarlama ekibi, müşterilerin büyük ana gruplardan alt segmentlere nasıl ayrıldığını görsel olarak takip ederek stratejilerini daha iyi kurabilir. |
| **Hiyerarşik Yapı Başarısı** | Kümelerin iç içe (nested) olduğu veya bir soy ağacı gibi bir hiyerarşiye sahip olduğu durumlarda çok iyi çalışır. | **Doğal İlişkileri Ortaya Çıkarır** | **Taksonomi:** Türlerin ve alt türlerin evrimsel ilişkilerini ve sınıflandırmasını modellemek için idealdir. |

# ❌ Kısıtlamalar (Limitations) of Hierarchical Clustering

| Kısıtlama | Açıklama | K-Means'ten Dezavantajı | Uygulama Etkisi |
| :--- | :--- | :--- | :--- |
| **Hesaplama Maliyeti (Complexity)** 🐢 | Algoritma, her adımda mesafeleri hesapladığı için büyük veri setlerinde **daha yavaştır**. | **Büyük Ölçekte Çalışmayı Zorlaştırır** | Milyonlarca müşterisi olan bir platformda gerçek zamanlı analiz zordur. |
| **Gürültüye ve Aykırı Değerlere Hassasiyet** 👂 | Özellikle "Single Linkage" gibi yöntemler, tek bir aykırı değerin kümeleri yanlışlıkla birbirine bağlamasına ("**chaining**" etkisi) neden olabilir. | **Sağlamlığı (Robustness) Azaltır** | Veri ön işleme (outlier removal) adımı, bu yöntemde kritik önem taşır. |
| **Geri Dönüş Yok (Irreversible Merges)** 🚫 | Bir hata yapılıp iki küme birleştirildiğinde (Agglomerative), bir sonraki adımda bu birleşme **geri alınamaz**. | **Hata Düzeltme Esnekliğini Sınırlar** | Yanlış bir birleşme kararı, sonraki tüm küme yapısını bozabilir. |

# 🌟 Uzman Görüşü: Ne Zaman Hiyerarşik Kümelemeyi (HC) Tercih Etmeliyiz?

Kümeleme yöntemi seçimi, projenizin hedeflerine ve veri setinizin özelliklerine bağlıdır:

| Seçim Kriteri | K-Means (Hızlı) 🏎️ | Hiyerarşik Kümeleme (Detaylı) 🌲 |
| :--- | :--- | :--- |
| **Ana Hedef** | Net bir küme sayısı (Örn: $K=4$ segment) ve **hız**. | Kümeler arasındaki **ilişki haritasını** ve **doğal hiyerarşiyi** keşfetmek. |
| **Veri Boyutu** | Büyük veri setleri. | Nispeten **küçük** veri setleri. |
| **Görselleştirme** | Küme merkezleri ve iki boyutlu ayrım (PCA/t-SNE ile). | **Dendrogram** üzerinde küme sayısının etkisini ve birleşmeleri görselleştirme. |

**Özet:**

* Eğer hedefiniz **net bir küme sayısı** ve **hız** ise, **K-Means** idealdir.
* Ancak, veri setiniz **nispeten küçük** ise ve asıl hedefiniz, kümeler arasındaki **ilişki haritasını** çıkarmak, veri yapısındaki **doğal hiyerarşiyi** keşfetmek ve küme sayısının etkisini dendrogram üzerinde görselleştirmek ise, **Hiyerarşik Kümeleme** vazgeçilmez bir araçtır.
