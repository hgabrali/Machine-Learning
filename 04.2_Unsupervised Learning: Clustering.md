# Unsupervised Learning: Clustering

# Types of Clustering Algorithms ğŸ§©

There are several fundamental approaches to grouping data in unsupervised learning:

### 1. K-Means Clustering âš™ï¸
* **Principle:** A prototype-based, partitioned clustering technique. It aims to partition $N$ observations into $K$ clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid).
* **Key Feature:** Requires the user to define the number of clusters ($K$) beforehand.

### 2. Hierarchical Clustering ğŸŒ³
* **Principle:** Creates a tree-like hierarchy of clusters, known as a dendrogram. It can be **Agglomerative** (starting with single points and merging) or **Divisive** (starting with one big cluster and splitting).
* **Key Feature:** Does not require the number of clusters to be specified initially; the number of clusters is chosen by cutting the dendrogram at a desired level.

### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ğŸ’¡
* **Principle:** Clusters are defined as areas of higher density than the remainder of the dataset. It can find arbitrarily shaped clusters and is robust to noise (outliers).
* **Key Feature:** Does not require a fixed number of clusters and is excellent at identifying outliers (points that do not belong to any cluster).


----

# TÃœRKCE

# KÃ¼meleme AlgoritmasÄ± TÃ¼rleri ğŸ§©

KÃ¼meleme (Clustering) yapmak iÃ§in kullanÄ±lan birkaÃ§ temel yaklaÅŸÄ±m vardÄ±r:

### ğŸŒŸ K-Ortalamalar KÃ¼meleme (K-Means Clustering)
* **Temel YapÄ±:** Veriyi, Ã¶nceden belirlenen $k$ (kÃ¼me sayÄ±sÄ±) adet merkeze (centroid) gÃ¶re gruplandÄ±rÄ±r. Her veri noktasÄ±, kendisine en yakÄ±n olan merkezin kÃ¼mesine atanÄ±r.
* **KullanÄ±m:** HÄ±zlÄ± ve basit olmasÄ± nedeniyle bÃ¼yÃ¼k veri setlerinde yaygÄ±n olarak kullanÄ±lÄ±r.

### ğŸŒ³ HiyerarÅŸik KÃ¼meleme (Hierarchical Clustering)
* **Temel YapÄ±:** KÃ¼meleme hiyerarÅŸik bir aÄŸaÃ§ yapÄ±sÄ± (dendrogram) oluÅŸturarak yapÄ±lÄ±r. Ä°ki ana yaklaÅŸÄ±mÄ± vardÄ±r: birleÅŸtirici (agglomerative - alttan yukarÄ±) veya bÃ¶lÃ¼cÃ¼ (divisive - Ã¼stten aÅŸaÄŸÄ±).
* **KullanÄ±m:** KÃ¼me sayÄ±sÄ±nÄ±n Ã¶nceden bilinmediÄŸi ve kÃ¼me hiyerarÅŸisinin Ã¶nemli olduÄŸu durumlar iÃ§in idealdir.

### ğŸ“ GÃ¼rÃ¼ltÃ¼lÃ¼ Uygulamalar iÃ§in YoÄŸunluk TabanlÄ± Mekansal KÃ¼meleme (DBSCAN - Density-Based Spatial Clustering of Applications with Noise)
* **Temel YapÄ±:** Veri noktalarÄ±nÄ±, yÃ¼ksek yoÄŸunluklu bÃ¶lgeler oluÅŸturarak gruplandÄ±rÄ±r. DÃ¼ÅŸÃ¼k yoÄŸunluklu bÃ¶lgelerdeki noktalarÄ± "gÃ¼rÃ¼ltÃ¼" veya "aykÄ±rÄ± deÄŸer" (outlier) olarak iÅŸaretler.
* **KullanÄ±m:** KÃ¼me ÅŸekillerinin dÃ¼zensiz olduÄŸu ve gÃ¼rÃ¼ltÃ¼lÃ¼ verilerin bulunduÄŸu durumlar iÃ§in Ã§ok etkilidir.



# KÃ¼meleme Nedir? (Clustering) ğŸ§©

KÃ¼meleme, denetimsiz Ã¶ÄŸrenmedeki en Ã¶nemli tekniklerden biridir.

Temel olarak, benzer veri noktalarÄ±nÄ± gruplara ayÄ±rmakla ilgilidir â€” bu gruplarÄ±n (etiketlerin) ne olduÄŸu size **sÃ¶ylenmeden**.

---

## KÃ¼meleme Nedir?

ğŸ‘‰ Bir kutunun Ã¼zerinde resmi olmayan bir yapboz yÄ±ÄŸÄ±nÄ±na sahip olduÄŸunuzu hayal edin.

KÃ¼meleme, parÃ§alarÄ± otomatik olarak benzer gÃ¶rÃ¼nen gruplara ayÄ±rmaya benzer, bÃ¶ylece onlarÄ± anlamlandÄ±rabilirsiniz.

* HiÃ§bir etiket verilmez â€” algoritma gruplarÄ± **kendi baÅŸÄ±na** keÅŸfeder.
* AynÄ± kÃ¼me iÃ§indeki veri noktalarÄ±, diÄŸer kÃ¼melerdeki noktalara gÃ¶re birbirine **daha fazla benzerdir**.

---

## KÃ¼meleme, SÄ±nÄ±flandÄ±rmadan NasÄ±l FarklÄ±dÄ±r? ğŸ¤”

<img width="761" height="379" alt="image" src="https://github.com/user-attachments/assets/9297ce01-d3b7-4e3c-89e3-dfc6722fe01a" />

[Picture](https://techdifferences.com/difference-between-classification-and-clustering.html)

Ä°lk bakÄ±ÅŸta, kÃ¼meleme, sÄ±nÄ±flandÄ±rmaya benzer gelebilir; her ikisi de veriyi gruplara ayÄ±rmayÄ± iÃ§erir. Ancak:

* **SÄ±nÄ±flandÄ±rma:** GruplarÄ± **Ã¶nceden biliriz** ve modelin onlarÄ± tahmin etmesini isteriz (Denetimli Ã–ÄŸrenme).
* **KÃ¼meleme:** GruplarÄ± **bilmeyiz** ve algoritmanÄ±n onlarÄ± keÅŸfetmesini isteriz (Denetimsiz Ã–ÄŸrenme).

### Ã–rnek 1: Grup KeÅŸfi vs. Tahmin ğŸ•ğŸ¦ˆ

| KÃ¼meleme (KeÅŸif) ğŸ—ºï¸ | SÄ±nÄ±flandÄ±rma (Tahmin) ğŸ¯ |
| :--- | :--- |
| Algoritma etiketleri (kedi, kÃ¶pek, balÄ±k, kÃ¶pek balÄ±ÄŸÄ±) bilmez. Sadece benzer ÅŸeyleri Ã¶zelliklerine gÃ¶re gruplar. | Burada etiketleri zaten biliyoruz ("Kedi = SÄ±nÄ±f 1", "KÃ¶pek = SÄ±nÄ±f 2", vb.). Model bu etiketlerden Ã¶ÄŸrenir ve yeni veri iÃ§in doÄŸru olanÄ± tahmin eder. |
| ğŸ‘‰ **Ã–rnek:** Kediyi ve kÃ¶peÄŸi bir araya (belki ikisi de karada yÃ¼rÃ¼yor), balÄ±ÄŸÄ± ve kÃ¶pek balÄ±ÄŸÄ±nÄ± bir araya (ikisi de yÃ¼zÃ¼yor) yerleÅŸtirir. | ğŸ‘‰ **Ã–rnek:** Modele yeni bir kÃ¶pek balÄ±ÄŸÄ± gÃ¶sterirsek, onu "SÄ±nÄ±f 4: KÃ¶pek BalÄ±ÄŸÄ±" olarak atayacaktÄ±r. |

### Ã–rnek 2: SÄ±nÄ±r KeÅŸfi vs. SÄ±nÄ±r Ã–ÄŸrenimi ğŸ”´ğŸ”µ

| KÃ¼meleme (Veri BenzerliÄŸine Odaklanma) âœ¨ | SÄ±nÄ±flandÄ±rma (SÄ±nÄ±r Ã‡izgisine Odaklanma) ğŸ“ |
| :--- | :--- |
| Etiket yoktur. Algoritma, yalnÄ±zca **benzerliÄŸe** dayanarak yakÄ±ndaki noktalarÄ± kÃ¼melere (KÃ¼me 1, KÃ¼me 2) ayÄ±rÄ±r. | Gruplar zaten etiketlenmiÅŸtir (SÄ±nÄ±f 1, SÄ±nÄ±f 2). Modelin gÃ¶revi, sÄ±nÄ±flarÄ± ayÄ±ran bir **sÄ±nÄ±r Ã§izgisi** Ã¶ÄŸrenmektir. |
| ğŸ‘‰ **Ã–rnek:** YalnÄ±zca bazÄ± noktalarÄ±n birbirine yakÄ±n olduÄŸunu "fark eder" ve onlarÄ± tek bir grup olarak ele alÄ±r. | ğŸ‘‰ **Ã–rnek:** Yeni bir nokta Ã§izginin Ã¼stÃ¼ne dÃ¼ÅŸerse, SÄ±nÄ±f 2'ye aittir. AltÄ±na dÃ¼ÅŸerse, SÄ±nÄ±f 1'e aittir. |


# KÃ¼meleme Ä°Ã§in Temel TanÄ±mlar ğŸ”‘

Belirli algoritmalara dalmadan Ã¶nce, kÃ¼melemenin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamanÄ±za yardÄ±mcÄ± olacak birkaÃ§ temel kavramÄ± tanÄ±mlayalÄ±m:

### ğŸŒ KÃ¼me (Cluster)

<img width="1092" height="421" alt="image" src="https://github.com/user-attachments/assets/2c819659-0004-4137-bfcb-c4975457b8a0" />

[Picture](https://dev.to/anurag629/centroid-based-clustering-a-powerful-machine-learning-technique-for-partitioning-datasets-41im)

* **TanÄ±m:** Birbirine **benzer** olan veri noktalarÄ±nÄ±n oluÅŸturduÄŸu bir gruptur.
* **Ã–nemi:** Her kÃ¼me, veri iÃ§indeki **ayrÄ±k bir grubu** veya **segmenti** temsil eder. Ã–rneÄŸin, mÃ¼ÅŸteri kÃ¼melemesinde her kÃ¼me, farklÄ± bir mÃ¼ÅŸteri tipine karÅŸÄ±lÄ±k gelir.

### ğŸ¯ Merkez NoktasÄ± (Centroid)

<img width="682" height="251" alt="image" src="https://github.com/user-attachments/assets/1fcce4cf-6260-44e3-a7d2-cf3fddcd1120" />

* **TanÄ±m:** Bir kÃ¼me iÃ§indeki tÃ¼m veri noktalarÄ±nÄ±n "ortalama" konumunu temsil eden **merkezi noktadÄ±r**.
* **Ã–nemi:** KÃ¼menin konumunu ve sÄ±nÄ±rlarÄ±nÄ± tanÄ±mlamak iÃ§in referans noktasÄ± gÃ¶revi gÃ¶rÃ¼r. Ã–zellikle **K-Ortalamalar (K-Means)** gibi algoritmalar bu merkez noktalarÄ±nÄ± kullanarak kÃ¼melemeyi gerÃ§ekleÅŸtirir.


### ğŸ“ Mesafe MetriÄŸi (Distance Metric)

<img width="514" height="604" alt="image" src="https://github.com/user-attachments/assets/45073d12-ea5a-40c8-9305-148631274de7" />

* **TanÄ±m:** Ä°ki noktanÄ±n birbirine ne kadar uzak olduÄŸunu Ã¶lÃ§mek iÃ§in kullanÄ±lan bir yÃ¶ntemdir.
* **YaygÄ±n Metrikler:**
    * **Ã–klid Mesafesi (Euclidean distance):** Ä°ki nokta arasÄ±ndaki dÃ¼z Ã§izgi mesafesidir (uzayÄ±n iki noktasÄ± arasÄ±ndaki en kÄ±sa mesafe).
    * **Manhattan Mesafesi (Manhattan distance):** Ä°ki nokta arasÄ±ndaki mesafenin, bir Ä±zgaranÄ±n eksenleri boyunca Ã¶lÃ§Ã¼len toplamÄ± (ÅŸehir bloklarÄ± arasÄ±nda yÃ¼rÃ¼meye benzer).

### â¬‡ï¸ Eylemsizlik (Inertia)
* **TanÄ±m:** KÃ¼melemede, eylemsizlik (inertia), veri noktalarÄ±nÄ±n ait olduklarÄ± kÃ¼melere ne kadar iyi uyduÄŸunu Ã¶lÃ§er.
* **Ã–nemi:** **Daha dÃ¼ÅŸÃ¼k eylemsizlik**, kÃ¼melerin daha **kompakt** olduÄŸu ve noktalarÄ±n kendi merkez noktalarÄ±na (**centroid**) daha yakÄ±n olduÄŸu anlamÄ±na gelir. BaÅŸka bir deyiÅŸle, dÃ¼ÅŸÃ¼k eylemsizlik daha iyi bir kÃ¼meleme kalitesine iÅŸaret eder.

---

# ğŸ“Š Clustering: KMeans

---

## 1. What is K-Means? ğŸ§

**K-Means**, makine Ã¶ÄŸrenmesinde en popÃ¼ler **kÃ¼meleme algoritmalarÄ±ndan** biridir.

* Veri noktalarÄ±nÄ± **benzerliÄŸe** gÃ¶re $K$ kÃ¼meye ayÄ±rÄ±r.
* **AmaÃ§**: AynÄ± kÃ¼medeki veri noktalarÄ±nÄ±n, diÄŸer kÃ¼melerdeki noktalara gÃ¶re birbirine daha benzer olmasÄ±nÄ± saÄŸlamaktÄ±r.
* ğŸ‘‰ SÄ±nÄ±flandÄ±rmanÄ±n aksine, K-Means'in etiketlere ihtiyacÄ± yoktur; gruplarÄ± **kendi baÅŸÄ±na** bulur (GÃ¶zetimsiz Ã–ÄŸrenme - Unsupervised Learning).

### Ã–rnek KullanÄ±m ğŸ›ï¸

* Bir Ã§evrimiÃ§i perakende ÅŸirketi, mÃ¼ÅŸterilerini **satÄ±n alma alÄ±ÅŸkanlÄ±klarÄ±na** gÃ¶re segmentlere ayÄ±rmak ister.
* MÃ¼ÅŸteri verileri (harcanan miktar, satÄ±n alma sÄ±klÄ±ÄŸÄ±) Ã¼zerinde K-Means kÃ¼melemesi Ã§alÄ±ÅŸtÄ±rÄ±larak, mÃ¼ÅŸteriler **yÃ¼ksek harcama yapanlar**, **ara sÄ±ra alÄ±cÄ±lar** ve **fiyat hassasiyeti olanlar** gibi farklÄ± segmentlere ayrÄ±labilir.

### Ana Problem âš ï¸

* K-Means algoritmasÄ±nÄ±n temel problemi, bizden **kÃ¼me sayÄ±sÄ±nÄ± ($K$) Ã¶nceden belirtmemizi** istemesidir. BekleyebileceÄŸiniz gibi, bu her zaman uygun ve hatta mÃ¼mkÃ¼n deÄŸildir.

---

## 2. How Does K-Means Work? âš™ï¸

K-Means, kÃ¼meleme hedefine ulaÅŸmak iÃ§in yinelenen (iteratif) adÄ±mlar kullanÄ±r:

1.  **KÃ¼me SayÄ±sÄ±nÄ± ($K$) SeÃ§in.**
    * *Ã–rnek*: $K = 3$ seÃ§mek, verileri $3$ gruba ayÄ±rmak istediÄŸimiz anlamÄ±na gelir.

2.  **Rastgele Merkezleri (Centroids) BaÅŸlatÄ±n.**
    * Veri alanÄ±na $K$ adet noktayÄ± rastgele yerleÅŸtirin (bunlar ilk kÃ¼me "**merkezleri**"dir).
    * 

3.  **Her Veri NoktasÄ±nÄ± En YakÄ±n Merkeze AtayÄ±n.**
    * Bu adÄ±m geÃ§ici kÃ¼meleri oluÅŸturur.

4.  **Merkezleri GÃ¼ncelleyin.**
    * Her kÃ¼medeki noktalarÄ±n **ortalama konumunu** hesaplayÄ±n ve merkezi (centroid) oraya taÅŸÄ±yÄ±n.

5.  **YakÄ±nsayana Kadar TekrarlayÄ±n (3-4. AdÄ±mlar).**
    * Merkezler Ã§ok fazla hareket etmeyi bÄ±rakana kadar (dengeye ulaÅŸana kadar) adÄ±mlarÄ± tekrarlayÄ±n.

âœ¨ **SonuÃ§**: Benzer veri noktalarÄ±ndan oluÅŸan $K$ adet grup elde edilir.

---

## Example 1: 2 clusters (K=2) ğŸ¯

AÅŸaÄŸÄ±da, 2 kÃ¼menin net bir ÅŸekilde gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ bir Ã¶rnek Ã¼zerinden K-means'in adÄ±m adÄ±m nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶relim:

<img width="571" height="615" alt="image" src="https://github.com/user-attachments/assets/10c29e5d-3998-41a9-a015-82c029c0e051" />


## Example 2: 3 clusters (K=3) ğŸŒŸ

Letâ€™s see how K-means works step by step.

Below, there is an example where we clearly want **3 clusters**. Here is how K-means will perform its job, iteratively refining the groups until convergence:

<img width="410" height="361" alt="image" src="https://github.com/user-attachments/assets/7cd2285f-6031-4d15-b8db-1f8d8c187589" />


### Step-by-Step Process:

1.  **Initial State (Random Centroids):**
    * $K=3$ centroids are placed randomly (e.g., Red, Blue, Green dots).
      

2.  **Assignment Phase:**
    * Every data point is assigned to the **nearest** centroid. This forms the initial, often rough, clusters.
      

3.  **Update Phase (Iteration 1):**
    * The centroids are moved to the **mean (average) position** of the points currently assigned to them.
      

4.  **Re-Assignment & Update (Iteration N):**
    * Steps 2 and 3 are repeated. Points are re-assigned to the *new* nearest centroid, and the centroids are updated again.
    * This continues until the centroid positions stabilize (i.e., they stop moving significantly).

### Final Result (Convergence) âœ…

The algorithm converges, yielding three tightly grouped clusters of similar data points.

* This example perfectly illustrates the **iterative refinement** that is central to the K-Means algorithm.

# 4. Pros & Cons of K-Means (K-Means'in ArtÄ±larÄ± ve Eksileri) âš–ï¸

---

## K-Means Avantaj ve DezavantajlarÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ± ğŸ“‹

| Kategori | âœ… Avantajlar (Pros) | âŒ Dezavantajlar (Cons) |
| :--- | :--- | :--- |
| **HÄ±z & Basitlik** | Basit ve hÄ±zlÄ±dÄ±r. | - |
| **Veri Boyutu** | BÃ¼yÃ¼k veri setleriyle iyi Ã§alÄ±ÅŸÄ±r. | - |
| **AnlaÅŸÄ±labilirlik** | AnlamasÄ± ve gÃ¶rselleÅŸtirmesi kolaydÄ±r. | - |
| **Parametre SeÃ§imi** | - | $K$ (kÃ¼me sayÄ±sÄ±) Ã¶nceden seÃ§ilmelidir. |
| **Veri Hassasiyeti** | - | AykÄ±rÄ± deÄŸerlere (outliers) karÅŸÄ± hassastÄ±r (tek bir aÅŸÄ±rÄ± deÄŸer, bir merkez noktasÄ±nÄ± kaydÄ±rabilir). |
| **KÃ¼me Åekli** | - | YalnÄ±zca kÃ¼meler kabaca **kÃ¼resel** (spherical) ve boyutlarÄ± **dengeli** ise iyi Ã§alÄ±ÅŸÄ±r. |

---

## ğŸ“Œ Summary (Ã–zet)

* **K-Means**, basit ve gÃ¼Ã§lÃ¼ bir kÃ¼meleme algoritmasÄ±dÄ±r.
* NoktalarÄ± en yakÄ±n merkeze atayarak ve merkezleri denge saÄŸlanana kadar gÃ¼ncelleyerek Ã§alÄ±ÅŸÄ±r.
* **Temel Zorluk**: DoÄŸru $K$ (kÃ¼me sayÄ±sÄ±) deÄŸerini seÃ§mektir.
* ```

---
# ğŸ’¡ K-Means'te Optimal KÃ¼me SayÄ±sÄ±nÄ± ($K$) Belirleme YÃ¶ntemleri ğŸ¯

K-Means'in en bÃ¼yÃ¼k zorluÄŸu olan $K$ sayÄ±sÄ±nÄ±n Ã¶nceden seÃ§ilmesi problemini Ã§Ã¶zmek iÃ§in kullanÄ±lan iki temel yÃ¶ntem aÅŸaÄŸÄ±dadÄ±r.

---

## 1. Dirsek YÃ¶ntemi (Elbow Method) ğŸ“‰

**Prensip:**
* Bu yÃ¶ntem, her $K$ deÄŸeri iÃ§in **KÃ¼me Ä°Ã§i Kareler ToplamÄ± (WCSS - Within-Cluster Sum of Squares)** metriÄŸini hesaplamayÄ± ve minimize etmeyi amaÃ§lar.
* WCSS, bir kÃ¼medeki her noktanÄ±n, o kÃ¼menin merkezine olan uzaklÄ±ÄŸÄ±nÄ±n karelerinin toplamÄ±dÄ±r.

**Ä°ÅŸleyiÅŸ:**
1.  $K=1$'den baÅŸlayarak belirli bir Ã¼st sÄ±nÄ±ra ($K=10$ gibi) kadar her bir $K$ deÄŸeri iÃ§in K-Means Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.
2.  Her bir Ã§alÄ±ÅŸmada elde edilen **WCSS** deÄŸeri kaydedilir.
3.  $K$ deÄŸerine karÅŸÄ±lÄ±k WCSS deÄŸerleri grafiÄŸe dÃ¶kÃ¼lÃ¼r.

**Ã‡Ã¶zÃ¼m:**
* Grafik Ã¼zerinde, WCSS deÄŸerinin dÃ¼ÅŸÃ¼ÅŸ hÄ±zÄ±nÄ±n aniden yavaÅŸladÄ±ÄŸÄ± bir nokta aranÄ±r. Bu noktaya **"dirsek noktasÄ±" (elbow point)** denir.
* Dirsek noktasÄ±, eklenen her yeni kÃ¼menin getirdiÄŸi faydanÄ±n (WCSS'teki dÃ¼ÅŸÃ¼ÅŸÃ¼n) azalmaya baÅŸladÄ±ÄŸÄ± $K$ deÄŸerini gÃ¶sterir ve bu, genellikle **optimal $K$ sayÄ±sÄ±** olarak kabul edilir.
* 

---

## 2. SilÃ¼et Skoru (Silhouette Score / Analysis) ğŸ“Š

**Prensip:**
* Bu yÃ¶ntem, hem kÃ¼me **iÃ§i uyumu (cohesion)** hem de kÃ¼meler **arasÄ± ayrÄ±mÄ± (separation)** aynÄ± anda Ã¶lÃ§erek daha net bir deÄŸerlendirme sunar.

**Skor AralÄ±ÄŸÄ±:**
* SilÃ¼et Skoru, **$-1$ ile $+1$** arasÄ±nda bir deÄŸer alÄ±r.

| Skor DeÄŸeri | AnlamÄ± |
| :--- | :--- |
| **$+1$'e yakÄ±n** | Veri noktasÄ± kendi kÃ¼mesine Ã§ok iyi atanmÄ±ÅŸ ve diÄŸerlerinden uzaktÄ±r (**MÃ¼kemmel KÃ¼meleme**). |
| **$0$'a yakÄ±n** | Veri noktasÄ± iki kÃ¼menin sÄ±nÄ±rÄ±ndadÄ±r, doÄŸru kÃ¼mede olduÄŸundan emin deÄŸildir. |
| **$-1$'e yakÄ±n** | Veri noktasÄ± **yanlÄ±ÅŸ kÃ¼mededir** (**KÃ¶tÃ¼ KÃ¼meleme**). |

**Ä°ÅŸleyiÅŸ:**
1.  $K$ deÄŸerleri bir aralÄ±kta (Ã¶rneÄŸin $K=2$'den baÅŸlayarak) denenir.
2.  Her bir $K$ deÄŸeri iÃ§in tÃ¼m veri noktalarÄ±nÄ±n **ortalama SilÃ¼et Skoru** hesaplanÄ±r.

**Ã‡Ã¶zÃ¼m:**
* **En yÃ¼ksek** ortalama SilÃ¼et Skoru'nu veren $K$ deÄŸeri, genellikle en iyi kÃ¼me sayÄ±sÄ±nÄ± gÃ¶sterir.


# ğŸ¥‹ YÃ¶ntemlerin KarÅŸÄ±laÅŸtÄ±rmalÄ± Ã–zeti

| Kriter | Dirsek YÃ¶ntemi (Elbow Method) ğŸ“‰ | SilÃ¼et Skoru (Silhouette Score) ğŸ“Š |
| :--- | :--- | :--- |
| **Ã–lÃ§Ã¼len Metrik** | KÃ¼me Ä°Ã§i Kareler ToplamÄ± (WCSS) | KÃ¼me Ä°Ã§i Uyum ve KÃ¼meler ArasÄ± AyrÄ±m Dengesi |
| **Aranan DeÄŸer** | DÃ¼ÅŸÃ¼ÅŸÃ¼n yavaÅŸladÄ±ÄŸÄ± "**Dirsek NoktasÄ±**" | En yÃ¼ksek skoru veren $K$ deÄŸeri ($\approx +1$) |
| **Odak NoktasÄ±** | KÃ¼melerin ne kadar **sÄ±kÄ±** (kompakt) olduÄŸu. | KÃ¼melerin hem **sÄ±kÄ±** hem de **ayrÄ±k** olduÄŸu. |
| **Dezavantaj** | Bazen "**dirsek**" net olmayabilir, yoruma aÃ§Ä±ktÄ±r. | HesaplamasÄ±, WCSS'e gÃ¶re daha maliyetlidir (Ã¶zellikle bÃ¼yÃ¼k veri setlerinde). |
| **Ne Zaman KullanÄ±lÄ±r?** | HÄ±zlÄ± bir ilk tahmin ve gÃ¶rsel sezgi gerektiÄŸinde. | Daha kesin bir sonuÃ§ istendiÄŸinde ve kÃ¼me Ã¶rtÃ¼ÅŸmesi olup olmadÄ±ÄŸÄ± kontrol edilmek istendiÄŸinde. |


---

## Ek Ã‡Ã¶zÃ¼m: Alan Bilgisi (Domain Knowledge) ğŸ§ 

BazÄ± durumlarda $K$ deÄŸerini belirlemek iÃ§in istatistiksel yÃ¶ntemlere ihtiyaÃ§ duyulmaz:

* **Ã–rnek:** Bir e-ticaret ÅŸirketi mÃ¼ÅŸterilerini **"Gold", "Silver", "Bronze"** olarak gruplamak istiyorsa, bu durumda $K$ deÄŸeri alan bilgisine gÃ¶re direkt olarak **$K=3$** olarak belirlenir.

Bu yÃ¶ntemler, "You must choose K beforehand" ($K$'yÄ± Ã¶nceden seÃ§melisiniz) zorunluluÄŸunu bilimsel bir yaklaÅŸÄ±mla Ã§Ã¶zerek, modelin gerÃ§ek veri yapÄ±sÄ±na en uygun $K$ deÄŸerini bulmasÄ±nÄ± saÄŸlar.



# ğŸ”¬ Clustering: K-Means DeÄŸerlendirmesi (Evaluation)

KÃ¼meleme (Clustering) algoritmalarÄ± doÄŸasÄ± gereÄŸi **gÃ¶zetimsizdir** (Unsupervised). Bu, elimizde verinin doÄŸru kÃ¼me atamalarÄ±nÄ± (ground-truth labels) gÃ¶steren etiketlerin **olmadÄ±ÄŸÄ±** anlamÄ±na gelir. Bu yÃ¼zden kÃ¼meleme modelini deÄŸerlendirirken:

* SÄ±nÄ±flandÄ±rmada kullandÄ±ÄŸÄ±mÄ±z doÄŸruluk (accuracy) veya kesinlik (precision) gibi **harici (external) metriklere deÄŸil**,
* KÃ¼melerin yapÄ±sÄ±nÄ± kendi iÃ§inde Ã¶lÃ§en **dahili (internal) metriklere** gÃ¼venmek zorundayÄ±z.

Bu dahili metriklerin en popÃ¼ler ikisi **Inertia** ve **Silhouette Skoru**'dur.

---

## Inertia (Eylemsizlik): KÃ¼me SÄ±kÄ±lÄ±ÄŸÄ±nÄ±n Ã–lÃ§Ã¼sÃ¼ ğŸ¯

**Inertia**, bir K-Means kÃ¼melemesinin ne kadar **kompakt** ve **sÄ±kÄ±** olduÄŸunu Ã¶lÃ§en temel metriktir.

### Inertia Nedir?

BasitÃ§e ifade etmek gerekirse:
> **Inertia**, veri noktalarÄ±nÄ±n kendi kÃ¼me merkezlerinden (centroid) ne kadar uzakta olduÄŸunun bir Ã¶lÃ§Ã¼sÃ¼dÃ¼r.

Daha teknik ifadeyle:
* Inertia, her bir veri noktasÄ±nÄ±n ait olduÄŸu kÃ¼menin merkezine olan uzaklÄ±ÄŸÄ±nÄ±n **karelerinin toplamÄ±dÄ±r** (*Sum of Squared Distances - SSD* veya *Within-Cluster Sum of Squares - WCSS* olarak da bilinir).

### Neden Inertia'yÄ± KullanÄ±rÄ±z?

K-Means'in ana amacÄ± mesafeyi minimize etmektir. Inertia, bu amaca ne kadar ulaÅŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir:

* **Daha DÃ¼ÅŸÃ¼k Inertia** ğŸ‘‡: KÃ¼meler daha **sÄ±kÄ±** ve daha **kompakt** oluÅŸmuÅŸtur. KÃ¼me iÃ§i benzerlik yÃ¼ksektir. (**Ä°yi KÃ¼meleme**)
* **Daha YÃ¼ksek Inertia** ğŸ‘†: Noktalar merkezden uzaktadÄ±r. KÃ¼meler muhtemelen **daÄŸÄ±nÄ±k** ve **kÃ¶tÃ¼ ÅŸekillidir**. (**KÃ¶tÃ¼ KÃ¼meleme**)

ğŸ‘‰ **AkÄ±lda KalmasÄ± Ä°Ã§in**: Inertia, her bir kÃ¼menin ne kadar **eÅŸ merkezli (cohesive)** olduÄŸunu Ã¶lÃ§er.

### ğŸ’¡ GÃ¼ncel Piyasa Ã–rneÄŸi: MÃ¼ÅŸteri Segmentasyonu

* Bir e-ticaret ÅŸirketi mÃ¼ÅŸterilerini $K=4$ gruba ayÄ±rdÄ±.
* **K=4 Modeli Inertia = 50.000**
* **K=3 Modeli Inertia = 90.000**
* **Yorum**: $K=4$ modeli, $K=3$'e gÃ¶re daha dÃ¼ÅŸÃ¼k Inertia'ya sahiptir. Bu, 4 kÃ¼menin mÃ¼ÅŸterileri **daha sÄ±kÄ± (homojen)** gruplara ayÄ±rdÄ±ÄŸÄ± anlamÄ±na gelir.

---

## Inertia'nÄ±n Kritik KÄ±sÄ±tlamasÄ± ve Ã‡Ã¶zÃ¼mÃ¼ âš ï¸

Inertia, tek baÅŸÄ±na **optimal $K$ sayÄ±sÄ±nÄ± seÃ§mek** iÃ§in doÄŸrudan kullanÄ±lamaz. Bunun temel nedeni:

* **K Artarsa, Inertia Daima AzalÄ±r.**
* MantÄ±k: KÃ¼me sayÄ±sÄ±nÄ± artÄ±rmak, her noktayÄ± merkezine daha da yaklaÅŸtÄ±rÄ±r. (Extreme durumda, $K=N$ olduÄŸunda Inertia sÄ±fÄ±rdÄ±r.)

Bu nedenle, sadece en dÃ¼ÅŸÃ¼k Inertia'yÄ± seÃ§mek, bizi her zaman yÃ¼ksek bir $K$ deÄŸerine (yani aÅŸÄ±rÄ± Ã¶ÄŸrenmeye - **overfitting'e**) gÃ¶tÃ¼rÃ¼r.

Bu kÄ±sÄ±tlamayÄ± aÅŸmak iÃ§in kullanÄ±lan yÃ¶ntemler:

* **Dirsek YÃ¶ntemi (Elbow Method):** Inertia deÄŸerinin dÃ¼ÅŸÃ¼ÅŸÃ¼nÃ¼n aniden yavaÅŸladÄ±ÄŸÄ±, yani eklenen her kÃ¼menin getirdiÄŸi faydanÄ±n azalmaya baÅŸladÄ±ÄŸÄ± "**dirsek**" noktasÄ±nÄ± bularak en iyi $K$ sayÄ±sÄ±nÄ± Ã¶nerir.

---

## Silhouette Skoru (Ä°puÃ§larÄ±) ğŸŒŸ

DiÄŸer Ã¶nemli metrik olan SilÃ¼et Skoru:

* Hem kÃ¼me **sÄ±kÄ±lÄ±ÄŸÄ±nÄ±** hem de kÃ¼meler **arasÄ± ayrÄ±mÄ±** (ne kadar uzakta olduklarÄ±nÄ±) aynÄ± anda Ã¶lÃ§er.
* **Ã‡ifte RolÃ¼**: SilÃ¼et, hem farklÄ± $K$ deÄŸerlerini karÅŸÄ±laÅŸtÄ±rarak **en iyi $K$ sayÄ±sÄ±nÄ± seÃ§mek** iÃ§in (Model SeÃ§imi) hem de seÃ§ilen son modelin kalitesini **raporlamak** iÃ§in (DeÄŸerlendirme) kullanÄ±lÄ±r.

Bu metrikleri anlamak, yaptÄ±ÄŸÄ±nÄ±z kÃ¼melemenin iÅŸ hedeflerinize ne kadar uygun olduÄŸunu **ispat etmeniz** iÃ§in hayati Ã¶nem taÅŸÄ±r.

# ğŸ’¡ K-Means: Optimal $K$ DeÄŸerini Belirleme ZorluÄŸu

Åimdiye kadar K-Means'in nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k: veri noktalarÄ±nÄ± $K$ adet kÃ¼meye ayÄ±rmaya Ã§alÄ±ÅŸÄ±r.

Ancak bÃ¼yÃ¼k bir soru var: ğŸ‘‰ **DoÄŸru $K$ deÄŸerini nasÄ±l seÃ§eriz?**

* EÄŸer **$K$ Ã§ok kÃ¼Ã§Ã¼k** seÃ§ilirse, Ã§ok farklÄ± gruplar bir araya toplanabilir.
* EÄŸer **$K$ Ã§ok bÃ¼yÃ¼k** seÃ§ilirse, algoritma veriyi "aÅŸÄ±rÄ± bÃ¶lebilir" (**over-split**), bu da anlamsÄ±z kÃ¼melere yol aÃ§abilir.

---

## 1. Dirsek YÃ¶ntemi (The Elbow Method) ğŸ“‰

Dirsek YÃ¶ntemi, daha fazla kÃ¼me eklemenin Ã§ok fazla iyileÅŸme saÄŸlamadÄ±ÄŸÄ± noktayÄ± bulmamÄ±za yardÄ±mcÄ± olur.

### Temel Metrik: Inertia (WCSS)

* **Inertia**: KÃ¼melerin ne kadar **daÄŸÄ±nÄ±k** olduÄŸunu Ã¶lÃ§er (daha dÃ¼ÅŸÃ¼k olmasÄ± daha iyidir).
* **Ä°ÅŸleyiÅŸ**: Inertia deÄŸerini, $K$ (kÃ¼me sayÄ±sÄ±) deÄŸerine karÅŸÄ± grafiklendiririz.

### Ã‡Ã¶zÃ¼m

Grafikte, eÄŸrinin dÃ¼zleÅŸmeye baÅŸladÄ±ÄŸÄ± veya gÃ¶rsel olarak bir **"dirseÄŸe"** benzediÄŸi noktayÄ± ararÄ±z. Bu nokta, eklenen her kÃ¼menin getirdiÄŸi faydanÄ±n (Inertia'daki dÃ¼ÅŸÃ¼ÅŸÃ¼n) azalmaya baÅŸladÄ±ÄŸÄ± **optimal $K$ deÄŸeri** olarak kabul edilir.

<img width="578" height="382" alt="image" src="https://github.com/user-attachments/assets/6965a3a8-9cc5-4188-b8b7-bf5ee41fa563" />

---

## 2. SilÃ¼et Skoru (The Silhouette Score) ğŸŒŸ

Optimal $K$ deÄŸerini belirlemenin bir baÅŸka yolu da **SilÃ¼et Skoru**'dur. Bu metrik, bir veri noktasÄ±nÄ±n kendi kÃ¼mesine ne kadar iyi uyduÄŸunu, diÄŸer kÃ¼melere kÄ±yasla Ã¶lÃ§er. Bu, aynÄ± zamanda en uygun $K$ sayÄ±sÄ±nÄ± seÃ§memize de yardÄ±mcÄ± olur.

### DeÄŸer AralÄ±ÄŸÄ±

SilÃ¼et Skoru, **$-1$ ile $+1$** arasÄ±nda deÄŸiÅŸir.

* **$+1$'e yakÄ±n** ğŸš€: Veri noktalarÄ± kendi kÃ¼meleri iÃ§inde Ã§ok iyi kÃ¼melenmiÅŸ ve diÄŸer kÃ¼melerden uzaktÄ±r. (**YÃ¼ksek Kaliteli KÃ¼meleme**)
* **$0$'a yakÄ±n** ğŸ§­: KÃ¼meler Ã§ok fazla **Ã¶rtÃ¼ÅŸÃ¼yor** veya veri noktasÄ± iki kÃ¼menin sÄ±nÄ±rÄ±nda.
* **Negatif deÄŸerler** âŒ: Veri noktalarÄ±nÄ±n **yanlÄ±ÅŸ kÃ¼mede** olabileceÄŸi anlamÄ±na gelir.

**AmaÃ§**: FarklÄ± $K$ deÄŸerlerini denerken **en yÃ¼ksek** ortalama SilÃ¼et Skorunu elde eden $K$'yÄ± seÃ§mektir.

<img width="524" height="337" alt="image" src="https://github.com/user-attachments/assets/3e33fcc3-30a6-442d-95b3-c30d92c06dc5" />

# ğŸ“Š SilÃ¼et Skoru Analizi ve K SeÃ§imi (Model Selection)

---

## SilÃ¼et Skoru (The Silhouette Score) Ä°ncelemesi ğŸŒŸ

**SilÃ¼et Skoru**, kÃ¼melerin ne kadar **iyi ayrÄ±lmÄ±ÅŸ (well-separated)** ve ne kadar **kompakt (compact)** olduÄŸunu aynÄ± anda Ã¶lÃ§er.

| Skor | AnlamÄ± | DeÄŸerlendirme |
| :--- | :--- | :--- |
| **Daha YÃ¼ksek** | KÃ¼meler belirgin ve iyi ÅŸekillendirilmiÅŸ. | **Daha Ä°yi** âœ… |
| **Daha DÃ¼ÅŸÃ¼k** | KÃ¼meler Ã¶rtÃ¼ÅŸÃ¼yor veya daÄŸÄ±nÄ±k (messy). | **Daha KÃ¶tÃ¼** âŒ |

### Grafik Yorumlama Ã–rneÄŸi ğŸ“ˆ

* **$K=2$:** SilÃ¼et Skoru en yÃ¼ksek ($\approx 0.52$). Bu, 2 kÃ¼me ile verinin **en temiz** ÅŸekilde ayrÄ±ldÄ±ÄŸÄ± anlamÄ±na gelir.
* **$K=3$:** Skor hala oldukÃ§a yÃ¼ksek ($\approx 0.50$). Bu, 3 kÃ¼menin de **makul** bir seÃ§im olduÄŸunu gÃ¶sterir.
* **$K > 3$:** $K$, 3'Ã¼n Ã¼zerine Ã§Ä±ktÄ±kÃ§a SilÃ¼et Skoru sÃ¼rekli dÃ¼ÅŸer. Bu, kÃ¼melerin **daha az belirginleÅŸtiÄŸi** ve daha fazla Ã¶rtÃ¼ÅŸtÃ¼ÄŸÃ¼ anlamÄ±na gelir.

**âœ… SonuÃ§:** Bu analize gÃ¶re, gruplarÄ±n en net ayrÄ±mÄ±nÄ± saÄŸladÄ±klarÄ± iÃ§in $K$ iÃ§in en iyi seÃ§imler **2 veya 3 kÃ¼medir**.

---

## â˜ğŸ½ Ã–zet: Optimal K DeÄŸerini SeÃ§mek

**$K$ sayÄ±sÄ±nÄ± seÃ§mek** bir miktar sanat, bir miktar bilimdir. En iyi sonucu almak iÃ§in farklÄ± yaklaÅŸÄ±mlarÄ±n birleÅŸtirilmesi gerekir:

* **âœ… Dirsek YÃ¶ntemi (Elbow Method):** Inertia eÄŸrisindeki **bÃ¼kÃ¼lme (bend)** noktasÄ±nÄ± arayÄ±n.
* **âœ… SilÃ¼et Skoru (Silhouette Score):** En yÃ¼ksek **ortalama kÃ¼me ayrÄ±mÄ±na** sahip olan $K$'yÄ± bulun.

GerÃ§ek projelerde, genellikle her iki yÃ¶ntemi (ve diÄŸerlerini) dener ve bu sonuÃ§larÄ± **alan bilginizle** (domain knowledge) birleÅŸtirirsiniz (Ã–rn: "Penguen veri setinde 3 penguen tÃ¼rÃ¼ olduÄŸunu biliyoruz").

# ğŸŒ³ Hierarchical Clustering (HiyerarÅŸik KÃ¼meleme)

Åimdiye kadar, algoritmayÄ± Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce **kÃ¼me sayÄ±sÄ± $K$**â€™yÄ± seÃ§meniz gereken **K-Means**'e baktÄ±k.

Peki, kÃ¼me sayÄ±sÄ±nÄ±n kaÃ§ olmasÄ± gerektiÄŸini **bilmiyorsanÄ±z** ne olur? Veya kÃ¼melerin farklÄ± **ayrÄ±ntÄ±lÄ±lÄ±k (granularity)** seviyelerinde nasÄ±l birleÅŸtiÄŸini ve bÃ¶lÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶rmek isterseniz?

Ä°ÅŸte bu noktada **HiyerarÅŸik KÃ¼meleme** devreye girer. HiyerarÅŸik KÃ¼meleme, kullanÄ±cÄ±nÄ±n **Ã¶nceden kÃ¼me sayÄ±sÄ±nÄ± belirtmesini gerektirmez**.

---

## Hierarchical Clustering Nedir?

HiyerarÅŸik KÃ¼meleme, adÄ±ndan da anlaÅŸÄ±lacaÄŸÄ± gibi, bir kÃ¼me **aÄŸacÄ±** oluÅŸturur. Tek bir sabit kÃ¼me sayÄ±sÄ±na karar vermek yerine, veri noktalarÄ±nÄ±n adÄ±m adÄ±m nasÄ±l gruplandÄ±rÄ±labileceÄŸini size gÃ¶sterir.

### Ä°ki Ana YaklaÅŸÄ±m

| YaklaÅŸÄ±m | YÃ¶n | BaÅŸlangÄ±Ã§ Durumu | Ä°ÅŸlem AdÄ±mÄ± | Son Durum |
| :--- | :--- | :--- | :--- | :--- |
| **Agglomerative** (Bottom-up) | â¬†ï¸ AÅŸaÄŸÄ±dan YukarÄ± | Her veri noktasÄ± kendi kÃ¼mesidir. | En yakÄ±n iki kÃ¼meyi **birleÅŸtir**. | Her ÅŸey tek bir bÃ¼yÃ¼k kÃ¼mede birleÅŸir. |
| **Divisive** (Top-down) | â¬‡ï¸ YukarÄ±dan AÅŸaÄŸÄ± | TÃ¼m noktalar tek bir kÃ¼mededir. | KÃ¼meyi adÄ±m adÄ±m daha kÃ¼Ã§Ã¼k gruplara **bÃ¶l**. | Her nokta kendi kÃ¼mesidir. |

ğŸ‘‰ **Pratikte:** Uygulamada **Agglomerative (AÅŸaÄŸÄ±dan YukarÄ±) kÃ¼meleme** Ã§ok daha sÄ±k kullanÄ±lÄ±r.

<img width="674" height="308" alt="image" src="https://github.com/user-attachments/assets/7ad3c28c-1d9a-47e3-af5a-4503c0cb55bb" />

# ğŸŒ³ Dendrogram: HiyerarÅŸik KÃ¼melemenin GÃ¶rselleÅŸtirilmesi

HiyerarÅŸik KÃ¼meleme'nin sonucu, kÃ¼melerin nasÄ±l birleÅŸtiÄŸini gÃ¶steren aÄŸaÃ§ benzeri bir diyagram olan **dendrogram** ile gÃ¶rselleÅŸtirilir.

### Dendrogram YapÄ±sÄ±

* **Dikey Eksen (Vertical Axis)** ğŸ“: **Mesafe** (veya benzemezlik/dissimilarity). YÃ¼ksekten kesmek, kÃ¼meler arasÄ± bÃ¼yÃ¼k bir mesafeyi kabul etmek demektir.
* **Yatay Eksen (Horizontal Axis)** ğŸ“Š: Veri noktalarÄ± veya kÃ¼meler.

### KÃ¼me SayÄ±sÄ±nÄ±n Belirlenmesi

* AÄŸacÄ± seÃ§ilen bir yÃ¼kseklikte **"keserek"**, kaÃ§ kÃ¼me tutacaÄŸÄ±nÄ±za karar verirsiniz.
* **Ã–rnek**: DendrogramÄ± belirli bir seviyede kesmek size 2 bÃ¼yÃ¼k kÃ¼me verebilirken, daha aÅŸaÄŸÄ±dan kesmek size 5 daha kÃ¼Ã§Ã¼k kÃ¼me verebilir.
    * 

---

## KÃ¼meler ArasÄ± Mesafeyi Ã–lÃ§me (Linkage Criteria) ğŸ”—

KÃ¼meler arasÄ±ndaki mesafe, farklÄ± ÅŸekillerde tanÄ±mlanabilir. Bu yÃ¶ntemlere **baÄŸlantÄ± kriterleri (linkage criteria)** denir:

| Kriter | AÃ§Ä±klama | Odak NoktasÄ± |
| :--- | :--- | :--- |
| **Single Linkage** | Her kÃ¼medeki **en yakÄ±n iki nokta** arasÄ±ndaki mesafe. | En kÄ±sa mesafeyle birleÅŸme. |
| **Complete Linkage** | Her kÃ¼medeki **en uzak iki nokta** arasÄ±ndaki mesafe. | KÃ¼melerin maksimum yayÄ±lÄ±mÄ±nÄ± kontrol etme. |
| **Average Linkage** | TÃ¼m nokta **Ã§iftleri** arasÄ±ndaki mesafelerin ortalamasÄ±. | Dengeli bir ortalama mesafe. |
| **Ward's Method** | KÃ¼me iÃ§indeki **varyansÄ± (daÄŸÄ±lÄ±mÄ±) minimize etmeyi** amaÃ§lar. | KÃ¼me sÄ±kÄ±lÄ±ÄŸÄ±nÄ± artÄ±rma (Ã‡ok popÃ¼lerdir). |

<img width="346" height="471" alt="image" src="https://github.com/user-attachments/assets/ae673f36-d073-4e70-ac5a-dde118e9dffa" />


# ğŸŒ² HiyerarÅŸik KÃ¼melemenin (HC) AvantajlarÄ± ve KÄ±sÄ±tlamalarÄ±

HiyerarÅŸik KÃ¼meleme, K-Means'in zorlandÄ±ÄŸÄ± durumlar iÃ§in gÃ¼Ã§lÃ¼ bir alternatif sunar, Ã¶zellikle veride doÄŸal bir hiyerarÅŸi olduÄŸundan ÅŸÃ¼phelenildiÄŸinde tercih edilir.

# âœ… Avantajlar (Pros) of Hierarchical Clustering

| Avantaj | AÃ§Ä±klama | K-Means'ten ÃœstÃ¼nlÃ¼ÄŸÃ¼ | Piyasa Ã–rneÄŸi ğŸ’¡ |
| :--- | :--- | :--- | :--- |
| **Ã–nceden $K$ Belirleme ZorunluluÄŸu Yok** | AlgoritmayÄ± Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce kaÃ§ kÃ¼me olacaÄŸÄ±nÄ± sÃ¶ylemeniz gerekmez. | **K-Means'in Ana ZorluÄŸunu Giderir** | **Biyoloji/Genetik:** Bilim insanlarÄ±, gen ifadesi verilerinde kaÃ§ genetik alt grubun olduÄŸunu bilmeden analize baÅŸlayabilir. |
| **GÃ¶rsel Yorumlama (Dendrogram)** | SonuÃ§larÄ±, kÃ¼melerin nasÄ±l birleÅŸtiÄŸini gÃ¶steren net bir aÄŸaÃ§ diyagramÄ± (**dendrogram**) ile sunar. | **Modelin Sezgisel AnlaÅŸÄ±lÄ±rlÄ±ÄŸÄ±nÄ± ArtÄ±rÄ±r** | **MÃ¼ÅŸteri Segmentasyonu:** Pazarlama ekibi, mÃ¼ÅŸterilerin bÃ¼yÃ¼k ana gruplardan alt segmentlere nasÄ±l ayrÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rsel olarak takip ederek stratejilerini daha iyi kurabilir. |
| **HiyerarÅŸik YapÄ± BaÅŸarÄ±sÄ±** | KÃ¼melerin iÃ§ iÃ§e (nested) olduÄŸu veya bir soy aÄŸacÄ± gibi bir hiyerarÅŸiye sahip olduÄŸu durumlarda Ã§ok iyi Ã§alÄ±ÅŸÄ±r. | **DoÄŸal Ä°liÅŸkileri Ortaya Ã‡Ä±karÄ±r** | **Taksonomi:** TÃ¼rlerin ve alt tÃ¼rlerin evrimsel iliÅŸkilerini ve sÄ±nÄ±flandÄ±rmasÄ±nÄ± modellemek iÃ§in idealdir. |

# âŒ KÄ±sÄ±tlamalar (Limitations) of Hierarchical Clustering

| KÄ±sÄ±tlama | AÃ§Ä±klama | K-Means'ten DezavantajÄ± | Uygulama Etkisi |
| :--- | :--- | :--- | :--- |
| **Hesaplama Maliyeti (Complexity)** ğŸ¢ | Algoritma, her adÄ±mda mesafeleri hesapladÄ±ÄŸÄ± iÃ§in bÃ¼yÃ¼k veri setlerinde **daha yavaÅŸtÄ±r**. | **BÃ¼yÃ¼k Ã–lÃ§ekte Ã‡alÄ±ÅŸmayÄ± ZorlaÅŸtÄ±rÄ±r** | Milyonlarca mÃ¼ÅŸterisi olan bir platformda gerÃ§ek zamanlÄ± analiz zordur. |
| **GÃ¼rÃ¼ltÃ¼ye ve AykÄ±rÄ± DeÄŸerlere Hassasiyet** ğŸ‘‚ | Ã–zellikle "Single Linkage" gibi yÃ¶ntemler, tek bir aykÄ±rÄ± deÄŸerin kÃ¼meleri yanlÄ±ÅŸlÄ±kla birbirine baÄŸlamasÄ±na ("**chaining**" etkisi) neden olabilir. | **SaÄŸlamlÄ±ÄŸÄ± (Robustness) AzaltÄ±r** | Veri Ã¶n iÅŸleme (outlier removal) adÄ±mÄ±, bu yÃ¶ntemde kritik Ã¶nem taÅŸÄ±r. |
| **Geri DÃ¶nÃ¼ÅŸ Yok (Irreversible Merges)** ğŸš« | Bir hata yapÄ±lÄ±p iki kÃ¼me birleÅŸtirildiÄŸinde (Agglomerative), bir sonraki adÄ±mda bu birleÅŸme **geri alÄ±namaz**. | **Hata DÃ¼zeltme EsnekliÄŸini SÄ±nÄ±rlar** | YanlÄ±ÅŸ bir birleÅŸme kararÄ±, sonraki tÃ¼m kÃ¼me yapÄ±sÄ±nÄ± bozabilir. |

# ğŸŒŸ Uzman GÃ¶rÃ¼ÅŸÃ¼: Ne Zaman HiyerarÅŸik KÃ¼melemeyi (HC) Tercih Etmeliyiz?

KÃ¼meleme yÃ¶ntemi seÃ§imi, projenizin hedeflerine ve veri setinizin Ã¶zelliklerine baÄŸlÄ±dÄ±r:

| SeÃ§im Kriteri | K-Means (HÄ±zlÄ±) ğŸï¸ | HiyerarÅŸik KÃ¼meleme (DetaylÄ±) ğŸŒ² |
| :--- | :--- | :--- |
| **Ana Hedef** | Net bir kÃ¼me sayÄ±sÄ± (Ã–rn: $K=4$ segment) ve **hÄ±z**. | KÃ¼meler arasÄ±ndaki **iliÅŸki haritasÄ±nÄ±** ve **doÄŸal hiyerarÅŸiyi** keÅŸfetmek. |
| **Veri Boyutu** | BÃ¼yÃ¼k veri setleri. | Nispeten **kÃ¼Ã§Ã¼k** veri setleri. |
| **GÃ¶rselleÅŸtirme** | KÃ¼me merkezleri ve iki boyutlu ayrÄ±m (PCA/t-SNE ile). | **Dendrogram** Ã¼zerinde kÃ¼me sayÄ±sÄ±nÄ±n etkisini ve birleÅŸmeleri gÃ¶rselleÅŸtirme. |

**Ã–zet:**

* EÄŸer hedefiniz **net bir kÃ¼me sayÄ±sÄ±** ve **hÄ±z** ise, **K-Means** idealdir.
* Ancak, veri setiniz **nispeten kÃ¼Ã§Ã¼k** ise ve asÄ±l hedefiniz, kÃ¼meler arasÄ±ndaki **iliÅŸki haritasÄ±nÄ±** Ã§Ä±karmak, veri yapÄ±sÄ±ndaki **doÄŸal hiyerarÅŸiyi** keÅŸfetmek ve kÃ¼me sayÄ±sÄ±nÄ±n etkisini dendrogram Ã¼zerinde gÃ¶rselleÅŸtirmek ise, **HiyerarÅŸik KÃ¼meleme** vazgeÃ§ilmez bir araÃ§tÄ±r.
