# ğŸ’¡ Features Preparation


* Features are the inputs you give to a machine learning model so it can make predictions. Theyâ€™re the measurable properties or characteristics that describe each data point.

  <img width="563" height="329" alt="image" src="https://github.com/user-attachments/assets/07bb778d-d2af-42d1-9119-bd9be9d612ef" />

  * Makine Ã–ÄŸreniminde (ML) bir veri sÃ¼tununu veya deÄŸiÅŸkenini ifade etmek iÃ§in kullanÄ±lan pek Ã§ok terim vardÄ±r ve bunlar baÄŸlama gÃ¶re hafifÃ§e farklÄ± anlamlara gelebilir.

Ä°ÅŸte Feature (Ã–zellik) kelimesi ile aynÄ± veya benzer kavramlarÄ± ifade eden temel terimleri karÅŸÄ±laÅŸtÄ±rmalÄ± olarak gÃ¶steren bir tablo.

## ğŸ’¡ ML'de 'Feature' (Ã–zellik) Yerine KullanÄ±lan Teknik Terimler

| Terim (Ä°ngilizce) | TÃ¼rkÃ§e KarÅŸÄ±lÄ±ÄŸÄ± | ML BaÄŸlamÄ±ndaki AnlamÄ± | Neden Bu Terim KullanÄ±lÄ±r? |
| :--- | :--- | :--- | :--- |
| **Feature** ğŸ“Š | **Ã–zellik** | Modelin bir tahmin yapmak iÃ§in kullandÄ±ÄŸÄ± **tek bir Ã¶lÃ§Ã¼lebilir deÄŸiÅŸken** (sÃ¼tun). ML'deki en yaygÄ±n terimdir. | Bir nesnenin veya durumun ayÄ±rt edici niteliÄŸini belirtmek iÃ§in kullanÄ±lÄ±r. Modelin girdi deÄŸerleridir. |
| **Variable** | **DeÄŸiÅŸken** | Bir veri setindeki herhangi bir sÃ¼tun veya nitelik. Ä°statistik ve veri analizinde en sÄ±k kullanÄ±lan genel terimdir. | DeÄŸeri deÄŸiÅŸebilen herhangi bir niceliÄŸi veya niteliÄŸi ifade etmek iÃ§in kullanÄ±lÄ±r. |
| **Attribute** | **Nitelik / Ã–znitelik** | Bir veri Ã¶rneÄŸinin (satÄ±rÄ±n) bir Ã¶zelliÄŸini tanÄ±mlayan sÃ¼tun. Ã–zellikle veri modelleme (data modeling) ve veritabanÄ± (database) terminolojisinde yaygÄ±ndÄ±r. | Bir nesnenin veya varlÄ±ÄŸÄ±n (entity) belirgin bir kalitesini veya Ã¶zelliÄŸini vurgular. |
| **Predictor** | **Tahmin Edici** | Ã–zellikle **Regresyon** modellerinde, tahmin edilmek istenen hedef deÄŸiÅŸkeni (**Target**) etkileyen girdi deÄŸiÅŸkeni. | AmacÄ±, hedef deÄŸeri tahmin etmeye yardÄ±mcÄ± olmak olduÄŸu iÃ§in iÅŸlevine odaklanÄ±r. |
| **Covariate** | **EÅŸ DeÄŸiÅŸken** | Ä°statistiksel modellemede, genellikle hedef deÄŸiÅŸken ile iliÅŸkili olan ve **kontrol edilen** girdi deÄŸiÅŸkeni. | Ã–zellikle deney tasarÄ±mÄ± ve istatistiksel hipotez testlerinde yaygÄ±ndÄ±r; modelin sonucunu etkileyen ikincil deÄŸiÅŸkenleri ifade eder. |
| **Input** | **Girdi** | Modele beslenen verinin genel birimi veya tek bir deÄŸiÅŸkeni. | Modelin iÅŸlem yapmasÄ± iÃ§in dÄ±ÅŸarÄ±dan alÄ±nan veriyi vurgulayan basit bir terimdir. |
| **Dimension** | **Boyut** | Veri setindeki toplam **Ã¶zellik (sÃ¼tun)** sayÄ±sÄ±nÄ± ifade eder. | Ã–zellikle Boyut Azaltma (Dimensionality Reduction) gibi tekniklerde, veri setinin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ifade etmek iÃ§in kullanÄ±lÄ±r. |

### ğŸ“ Ã–zet: ML Terimleri ArasÄ±ndaki Farklar

Bir ML projesinde, teknik olarak bir veri sÃ¼tununa genellikle **Feature (Ã–zellik)** denir. Ancak bu sÃ¼tun aynÄ± zamanda bir **Variable (DeÄŸiÅŸken)** veya **Attribute (Nitelik/Ã–znitelik)** olarak da adlandÄ±rÄ±labilir.

EÄŸer bu sÃ¼tun bir tahminde kullanÄ±lÄ±yorsa, ona **Predictor (Tahmin Edici)** demek de doÄŸrudur.

## ğŸ› ï¸ Ã–zellik HazÄ±rlamada Temel AdÄ±mlar (Feature Preparation Steps)

Ã–zellik HazÄ±rlama (**Feature Preparation**) ham veriyi temiz, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve tutarlÄ± bir forma dÃ¶nÃ¼ÅŸtÃ¼ren kritik sÃ¼reÃ§tir.

| AdÄ±m No. | AÅŸama AdÄ± (Ä°ngilizce Terim) | AmaÃ§ ve AÃ§Ä±klama | Somut Ã–rnekler |
| :---: | :--- | :--- | :--- |
| **1** â“ | **Handling Missing Data** (Eksik Veri YÃ¶netimi) | Veri setinde hiÃ§ veri giriÅŸi olmayan boÅŸ hÃ¼creleri (NaN) ele alma. Veri kaybÄ±nÄ± en aza indirerek veri setinin bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak. | **SayÄ±sal:** YaÅŸ (**Age**) verisindeki boÅŸluklarÄ±, ortalama (**mean**) veya medyan (**median**) ile doldurmak (**Imputation**).<br>**Kategorik:** Eksik deÄŸeri "**Bilinmiyor**" (**Unknown**) adÄ±nda yeni bir kategori olarak iÅŸaretlemek. |
| **2** â— | **Handling Outliers** (AykÄ±rÄ± DeÄŸer YÃ¶netimi) | Veri setinin geri kalanÄ±ndan Ã¶nemli Ã¶lÃ§Ã¼de farklÄ± olan aÅŸÄ±rÄ± deÄŸerleri tespit etmek ve dÃ¼zeltmek. Modelin bu uÃ§ deÄŸerlerden yanlÄ±ÅŸ Ã¶ÄŸrenmesini engellemek. | Gelir verisinde 1.000.000.000 USD gibi bir deÄŸerin tespiti. Bu deÄŸeri kaldÄ±rabilir veya kabul edilebilir bir Ã¼st sÄ±nÄ±rla (**capping**) deÄŸiÅŸtirebiliriz. |
| **3** ğŸ·ï¸ | **Handling Categorical Data** (Kategorik Veri YÃ¶netimi) | Metin tabanlÄ± kategorik Ã¶zellikleri (Ã–rn: ÅŸehir adlarÄ±, renkler) ML algoritmalarÄ±nÄ±n anlayabileceÄŸi sayÄ±sal formata Ã§evirme. | **Nominal:** "KÄ±rmÄ±zÄ±", "Mavi", "YeÅŸil" gibi sÄ±rasÄ±z renkler iÃ§in **One-Hot Encoding** kullanmak.<br>**Ordinal:** "KÃ¶tÃ¼", "Orta", "Ä°yi" gibi sÄ±ralÄ± derecelendirmeler iÃ§in **Label Encoding** kullanmak (1, 2, 3 gibi). |
| **4** âš–ï¸ | **Feature Scaling** (Ã–zellik Ã–lÃ§eklendirme) | SayÄ±sal Ã¶zelliklerin deÄŸer aralÄ±klarÄ±nÄ± ortak bir standarda getirmek. Modelin, bÃ¼yÃ¼k deÄŸer aralÄ±ÄŸÄ±na sahip Ã¶zelliklere haksÄ±z yere daha fazla Ã¶nem vermesini Ã¶nler. | **Normalizasyon (Normalization):** Veriyi 0 ile 1 arasÄ±na Ã¶lÃ§eklendirme.<br>**Standartizasyon (Standardization):** Veriyi ortalamasÄ± 0 ve standart sapmasÄ± 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rme. |
| **5** âœ¨ | **Feature Creation and Transformation** (Ã–zellik OluÅŸturma ve DÃ¶nÃ¼ÅŸtÃ¼rme) | Mevcut Ã¶zelliklerden yeni ve daha bilgilendirici Ã¶zellikler tÃ¼retme veya mevcut Ã¶zellikleri dÃ¶nÃ¼ÅŸtÃ¼rme. Modelin Ã¶ÄŸrenmesine yeni bakÄ±ÅŸ aÃ§Ä±larÄ± katmak. | MÃ¼ÅŸterinin doÄŸum tarihinden "**MÃ¼ÅŸteri YaÅŸÄ±**" veya "**MÃ¼ÅŸteri Olma SÃ¼resi**" gibi yeni bir Ã¶zellik tÃ¼retme. Ä°ki sÃ¼tunu Ã§arparak yeni bir etkileÅŸim terimi (**interaction term**) oluÅŸturma. |
| **6** ğŸ¯ | **Feature Selection** (Ã–zellik SeÃ§imi) | Tahmin hedefiyle en ilgili olan Ã¶zelliklerin alt kÃ¼mesini seÃ§me. AlakasÄ±z veya gereksiz Ã¶zellikleri kaldÄ±rarak modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ve eÄŸitim sÃ¼resini azaltmak. | Bir ev fiyatÄ± tahmini modelinde, adresin kapÄ± numarasÄ±nÄ± veya rengini kaldÄ±rÄ±p, sadece metrekare ve oda sayÄ±sÄ± gibi daha alakalÄ± Ã¶zelliklere odaklanmak. |
5.  **Veriyi AyÄ±rma (Splitting data) ğŸª“:** Veriyi eÄŸitim (training) ve test (test) setlerine bÃ¶lme.

> **UnutmayÄ±n:** BaÅŸarÄ±lÄ± bir ML projesinin temeli, daima **temizlenmiÅŸ ve doÄŸru ÅŸekilde hazÄ±rlanmÄ±ÅŸ** veriye dayanÄ±r.

## Neden Ã–zellik HazÄ±rlama Ã–nemlidir? (Why is Feature Preparation Important?) ğŸ’¡

Ham veri (**raw data**) nadiren kullanÄ±ma hazÄ±r bir formatta gelir. Genellikle ÅŸunlarÄ± iÃ§erir:

* **Eksik deÄŸerler** (**Missing values**) âŒ
* **Ä°lgisiz veya gereksiz bilgiler** (**Irrelevant or redundant information**) âŒ
* **SayÄ±sal Ã¶zellikler iÃ§in farklÄ± Ã¶lÃ§ekler** (Ã–rn: price in thousands vs. age in years). âŒ
* **KarÄ±ÅŸÄ±k formatlar** (text, numbers, categories). âŒ

Bu ham veriyi dÃ¼zeltmeden doÄŸrudan bir modele beslersek, sonuÃ§larÄ±:

* **DÃ¼ÅŸÃ¼k doÄŸruluk** (Poor accuracy).
* **Daha yavaÅŸ eÄŸitim** (Slower training).
* **Yeni veriyi genelleÅŸtiremeyen modeller** (Models that can't generalize to new data).

> ğŸ‘† **Ana Fikir:** Ã–zellik hazÄ±rlama (**Feature preparation**) daÄŸÄ±nÄ±k ham veriyi **temiz, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve tutarlÄ±** bir forma dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Bu, modelin daha hÄ±zlÄ± Ã¶ÄŸrenmesini, daha iyi performans gÃ¶stermesini ve yeni veriyi genellemesini saÄŸlar.

---

<img width="827" height="513" alt="image" src="https://github.com/user-attachments/assets/0214c465-8578-49a7-802a-4ffe3d6ef59d" />

<img width="801" height="477" alt="image" src="https://github.com/user-attachments/assets/9601f514-bb33-4be4-9bfd-bc982d76f904" />

<img width="804" height="560" alt="image" src="https://github.com/user-attachments/assets/79a34d9e-f0de-4afe-b28c-12a147ccd74d" />


<img width="920" height="459" alt="image" src="https://github.com/user-attachments/assets/884a702f-b29e-4350-bc86-ef002a69efc1" />


## âœ¨ Feature Preparation (Ã–zellik HazÄ±rlama) AdÄ±mlarÄ± ve YÃ¶ntemleri

| AdÄ±m No. | AÅŸama (Ä°ngilizce Terim) | Temel AmaÃ§ | Uygulanan BaÅŸlÄ±ca YÃ¶ntemler ve KarÅŸÄ±laÅŸtÄ±rmasÄ± |
| :---: | :--- | :--- | :--- |
| **1** â“ | **Handling Missing Data** (Eksik Veri YÃ¶netimi) | Eksik deÄŸerleri (NaN/Null) ele alarak veri setinin bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak ve modelin Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlamak. | **1. KaldÄ±rma (Deletion):** YalnÄ±zca Ã§ok az sayÄ±da satÄ±r/sÃ¼tun eksikse kullanÄ±lÄ±r. **2. Doldurma (Imputation):** Eksik veriyi istatistiksel deÄŸerlerle doldurmak: â€” **SayÄ±sal:** Ortalama (Mean), Medyan (Median) veya Grup OrtalamasÄ± (Group Mean) ile doldurma. â€” **Kategorik:** En sÄ±k tekrar eden deÄŸer (Mode) veya "Bilinmiyor" etiketi ile doldurma. |
| **2** â— | **Handling Outliers** (AykÄ±rÄ± DeÄŸer YÃ¶netimi) | AykÄ±rÄ± deÄŸerlerin (extreme values) modelin Ã¶ÄŸrenmesini bozmasÄ±nÄ± veya tahminleri Ã§arpÄ±tmasÄ±nÄ± engellemek. | **1. Tespit:** Kutu Grafikleri (Boxplots), Z-Skoru (Z-Score) veya IQR (Interquartile Range) metodu ile tespit etme. **2. Ele Alma:** â€” **KaldÄ±rma (Removal):** Verinin bir kÄ±smÄ±nÄ± kaybetme riskiyle kaldÄ±rma. â€” **KÄ±sÄ±tlama (Capping/Winsorization):** DeÄŸeri belirli bir Ã¼st veya alt eÅŸiÄŸe sabitleme. â€” **DÃ¶nÃ¼ÅŸtÃ¼rme (Transformation):** Logaritmik dÃ¶nÃ¼ÅŸÃ¼m (Log Transform) gibi yÃ¶ntemlerle daÄŸÄ±lÄ±mÄ± normale yakÄ±n hale getirme. |
| **3** ğŸ·ï¸ | **Handling Categorical Data** (Kategorik Veri YÃ¶netimi) | Metin tabanlÄ± kategorik Ã¶zellikleri (Nominal/Ordinal) ML algoritmalarÄ±nÄ±n anlayabileceÄŸi sayÄ±sal formata Ã§evirmek. | **1. One-Hot Encoding:** **Nominal** (sÄ±rasÄ±z) veriler iÃ§in idealdir. Her kategori iÃ§in yeni bir ikili sÃ¼tun oluÅŸturur (dummy variables). **2. Label Encoding:** **Ordinal** (sÄ±ralÄ±) veriler iÃ§in idealdir. Kategorilere rÃ¼tbelerine gÃ¶re sayÄ±sal deÄŸerler atar (Ã–rn: KÃ¶tÃ¼=1, Ä°yi=2). **3. Target Encoding:** YÃ¼ksek kardinaliteli (Ã§ok fazla farklÄ± deÄŸer iÃ§eren) kategorik veriler iÃ§in etkilidir. |
| **4** âš–ï¸ | **Feature Scaling** (Ã–zellik Ã–lÃ§eklendirme) | SayÄ±sal Ã¶zelliklerin deÄŸer aralÄ±klarÄ±nÄ± ortak bir standarda getirerek, modelin bir Ã¶zelliÄŸe diÄŸerinden haksÄ±z yere daha fazla Ã¶nem vermesini Ã¶nlemek. | **1. Normalizasyon (Min-Max Scaling):** DeÄŸerleri **0 ile 1** arasÄ±na Ã¶lÃ§eklendirir. AykÄ±rÄ± deÄŸerlere hassastÄ±r. **2. Standartizasyon (Z-Score Scaling):** Veriyi ortalamasÄ± **0** ve standart sapmasÄ± **1** olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. AykÄ±rÄ± deÄŸerlere karÅŸÄ± daha dayanÄ±klÄ±dÄ±r. |
| **5** âœ¨ | **Feature Creation and Transformation** (Ã–zellik OluÅŸturma ve DÃ¶nÃ¼ÅŸtÃ¼rme) | Modelin Ã¶ÄŸrenmesini geliÅŸtirecek yeni, daha anlamlÄ± Ã¶zellikler tÃ¼retmek veya mevcut Ã¶zelliklerin daÄŸÄ±lÄ±mÄ±nÄ± dÃ¼zeltmek. | **1. Ã–zellik Ã‡Ä±karma:** Mevcut sÃ¼tunlardan yeni bilgi tÃ¼retme (Ã–rn: DoÄŸum tarihinden **YaÅŸ** veya **MÃ¼ÅŸteri Olma SÃ¼resi**). **2. Ä°ndisleme (Binning):** SÃ¼rekli bir Ã¶zelliÄŸi kategorik aralÄ±klara ayÄ±rma (Ã–rn: MaaÅŸÄ± DÃ¼ÅŸÃ¼k, Orta, YÃ¼ksek gruplarÄ±na ayÄ±rma). **3. EtkileÅŸim Ã–zellikleri (Interaction Features):** Ä°ki farklÄ± Ã¶zelliÄŸin Ã§arpÄ±mÄ±ndan veya kombinasyonundan yeni bir Ã¶zellik oluÅŸturma. |
| **6** ğŸ¯ | **Feature Selection** (Ã–zellik SeÃ§imi) | Modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ve eÄŸitim sÃ¼resini azaltmak iÃ§in tahmin hedefiyle en ilgili olan Ã¶zelliklerin alt kÃ¼mesini seÃ§mek. | **1. Filtre YÃ¶ntemleri (Filter Methods):** Ä°statistiksel testlerle (korelasyon, Ki-kare) en iyi Ã¶zellikleri seÃ§me. **2. SarÄ±cÄ± YÃ¶ntemler (Wrapper Methods):** Modelin kendisini kullanarak en iyi Ã¶zellik kombinasyonunu arama (Ã–rn: RFE). **3. GÃ¶mÃ¼lÃ¼ YÃ¶ntemler (Embedded Methods):** Modelin eÄŸitim sÃ¼reci sÄ±rasÄ±nda Ã¶zellik Ã¶nemini belirlemesi (Ã–rn: Lasso). |



---
# ğŸ› ï¸ Data Preparation Techniques for Machine Learning

Data preparation is the crucial process of transforming raw data into a form more suitable for modeling. The required steps depend on the specific data and the algorithms to be used.

---

## The 5 Core Data Preparation Tasks

These tasks form the framework for preparing structured (tabular) data in a predictive modeling project:

### 1. Data Cleaning ğŸ§¹
* **Definition:** Identifying and correcting mistakes or errors in "messy" data.
* **Goal:** Ensure data quality and reliability.
* **Key Operations:**
    * Identifying and addressing **outliers** (using statistical methods).
    * Removing **duplicate rows** of data.
    * Identifying columns with zero variance (same value) and removing them.
    * Marking empty values as missing and **imputing** (filling) missing values using statistics (mean, median) or a learned model.

### 2. Feature Selection ğŸ¯
* **Definition:** Selecting a subset of input features that are most relevant to the target variable.
* **Goal:** Improve model performance and favor the simplest possible model by removing **irrelevant and redundant variables**.
* **Technique Groups:**
    * **Filter Methods:** Scoring input features (e.g., using correlation statistics) and selecting the top subset.
    * **Wrapper Methods:** Explicitly choosing features that result in the best-performing model (e.g., RFE).
    * **Intrinsic Methods:** Models that automatically select features during the fitting process.

### 3. Data Transforms ğŸ”„
* **Definition:** Changing the type or distribution of variables to meet algorithm requirements.
* **Key Transforms:**
    * **Encoding Categorical Data:** Converting non-numeric labels into a numeric form:
        * *One-Hot Transform:* For nominal (unordered) variables.
        * *Ordinal Transform:* For ordinal (ranked) variables.
    * **Scaling Numeric Data:** Adjusting the range of real-valued variables:
        * **Normalization:** Scaling a variable to a range between 0 and 1.
        * **Standardization:** Shifting data to a Standard Gaussian (mean of zero, std dev of one).
    * **Power Transform / Quantile Transform:** Changing the probability distribution of numerical variables (e.g., making the distribution more Gaussian).

### 4. Feature Engineering âœ¨
* **Definition:** Creating **new input variables** from the available data. This often requires deep **domain expertise**.
* **Goal:** Provide the model with a more straightforward perspective on the input data and add broader context.
* **Examples:**
    * Adding a **boolean flag** for a specific state (e.g., *IsWeekend*).
    * Deriving **summary statistics** (e.g., adding a global mean).
    * Decomposing complex variables (e.g., splitting a **date-time** into separate Day, Month, Year variables).

### 5. Dimensionality Reduction ğŸ“‰
* **Definition:** Creating a projection of the data into a **lower-dimensional space** while preserving the most important properties.
* **Goal:** Address the **"curse of dimensionality"** (when too many input variables lead to a sparse and unrepresentative sampling of the space).
* **Key Techniques:**
    * **Principal Component Analysis (PCA)**
    * **Singular Value Decomposition (SVD)**

> **Note:** Unlike Feature Selection, variables created by dimensionality reduction are not directly related to the original inputs, making the results difficult to interpret.

 [Source: ML Mastery ,Jason BrownleeÂ PhD](https://machinelearningmastery.com/data-preparation-techniques-for-machine-learning/)
