# Features Preparation


* Features are the inputs you give to a machine learning model so it can make predictions. Theyâ€™re the measurable properties or characteristics that describe each data point.

  <img width="563" height="329" alt="image" src="https://github.com/user-attachments/assets/07bb778d-d2af-42d1-9119-bd9be9d612ef" />

## ğŸ› ï¸ Ã–zellik HazÄ±rlamada Temel AdÄ±mlar (Feature Preparation Steps)

Ã–zellik HazÄ±rlama (**Feature Preparation**) ham veriyi temiz, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve tutarlÄ± bir forma dÃ¶nÃ¼ÅŸtÃ¼ren kritik sÃ¼reÃ§tir.

| AdÄ±m No. | AÅŸama AdÄ± (Ä°ngilizce Terim) | AmaÃ§ ve AÃ§Ä±klama | Somut Ã–rnekler |
| :---: | :--- | :--- | :--- |
| **1** â“ | **Handling Missing Data** (Eksik Veri YÃ¶netimi) | Veri setinde hiÃ§ veri giriÅŸi olmayan boÅŸ hÃ¼creleri (NaN) ele alma. Veri kaybÄ±nÄ± en aza indirerek veri setinin bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak. | **SayÄ±sal:** YaÅŸ (**Age**) verisindeki boÅŸluklarÄ±, ortalama (**mean**) veya medyan (**median**) ile doldurmak (**Imputation**).<br>**Kategorik:** Eksik deÄŸeri "**Bilinmiyor**" (**Unknown**) adÄ±nda yeni bir kategori olarak iÅŸaretlemek. |
| **2** â— | **Handling Outliers** (AykÄ±rÄ± DeÄŸer YÃ¶netimi) | Veri setinin geri kalanÄ±ndan Ã¶nemli Ã¶lÃ§Ã¼de farklÄ± olan aÅŸÄ±rÄ± deÄŸerleri tespit etmek ve dÃ¼zeltmek. Modelin bu uÃ§ deÄŸerlerden yanlÄ±ÅŸ Ã¶ÄŸrenmesini engellemek. | Gelir verisinde 1.000.000.000 USD gibi bir deÄŸerin tespiti. Bu deÄŸeri kaldÄ±rabilir veya kabul edilebilir bir Ã¼st sÄ±nÄ±rla (**capping**) deÄŸiÅŸtirebiliriz. |
| **3** ğŸ·ï¸ | **Handling Categorical Data** (Kategorik Veri YÃ¶netimi) | Metin tabanlÄ± kategorik Ã¶zellikleri (Ã–rn: ÅŸehir adlarÄ±, renkler) ML algoritmalarÄ±nÄ±n anlayabileceÄŸi sayÄ±sal formata Ã§evirme. | **Nominal:** "KÄ±rmÄ±zÄ±", "Mavi", "YeÅŸil" gibi sÄ±rasÄ±z renkler iÃ§in **One-Hot Encoding** kullanmak.<br>**Ordinal:** "KÃ¶tÃ¼", "Orta", "Ä°yi" gibi sÄ±ralÄ± derecelendirmeler iÃ§in **Label Encoding** kullanmak (1, 2, 3 gibi). |
| **4** âš–ï¸ | **Feature Scaling** (Ã–zellik Ã–lÃ§eklendirme) | SayÄ±sal Ã¶zelliklerin deÄŸer aralÄ±klarÄ±nÄ± ortak bir standarda getirmek. Modelin, bÃ¼yÃ¼k deÄŸer aralÄ±ÄŸÄ±na sahip Ã¶zelliklere haksÄ±z yere daha fazla Ã¶nem vermesini Ã¶nler. | **Normalizasyon (Normalization):** Veriyi 0 ile 1 arasÄ±na Ã¶lÃ§eklendirme.<br>**Standartizasyon (Standardization):** Veriyi ortalamasÄ± 0 ve standart sapmasÄ± 1 olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rme. |
| **5** âœ¨ | **Feature Creation and Transformation** (Ã–zellik OluÅŸturma ve DÃ¶nÃ¼ÅŸtÃ¼rme) | Mevcut Ã¶zelliklerden yeni ve daha bilgilendirici Ã¶zellikler tÃ¼retme veya mevcut Ã¶zellikleri dÃ¶nÃ¼ÅŸtÃ¼rme. Modelin Ã¶ÄŸrenmesine yeni bakÄ±ÅŸ aÃ§Ä±larÄ± katmak. | MÃ¼ÅŸterinin doÄŸum tarihinden "**MÃ¼ÅŸteri YaÅŸÄ±**" veya "**MÃ¼ÅŸteri Olma SÃ¼resi**" gibi yeni bir Ã¶zellik tÃ¼retme. Ä°ki sÃ¼tunu Ã§arparak yeni bir etkileÅŸim terimi (**interaction term**) oluÅŸturma. |
| **6** ğŸ¯ | **Feature Selection** (Ã–zellik SeÃ§imi) | Tahmin hedefiyle en ilgili olan Ã¶zelliklerin alt kÃ¼mesini seÃ§me. AlakasÄ±z veya gereksiz Ã¶zellikleri kaldÄ±rarak modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ve eÄŸitim sÃ¼resini azaltmak. | Bir ev fiyatÄ± tahmini modelinde, adresin kapÄ± numarasÄ±nÄ± veya rengini kaldÄ±rÄ±p, sadece metrekare ve oda sayÄ±sÄ± gibi daha alakalÄ± Ã¶zelliklere odaklanmak. |
5.  **Veriyi AyÄ±rma (Splitting data) ğŸª“:** Veriyi eÄŸitim (training) ve test (test) setlerine bÃ¶lme.

> **UnutmayÄ±n:** BaÅŸarÄ±lÄ± bir ML projesinin temeli, daima **temizlenmiÅŸ ve doÄŸru ÅŸekilde hazÄ±rlanmÄ±ÅŸ** veriye dayanÄ±r.




<img width="827" height="513" alt="image" src="https://github.com/user-attachments/assets/0214c465-8578-49a7-802a-4ffe3d6ef59d" />

<img width="801" height="477" alt="image" src="https://github.com/user-attachments/assets/9601f514-bb33-4be4-9bfd-bc982d76f904" />

<img width="804" height="560" alt="image" src="https://github.com/user-attachments/assets/79a34d9e-f0de-4afe-b28c-12a147ccd74d" />


<img width="920" height="459" alt="image" src="https://github.com/user-attachments/assets/884a702f-b29e-4350-bc86-ef002a69efc1" />


# ğŸ› ï¸ Data Preparation Techniques for Machine Learning

Data preparation is the crucial process of transforming raw data into a form more suitable for modeling. The required steps depend on the specific data and the algorithms to be used.

---

## The 5 Core Data Preparation Tasks

These tasks form the framework for preparing structured (tabular) data in a predictive modeling project:

### 1. Data Cleaning ğŸ§¹
* **Definition:** Identifying and correcting mistakes or errors in "messy" data.
* **Goal:** Ensure data quality and reliability.
* **Key Operations:**
    * Identifying and addressing **outliers** (using statistical methods).
    * Removing **duplicate rows** of data.
    * Identifying columns with zero variance (same value) and removing them.
    * Marking empty values as missing and **imputing** (filling) missing values using statistics (mean, median) or a learned model.

### 2. Feature Selection ğŸ¯
* **Definition:** Selecting a subset of input features that are most relevant to the target variable.
* **Goal:** Improve model performance and favor the simplest possible model by removing **irrelevant and redundant variables**.
* **Technique Groups:**
    * **Filter Methods:** Scoring input features (e.g., using correlation statistics) and selecting the top subset.
    * **Wrapper Methods:** Explicitly choosing features that result in the best-performing model (e.g., RFE).
    * **Intrinsic Methods:** Models that automatically select features during the fitting process.

### 3. Data Transforms ğŸ”„
* **Definition:** Changing the type or distribution of variables to meet algorithm requirements.
* **Key Transforms:**
    * **Encoding Categorical Data:** Converting non-numeric labels into a numeric form:
        * *One-Hot Transform:* For nominal (unordered) variables.
        * *Ordinal Transform:* For ordinal (ranked) variables.
    * **Scaling Numeric Data:** Adjusting the range of real-valued variables:
        * **Normalization:** Scaling a variable to a range between 0 and 1.
        * **Standardization:** Shifting data to a Standard Gaussian (mean of zero, std dev of one).
    * **Power Transform / Quantile Transform:** Changing the probability distribution of numerical variables (e.g., making the distribution more Gaussian).

### 4. Feature Engineering âœ¨
* **Definition:** Creating **new input variables** from the available data. This often requires deep **domain expertise**.
* **Goal:** Provide the model with a more straightforward perspective on the input data and add broader context.
* **Examples:**
    * Adding a **boolean flag** for a specific state (e.g., *IsWeekend*).
    * Deriving **summary statistics** (e.g., adding a global mean).
    * Decomposing complex variables (e.g., splitting a **date-time** into separate Day, Month, Year variables).

### 5. Dimensionality Reduction ğŸ“‰
* **Definition:** Creating a projection of the data into a **lower-dimensional space** while preserving the most important properties.
* **Goal:** Address the **"curse of dimensionality"** (when too many input variables lead to a sparse and unrepresentative sampling of the space).
* **Key Techniques:**
    * **Principal Component Analysis (PCA)**
    * **Singular Value Decomposition (SVD)**

> **Note:** Unlike Feature Selection, variables created by dimensionality reduction are not directly related to the original inputs, making the results difficult to interpret.

 [Source: ML Mastery ,Jason BrownleeÂ PhD](https://machinelearningmastery.com/data-preparation-techniques-for-machine-learning/)
