# 📉 Boyut Azaltmaya (Dimensionality Reduction) Giriş

<img width="923" height="466" alt="image" src="https://github.com/user-attachments/assets/544aa4de-d05c-4919-b13d-4ee83bfd99b5" />

<img width="948" height="476" alt="image" src="https://github.com/user-attachments/assets/6baadaaf-29c2-4274-8914-7dbf68f23b5c" />


[Picture](https://www.geeksforgeeks.org/machine-learning/dimensionality-reduction/)


Boyut azaltma, veri setlerindeki gereksiz karmaşıklığı gidererek makine öğrenmesi modellerini daha verimli ve güçlü hale getiren kritik bir süreçtir.

## 1. Boyut Azaltma Nedir?

Çok sayıda özelliğe sahip veri kümelerine **yüksek boyutlu veri (high-dimensional data)** denir. Aşırı özellik sayısı, algoritmaların öğrenmesini zorlaştırabilir (Boyutların Laneti - The Curse of Dimensionality).

👉 **Boyut Azaltma:**
* Bir veri kümesindeki **önemli bilginin çoğunu koruyarak** özelliği daha az özellikle temsil etme eylemidir.

---

## 2. Neden İhtiyaç Duyarız? (Faydaları)

| Fayda | Açıklama |
| :--- | :--- |
| ✅ **Daha Hızlı Hesaplama** | Daha az özellik, modelleri eğitmek için **daha az zaman** demektir. |
| ✅ **Daha İyi Görselleştirme** | İndirgenmiş veriyi 2D veya 3D olarak çizerek **kümeleri (clusters)** veya paternleri "görebiliriz". |
| ✅ **Aşırı Öğrenmeden Kaçınma** | Daha az gürültülü özellik, modellerin yeni verilere **daha iyi genelleme yapmasını** sağlar. |
| ✅ **Fazlalığı Giderme** | Birbiriyle ilişkili (**correlated**) özellikleri alarak, onları daha az sayıda **bağımsız boyuta** sıkıştırır. |

---

## 3. Boyut Azaltma Tekniklerinin Türleri

| Kategori | Tanım | Örnek Algoritma |
| :--- | :--- | :--- |
| **🔹 Özellik Seçimi (Feature Selection)** | Yalnızca en alakalı özellikleri seçip geri kalanları atmak. | Rastgele Orman'dan (Random Forest) en önemli özellikleri tutmak. |
| **🔹 Özellik Çıkarma (Feature Extraction)** | Orijinal özelliklerden, daha düşük bir uzayda **yeni özellikler** oluşturmak. | **Temel Bileşen Analizi (PCA)** ve **t-SNE**. |

---

## 4. Zorluklar ve Sınırlamalar

| Sınırlama | Açıklama |
| :--- | :--- |
| ❌ **Yorumlanabilirlik Kaybı** | PCA ile oluşturulan yeni bileşenlerin anlamını açıklamak zordur. |
| ❌ **Bilgi Kaybı** | Süreç **kayıplıdır (lossy)**; bir miktar bilgi her zaman atılır. |
| ❌ **Deneysel Seçim** | İhtiyaç duyulan doğru boyut sayısını seçmek **açıklanan varyansa** (explained variance) bakarak deney gerektirir. |

---

## ☝🏽 Özet

* **Boyut Azaltma**, veri setlerini özellik sayısını azaltarak basitleştirir.
* **Özellik Seçimi** = Önemli olanları tutar.
* **Özellik Çıkarma** (PCA, t-SNE) = Yeni özellikler oluşturur.
* **Fayda:** Daha hızlı, daha az gürültü, daha iyi görselleştirme.
* **Denge:** Yorumlanabilirlik veya bir miktar veri bilgisi kaybolabilir.

# 🔬 Boyut Azaltma: Temel Bileşen Analizi (PCA)


<img width="714" height="279" alt="image" src="https://github.com/user-attachments/assets/da581094-f891-4363-b5d0-a896f6b88806" />


**Temel Bileşen Analizi (PCA - Principal Component Analysis)**, veri setindeki **gürültüyü azaltmak**, hesaplama süresini hızlandırmak ve veriyi görselleştirmek için kullanılan, **doğrusal (linear)** bir **Özellik Çıkarma (Feature Extraction)** tekniğidir. PCA'nın temel amacı, verideki **maksimum varyansı (maximum variance)** yakalayan yeni, **ortogonal (birbirine dik)** eksenler bulmaktır.

## PCA Nedir ve Nasıl Çalışır?

PCA, orijinal özellikler uzayını alıp, verinin en fazla bilgi taşıyan (en fazla yayılan) yönlerine hizalanmış yeni bir boyut uzayına yansıtır.

### 1. Varyansı Maksimize Etmek

* **Varyans = Bilgi 💡:** Bir veri setinde, özelliklerin en çok yayıldığı (yani varyansın en yüksek olduğu) yön, genellikle en çok bilgiyi taşıyan yöndür.
* **Temel Bileşenler (Principal Components - PC'ler):** PCA'nın bulduğu yeni eksenlerdir.
    * **PC1:** Verideki en fazla varyansı açıklayan ilk eksendir. (En çok bilgiyi taşır.)
    * **PC2:** PC1'e dik (ortogonal) olan ve kalan varyansın en çoğunu açıklayan ikinci eksendir.
    * Bu süreç, veri setindeki tüm varyans açıklanana kadar devam eder.

### 2. Yeni Uzaya Dönüşüm

PCA, orijinal koordinat sistemini PC'ler ile tanımlanan yeni bir koordinat sistemine döndürmüş olur. Yeni PC'ler, orijinal özelliklerin **doğrusal bir kombinasyonudur (linear combination)**:

$$\text{PC}_i = w_{i1}x_1 + w_{i2}x_2 + \dots + w_{in}x_n$$

Burada $w_{ij}$ değerleri, PC'nin oluşturulmasına her bir orijinal özelliğin ne kadar katkıda bulunduğunu gösteren **ağırlıklar (weights)** veya **yüklerdir (loadings)**.

---

## 🔑 PCA'nın Ana Adımları

| Adım | Açıklama |
| :--- | :--- |
| **Standardizasyon (Scaling)** | PCA mesafeye dayalı olduğu için, tüm özelliklerin aynı ölçekte olması **kesinlikle gereklidir**. (Büyük değerler varyansa domine etmemelidir.) |
| **Kovaryans Matrisi (Covariance Matrix)** | Özellikler arasındaki ilişkileri (kovaryansları) hesaplar. |
| **Özdeğer/Özvektör Hesabı** | Kovaryans matrisinin **özdeğerleri (eigenvalues)** ve **özvektörleri (eigenvectors)** hesaplanır. Özvektörler yönü, Özdeğerler ise açıklanan varyansı gösterir. |
| **Boyut Azaltma** | En büyük özdeğerlere sahip $k$ adet özvektör seçilerek $k$ boyutlu yeni bir özellik uzayına dönüşüm yapılır. |

---

## 📉 Ne Kadar Boyut ($k$) Seçilmeli?

Kaç temel bileşenin ($k$) korunacağına karar vermek için iki yaygın yöntem kullanılır:

| Yöntem | Açıklama |
| :--- | :--- |
| **Açıklanan Varyans Oranı (Explained Variance Ratio)** | Toplam varyansın (örneğin %90'ı veya %95'i) ne kadarının korunduğuna bakılır. $k$ boyut seçildiğinde korunan bu oran, temel karar kriteridir. |
| **Serpinti Grafiği (Scree Plot)** | Özdeğerleri azalan sırada çizerek varyansın açıklanma hızının keskin bir şekilde düştüğü **"dirsek" noktasını (elbow point)** görsel olarak belirleriz. |

<img width="580" height="388" alt="image" src="https://github.com/user-attachments/assets/464928ec-b773-40fb-b9cc-02349cc2a7c5" />

# 🚀 PCA'nın Avantajları ve Uygulama Alanları

Temel Bileşen Analizi (PCA), makine öğrenmesi projelerine birçok fayda sağlar:

| Avantaj | Uygulama Alanı |
| :--- | :--- |
| **Gürültü Azaltma** 🚫 | En düşük varyansa sahip bileşenleri atarak, verideki **gürültüyü (noise)** ve aşırı öğrenme (overfitting) riskini azaltır. |
| **Hafıza ve Hesaplama** 💾 | Veri setinin boyutunu (sütun sayısını) düşürdüğü için **bellek kullanımını** ve **eğitim süresini** azaltır. |
| **Veri Sıkıştırma** 🗜️ | Birbirleriyle **ilişkili (correlated)** özellikleri, tek bir bağımsız bileşen altında birleştirerek veriyi sıkıştırır. |
| **Görselleştirme** 📊 | Veriyi 2 veya 3 boyuta indirgeyerek kümeleme analizinin sonuçlarını veya veri dağılımını gösterir. |
