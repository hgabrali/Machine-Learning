# ğŸ“‰ Boyut Azaltmaya (Dimensionality Reduction) GiriÅŸ

<img width="923" height="466" alt="image" src="https://github.com/user-attachments/assets/544aa4de-d05c-4919-b13d-4ee83bfd99b5" />

<img width="948" height="476" alt="image" src="https://github.com/user-attachments/assets/6baadaaf-29c2-4274-8914-7dbf68f23b5c" />


[Picture](https://www.geeksforgeeks.org/machine-learning/dimensionality-reduction/)


Boyut azaltma, veri setlerindeki gereksiz karmaÅŸÄ±klÄ±ÄŸÄ± gidererek makine Ã¶ÄŸrenmesi modellerini daha verimli ve gÃ¼Ã§lÃ¼ hale getiren kritik bir sÃ¼reÃ§tir.

## 1. Boyut Azaltma Nedir?

Ã‡ok sayÄ±da Ã¶zelliÄŸe sahip veri kÃ¼melerine **yÃ¼ksek boyutlu veri (high-dimensional data)** denir. AÅŸÄ±rÄ± Ã¶zellik sayÄ±sÄ±, algoritmalarÄ±n Ã¶ÄŸrenmesini zorlaÅŸtÄ±rabilir (BoyutlarÄ±n Laneti - The Curse of Dimensionality).

ğŸ‘‰ **Boyut Azaltma:**
* Bir veri kÃ¼mesindeki **Ã¶nemli bilginin Ã§oÄŸunu koruyarak** Ã¶zelliÄŸi daha az Ã¶zellikle temsil etme eylemidir.

---

## 2. Neden Ä°htiyaÃ§ DuyarÄ±z? (FaydalarÄ±)

| Fayda | AÃ§Ä±klama |
| :--- | :--- |
| âœ… **Daha HÄ±zlÄ± Hesaplama** | Daha az Ã¶zellik, modelleri eÄŸitmek iÃ§in **daha az zaman** demektir. |
| âœ… **Daha Ä°yi GÃ¶rselleÅŸtirme** | Ä°ndirgenmiÅŸ veriyi 2D veya 3D olarak Ã§izerek **kÃ¼meleri (clusters)** veya paternleri "gÃ¶rebiliriz". |
| âœ… **AÅŸÄ±rÄ± Ã–ÄŸrenmeden KaÃ§Ä±nma** | Daha az gÃ¼rÃ¼ltÃ¼lÃ¼ Ã¶zellik, modellerin yeni verilere **daha iyi genelleme yapmasÄ±nÄ±** saÄŸlar. |
| âœ… **FazlalÄ±ÄŸÄ± Giderme** | Birbiriyle iliÅŸkili (**correlated**) Ã¶zellikleri alarak, onlarÄ± daha az sayÄ±da **baÄŸÄ±msÄ±z boyuta** sÄ±kÄ±ÅŸtÄ±rÄ±r. |

---

## 3. Boyut Azaltma Tekniklerinin TÃ¼rleri

| Kategori | TanÄ±m | Ã–rnek Algoritma |
| :--- | :--- | :--- |
| **ğŸ”¹ Ã–zellik SeÃ§imi (Feature Selection)** | YalnÄ±zca en alakalÄ± Ã¶zellikleri seÃ§ip geri kalanlarÄ± atmak. | Rastgele Orman'dan (Random Forest) en Ã¶nemli Ã¶zellikleri tutmak. |
| **ğŸ”¹ Ã–zellik Ã‡Ä±karma (Feature Extraction)** | Orijinal Ã¶zelliklerden, daha dÃ¼ÅŸÃ¼k bir uzayda **yeni Ã¶zellikler** oluÅŸturmak. | **Temel BileÅŸen Analizi (PCA)** ve **t-SNE**. |

---

## 4. Zorluklar ve SÄ±nÄ±rlamalar

| SÄ±nÄ±rlama | AÃ§Ä±klama |
| :--- | :--- |
| âŒ **Yorumlanabilirlik KaybÄ±** | PCA ile oluÅŸturulan yeni bileÅŸenlerin anlamÄ±nÄ± aÃ§Ä±klamak zordur. |
| âŒ **Bilgi KaybÄ±** | SÃ¼reÃ§ **kayÄ±plÄ±dÄ±r (lossy)**; bir miktar bilgi her zaman atÄ±lÄ±r. |
| âŒ **Deneysel SeÃ§im** | Ä°htiyaÃ§ duyulan doÄŸru boyut sayÄ±sÄ±nÄ± seÃ§mek **aÃ§Ä±klanan varyansa** (explained variance) bakarak deney gerektirir. |

---

## â˜ğŸ½ Ã–zet

* **Boyut Azaltma**, veri setlerini Ã¶zellik sayÄ±sÄ±nÄ± azaltarak basitleÅŸtirir.
* **Ã–zellik SeÃ§imi** = Ã–nemli olanlarÄ± tutar.
* **Ã–zellik Ã‡Ä±karma** (PCA, t-SNE) = Yeni Ã¶zellikler oluÅŸturur.
* **Fayda:** Daha hÄ±zlÄ±, daha az gÃ¼rÃ¼ltÃ¼, daha iyi gÃ¶rselleÅŸtirme.
* **Denge:** Yorumlanabilirlik veya bir miktar veri bilgisi kaybolabilir.

# ğŸ”¬ Boyut Azaltma: Temel BileÅŸen Analizi (PCA)


<img width="714" height="279" alt="image" src="https://github.com/user-attachments/assets/da581094-f891-4363-b5d0-a896f6b88806" />


**Temel BileÅŸen Analizi (PCA - Principal Component Analysis)**, veri setindeki **gÃ¼rÃ¼ltÃ¼yÃ¼ azaltmak**, hesaplama sÃ¼resini hÄ±zlandÄ±rmak ve veriyi gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lan, **doÄŸrusal (linear)** bir **Ã–zellik Ã‡Ä±karma (Feature Extraction)** tekniÄŸidir. PCA'nÄ±n temel amacÄ±, verideki **maksimum varyansÄ± (maximum variance)** yakalayan yeni, **ortogonal (birbirine dik)** eksenler bulmaktÄ±r.

## PCA Nedir ve NasÄ±l Ã‡alÄ±ÅŸÄ±r?

PCA, orijinal Ã¶zellikler uzayÄ±nÄ± alÄ±p, verinin en fazla bilgi taÅŸÄ±yan (en fazla yayÄ±lan) yÃ¶nlerine hizalanmÄ±ÅŸ yeni bir boyut uzayÄ±na yansÄ±tÄ±r.

### 1. VaryansÄ± Maksimize Etmek

* **Varyans = Bilgi ğŸ’¡:** Bir veri setinde, Ã¶zelliklerin en Ã§ok yayÄ±ldÄ±ÄŸÄ± (yani varyansÄ±n en yÃ¼ksek olduÄŸu) yÃ¶n, genellikle en Ã§ok bilgiyi taÅŸÄ±yan yÃ¶ndÃ¼r.
* **Temel BileÅŸenler (Principal Components - PC'ler):** PCA'nÄ±n bulduÄŸu yeni eksenlerdir.
    * **PC1:** Verideki en fazla varyansÄ± aÃ§Ä±klayan ilk eksendir. (En Ã§ok bilgiyi taÅŸÄ±r.)
    * **PC2:** PC1'e dik (ortogonal) olan ve kalan varyansÄ±n en Ã§oÄŸunu aÃ§Ä±klayan ikinci eksendir.
    * Bu sÃ¼reÃ§, veri setindeki tÃ¼m varyans aÃ§Ä±klanana kadar devam eder.

### 2. Yeni Uzaya DÃ¶nÃ¼ÅŸÃ¼m

PCA, orijinal koordinat sistemini PC'ler ile tanÄ±mlanan yeni bir koordinat sistemine dÃ¶ndÃ¼rmÃ¼ÅŸ olur. Yeni PC'ler, orijinal Ã¶zelliklerin **doÄŸrusal bir kombinasyonudur (linear combination)**:

$$\text{PC}_i = w_{i1}x_1 + w_{i2}x_2 + \dots + w_{in}x_n$$

Burada $w_{ij}$ deÄŸerleri, PC'nin oluÅŸturulmasÄ±na her bir orijinal Ã¶zelliÄŸin ne kadar katkÄ±da bulunduÄŸunu gÃ¶steren **aÄŸÄ±rlÄ±klar (weights)** veya **yÃ¼klerdir (loadings)**.

---

## ğŸ”‘ PCA'nÄ±n Ana AdÄ±mlarÄ±

| AdÄ±m | AÃ§Ä±klama |
| :--- | :--- |
| **Standardizasyon (Scaling)** | PCA mesafeye dayalÄ± olduÄŸu iÃ§in, tÃ¼m Ã¶zelliklerin aynÄ± Ã¶lÃ§ekte olmasÄ± **kesinlikle gereklidir**. (BÃ¼yÃ¼k deÄŸerler varyansa domine etmemelidir.) |
| **Kovaryans Matrisi (Covariance Matrix)** | Ã–zellikler arasÄ±ndaki iliÅŸkileri (kovaryanslarÄ±) hesaplar. |
| **Ã–zdeÄŸer/Ã–zvektÃ¶r HesabÄ±** | Kovaryans matrisinin **Ã¶zdeÄŸerleri (eigenvalues)** ve **Ã¶zvektÃ¶rleri (eigenvectors)** hesaplanÄ±r. Ã–zvektÃ¶rler yÃ¶nÃ¼, Ã–zdeÄŸerler ise aÃ§Ä±klanan varyansÄ± gÃ¶sterir. |
| **Boyut Azaltma** | En bÃ¼yÃ¼k Ã¶zdeÄŸerlere sahip $k$ adet Ã¶zvektÃ¶r seÃ§ilerek $k$ boyutlu yeni bir Ã¶zellik uzayÄ±na dÃ¶nÃ¼ÅŸÃ¼m yapÄ±lÄ±r. |

---

## ğŸ“‰ Ne Kadar Boyut ($k$) SeÃ§ilmeli?

KaÃ§ temel bileÅŸenin ($k$) korunacaÄŸÄ±na karar vermek iÃ§in iki yaygÄ±n yÃ¶ntem kullanÄ±lÄ±r:

| YÃ¶ntem | AÃ§Ä±klama |
| :--- | :--- |
| **AÃ§Ä±klanan Varyans OranÄ± (Explained Variance Ratio)** | Toplam varyansÄ±n (Ã¶rneÄŸin %90'Ä± veya %95'i) ne kadarÄ±nÄ±n korunduÄŸuna bakÄ±lÄ±r. $k$ boyut seÃ§ildiÄŸinde korunan bu oran, temel karar kriteridir. |
| **Serpinti GrafiÄŸi (Scree Plot)** | Ã–zdeÄŸerleri azalan sÄ±rada Ã§izerek varyansÄ±n aÃ§Ä±klanma hÄ±zÄ±nÄ±n keskin bir ÅŸekilde dÃ¼ÅŸtÃ¼ÄŸÃ¼ **"dirsek" noktasÄ±nÄ± (elbow point)** gÃ¶rsel olarak belirleriz. |

<img width="580" height="388" alt="image" src="https://github.com/user-attachments/assets/464928ec-b773-40fb-b9cc-02349cc2a7c5" />

# ğŸš€ PCA'nÄ±n AvantajlarÄ± ve Uygulama AlanlarÄ±

Temel BileÅŸen Analizi (PCA), makine Ã¶ÄŸrenmesi projelerine birÃ§ok fayda saÄŸlar:

| Avantaj | Uygulama AlanÄ± |
| :--- | :--- |
| **GÃ¼rÃ¼ltÃ¼ Azaltma** ğŸš« | En dÃ¼ÅŸÃ¼k varyansa sahip bileÅŸenleri atarak, verideki **gÃ¼rÃ¼ltÃ¼yÃ¼ (noise)** ve aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riskini azaltÄ±r. |
| **HafÄ±za ve Hesaplama** ğŸ’¾ | Veri setinin boyutunu (sÃ¼tun sayÄ±sÄ±nÄ±) dÃ¼ÅŸÃ¼rdÃ¼ÄŸÃ¼ iÃ§in **bellek kullanÄ±mÄ±nÄ±** ve **eÄŸitim sÃ¼resini** azaltÄ±r. |
| **Veri SÄ±kÄ±ÅŸtÄ±rma** ğŸ—œï¸ | Birbirleriyle **iliÅŸkili (correlated)** Ã¶zellikleri, tek bir baÄŸÄ±msÄ±z bileÅŸen altÄ±nda birleÅŸtirerek veriyi sÄ±kÄ±ÅŸtÄ±rÄ±r. |
| **GÃ¶rselleÅŸtirme** ğŸ“Š | Veriyi 2 veya 3 boyuta indirgeyerek kÃ¼meleme analizinin sonuÃ§larÄ±nÄ± veya veri daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶sterir. |
